{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UFC1PDWCWZox",
        "gwrpmeblX-1u"
      ],
      "mount_file_id": "1W-EQw-9wFkcIxPj5VYRQt74NoHDZju5g",
      "authorship_tag": "ABX9TyPYLBrwcMCJhyL7Que8HoV1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Projects/blob/main/Mask_NoMask_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "from roboflow import Roboflow\n",
        "import xml.etree.ElementTree as ET"
      ],
      "metadata": {
        "id": "Ogf9_gtrGdgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "t1DR9_jvZJAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = Roboflow(api_key=\"Tqkp5uu6f4yAOuShs3oP\")\n",
        "project = rf.workspace(\"joseph-nelson\").project(\"mask-wearing\")\n",
        "dataset = project.version(4).download(\"tensorflow\")\n",
        "os.rename('/content/Mask-Wearing-4', '/content/data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbSezrfrPavr",
        "outputId": "3af30f0c-8b76-433f-e8c2-83fb7f4036bd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in Mask-Wearing-4 to tensorflow: 100% [19457480 / 19457480] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to Mask-Wearing-4 in tensorflow:: 100%|██████████| 154/154 [00:00<00:00, 793.86it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XML to CSV\n",
        "Convert annotation files(xml format) to a csv file.\n",
        "(for manually annotated imges)\n"
      ],
      "metadata": {
        "id": "UFC1PDWCWZox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xml_to_csv(path):\n",
        "    xml_list = []\n",
        "    for xml_file in glob.glob(path + '/*.xml'):\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        for member in root.findall('object'):\n",
        "            value = (root.find('filename').text,\n",
        "                     int(root.find('size')[0].text),\n",
        "                     int(root.find('size')[1].text),\n",
        "                     member[0].text,\n",
        "                     int(member[4][0].text),\n",
        "                     int(member[4][1].text),\n",
        "                     int(member[4][2].text),\n",
        "                     int(member[4][3].text))\n",
        "            \n",
        "            xml_list.append(value)\n",
        "    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
        "    return xml_df"
      ],
      "metadata": {
        "id": "Tq7LdCW0Gene"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_path = '/content/'\n",
        "\n",
        "xml_df = xml_to_csv(images_path)\n",
        "xml_df.to_csv('/content/annotations.csv', index=None)    "
      ],
      "metadata": {
        "id": "b3YIegnoLa2j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label Map\n",
        "\n",
        "Used in TensorFlow Object Detection API"
      ],
      "metadata": {
        "id": "gwrpmeblX-1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labelmap_file = '''item {\n",
        "    id: 1\n",
        "    name: 'mask'\n",
        "}\n",
        "item {\n",
        "    id: 2\n",
        "    name: 'no-mask'\n",
        "}'''\n",
        "\n",
        "with open('/content/data/labelmap.pbtxt', 'w+') as f:\n",
        "  f.write(labelmap_file)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "u2ZXsJFhRZ4j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer Dataset to TFrecords"
      ],
      "metadata": {
        "id": "QDAyV56CZOeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Object Detection API\n"
      ],
      "metadata": {
        "id": "ijYTTY63cVKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "# Clone the tensorflow models repository if it doesn't already exist\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "  while \"models\" in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "  !git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWu_eZr7cdOs",
        "outputId": "96b2529a-7de3-41c4-ffdf-bd29474a4b49"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 3634, done.\u001b[K\n",
            "remote: Counting objects: 100% (3634/3634), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3030/3030), done.\u001b[K\n",
            "remote: Total 3634 (delta 960), reused 1536 (delta 550), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (3634/3634), 47.37 MiB | 18.46 MiB/s, done.\n",
            "Resolving deltas: 100% (960/960), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Object Detection API\n",
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v17vFLlkcgdM",
        "outputId": "bbaf3b1e-1e2c-4094-dfed-27e8b8fd7caf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/models/research\n",
            "Collecting avro-python3\n",
            "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
            "Collecting apache-beam\n",
            "  Downloading apache_beam-2.43.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.5 MB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (7.1.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (4.9.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (3.2.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (0.29.32)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (0.5.5)\n",
            "Collecting tf-slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (1.15.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (2.0.6)\n",
            "Collecting lvis\n",
            "  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (1.7.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (1.3.5)\n",
            "Collecting tf-models-official>=2.5.1\n",
            "  Downloading tf_models_official-2.11.2-py2.py3-none-any.whl (2.3 MB)\n",
            "Collecting tensorflow_io\n",
            "  Downloading tensorflow_io-0.29.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (26.9 MB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (2.9.0)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (2.4.7)\n",
            "Collecting sacrebleu<=2.2.0\n",
            "  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (1.21.6)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.8/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (0.8.10)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.5.12)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.6.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n",
            "Collecting tensorflow-text~=2.11.0\n",
            "  Downloading tensorflow_text-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "Collecting immutabledict\n",
            "  Downloading immutabledict-2.2.3-py3-none-any.whl (4.0 kB)\n",
            "Collecting tensorflow~=2.11.0\n",
            "  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.6.0.66)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "Collecting pyyaml<6.0,>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.12.11)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.4.8)\n",
            "Collecting tensorflow-model-optimization>=0.4.1\n",
            "  Downloading tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.17.4)\n",
            "Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.15.0)\n",
            "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.8.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.0.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.57.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.8/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.28.1)\n",
            "Requirement already satisfied: protobuf<5.0.0dev,>=3.15.0 in /usr/local/lib/python3.8/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.19.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.2.8)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2021.5.30)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.64.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.26.6)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (7.0.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->object-detection==0.1) (2022.6)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.4.8)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.10)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (1.51.1)\n",
            "Collecting tensorboard<2.12,>=2.11\n",
            "  Downloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n",
            "Collecting keras\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (14.0.6)\n",
            "Collecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-22.12.6-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (2.1.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (0.28.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (4.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (21.3)\n",
            "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (0.38.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (3.4.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (5.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (3.11.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (3.2.2)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.7)\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (1.22.1)\n",
            "Collecting pymongo<4.0.0,>=3.8.0\n",
            "  Downloading pymongo-3.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (526 kB)\n",
            "Collecting fasteners<1.0,>=0.3\n",
            "  Downloading fasteners-0.18-py3-none-any.whl (18 kB)\n",
            "Collecting fastavro<2,>=0.23.6\n",
            "  Downloading fastavro-1.7.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "Requirement already satisfied: pyarrow<10.0.0,>=0.15.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (9.0.0)\n",
            "Collecting objsize<0.6.0,>=0.5.2\n",
            "  Downloading objsize-0.5.2-py3-none-any.whl (8.2 kB)\n",
            "Collecting cloudpickle~=2.2.0\n",
            "  Downloading cloudpickle-2.2.0-py3-none-any.whl (25 kB)\n",
            "Collecting zstandard<1,>=0.18.0\n",
            "  Downloading zstandard-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n",
            "Collecting orjson<4.0\n",
            "  Downloading orjson-3.8.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\n",
            "Collecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from lvis->object-detection==0.1) (0.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.8/dist-packages (from lvis->object-detection==0.1) (4.6.0.66)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from lvis->object-detection==0.1) (1.4.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons->tf-models-official>=2.5.1->object-detection==0.1) (2.7.1)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.9.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.10.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (5.10.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.12.0)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.29.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
            "Building wheels for collected packages: object-detection, dill, avro-python3, docopt, seqeval\n",
            "  Building wheel for object-detection (setup.py): started\n",
            "  Building wheel for object-detection (setup.py): finished with status 'done'\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1696474 sha256=9f5a455a0f6a48fa557644bd4d81fd27b2fe23a6dd8a385750711bd621076beb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-znhflghj/wheels/7d/96/c1/072a751379735e8dfdada1def1c62a89afb3cc45654fd6fd28\n",
            "  Building wheel for dill (setup.py): started\n",
            "  Building wheel for dill (setup.py): finished with status 'done'\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=43d93c1c06b9a5f1ef560e8fb21bae60be799daadeda31dee973dfdf6f98956a\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/35/78/e9004fa30578734db7f10e7a211605f3f0778d2bdde38a239d\n",
            "  Building wheel for avro-python3 (setup.py): started\n",
            "  Building wheel for avro-python3 (setup.py): finished with status 'done'\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=44009 sha256=66c8a57df5d5022acd6b0647377b2ea8b94c655cb14a6215bb4a4ad9c70734fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/73/e9/d273421f5723c4bf544dcf9eb097bda94421ef8d3252699f0a\n",
            "  Building wheel for docopt (setup.py): started\n",
            "  Building wheel for docopt (setup.py): finished with status 'done'\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=bfbebe138aeca4c1daf675148d71018256510b3651d3e3a6c8b3be3b2e823116\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
            "  Building wheel for seqeval (setup.py): started\n",
            "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16179 sha256=3ac343a7326b1bfa9b03d2f38f685ce0f4884d8455b724413dc9716d6e6290f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n",
            "Successfully built object-detection dill avro-python3 docopt seqeval\n",
            "Installing collected packages: tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, keras, flatbuffers, tensorflow, portalocker, docopt, dill, colorama, zstandard, tf-slim, tensorflow-text, tensorflow-model-optimization, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pyyaml, pymongo, py-cpuinfo, orjson, objsize, immutabledict, hdfs, fasteners, fastavro, cloudpickle, tf-models-official, tensorflow-io, lvis, avro-python3, apache-beam, object-detection\n",
            "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
            "    Found existing installation: tensorflow-io-gcs-filesystem 0.28.0\n",
            "    Uninstalling tensorflow-io-gcs-filesystem-0.28.0:\n",
            "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.28.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "  Attempting uninstall: pymongo\n",
            "    Found existing installation: pymongo 4.3.3\n",
            "    Uninstalling pymongo-4.3.3:\n",
            "      Successfully uninstalled pymongo-4.3.3\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.5.0\n",
            "    Uninstalling cloudpickle-1.5.0:\n",
            "      Successfully uninstalled cloudpickle-1.5.0\n",
            "Successfully installed apache-beam-2.43.0 avro-python3-1.10.2 cloudpickle-2.2.0 colorama-0.4.6 dill-0.3.1.1 docopt-0.6.2 fastavro-1.7.0 fasteners-0.18 flatbuffers-22.12.6 hdfs-2.7.0 immutabledict-2.2.3 keras-2.11.0 lvis-0.5.3 object-detection-0.1 objsize-0.5.2 orjson-3.8.3 portalocker-2.6.0 py-cpuinfo-9.0.0 pymongo-3.13.0 pyyaml-5.4.1 sacrebleu-2.2.0 sentencepiece-0.1.97 seqeval-1.2.2 tensorboard-2.11.0 tensorflow-2.11.0 tensorflow-addons-0.19.0 tensorflow-estimator-2.11.0 tensorflow-io-0.29.0 tensorflow-io-gcs-filesystem-0.29.0 tensorflow-model-optimization-0.7.3 tensorflow-text-2.11.0 tf-models-official-2.11.2 tf-slim-1.1.0 zstandard-0.19.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import argparse\n",
        "import collections\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "\n",
        "from object_detection.utils import dataset_util\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "def split(df, group):\n",
        "\tdata = collections.namedtuple('data', ['filename', 'object'])\n",
        "\tgb = df.groupby(group)\n",
        "\treturn [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "\n",
        "def create_tf_example(group, path, category_idx):\n",
        "\twith tf.io.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "\t\tencoded_jpg = fid.read()\n",
        "\tencoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "\timage = Image.open(encoded_jpg_io)\n",
        "\twidth, height = image.size\n",
        "\n",
        "\tfilename = group.filename.encode('utf8')\n",
        "\timage_format = b'jpg'\n",
        "\txmins = []\n",
        "\txmaxs = []\n",
        "\tymins = []\n",
        "\tymaxs = []\n",
        "\tclasses_text = []\n",
        "\tclasses = []\n",
        "\n",
        "\tfor index, row in group.object.iterrows():\n",
        "\t\txmins.append(float(row['xmin']) / width)\n",
        "\t\txmaxs.append(float(row['xmax']) / width)\n",
        "\t\tymins.append(float(row['ymin']) / height)\n",
        "\t\tymaxs.append(float(row['ymax']) / height)\n",
        "\t\tclasses_text.append(row['class'].encode('utf8'))\n",
        "\t\tclasses.append(category_idx[row['class']])\n",
        "\n",
        "\ttf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "\t\t'image/height': dataset_util.int64_feature(height),\n",
        "\t\t'image/width': dataset_util.int64_feature(width),\n",
        "\t\t'image/filename': dataset_util.bytes_feature(filename),\n",
        "\t\t'image/source_id': dataset_util.bytes_feature(filename),\n",
        "\t\t'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "\t\t'image/format': dataset_util.bytes_feature(image_format),\n",
        "\t\t'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "\t\t'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "\t\t'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "\t\t'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "\t\t'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "\t\t'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "\t}))\n",
        "\treturn tf_example"
      ],
      "metadata": {
        "id": "srCb-afeRhuS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for f_name in ['train', 'test', 'valid']:\n",
        "\n",
        "  img_path = f'/content/data/{f_name}'\n",
        "  csv_path = f'/content/data/{f_name}/_annotations.csv'\n",
        "  tfrecord_path = f'/content/data/{f_name}/{f_name}.tfrecord'\n",
        "  labelmap_path  = '/content/data/labelmap.pbtxt'\n",
        "\n",
        "  label_map_dict = label_map_util.get_label_map_dict(labelmap_path)\n",
        "  writer = tf.io.TFRecordWriter(tfrecord_path)\n",
        "\n",
        "  examples = pd.read_csv(csv_path)\n",
        "  grouped = split(examples, 'filename')\n",
        "  for group in grouped:\n",
        "    tf_example = create_tf_example(group, img_path, label_map_dict)\n",
        "    writer.write(tf_example.SerializeToString())\n",
        "\n",
        "  writer.close()\n",
        "  print(f'Successfully created the TFRecords: {tfrecord_path}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woE6k-l-bkIg",
        "outputId": "fdc07f60-e3c5-43b5-8b4d-924a506351d7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully created the TFRecords: /content/data/train/train.tfrecord\n",
            "Successfully created the TFRecords: /content/data/test/test.tfrecord\n",
            "Successfully created the TFRecords: /content/data/valid/valid.tfrecord\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "yqPJkSe2ttiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install a model from the model garden\n",
        "Choose from a large selection of object detection models in the [TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)."
      ],
      "metadata": {
        "id": "MrkQIt95_rtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"faster_rcnn_resnet50_v1_640x640_coco17_tpu-8\"\n",
        "\n",
        "%cd /content/models/research/object_detection\n",
        "!mkdir pre-trained-models\n",
        "%cd pre-trained-models\n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/{MODEL}.tar.gz\n",
        "!tar -xf {MODEL}.tar.gz\n",
        "!rm {MODEL}.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skA7tAJA_rTc",
        "outputId": "0d1afba2-16c1-4e7d-9a8b-6e84552eefd4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research/object_detection\n",
            "/content/models/research/object_detection/pre-trained-models\n",
            "--2022-12-29 19:00:41--  http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 74.125.195.128, 2607:f8b0:400e:c09::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|74.125.195.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 211996178 (202M) [application/x-tar]\n",
            "Saving to: ‘faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.tar.gz’\n",
            "\n",
            "faster_rcnn_resnet5 100%[===================>] 202.17M   143MB/s    in 1.4s    \n",
            "\n",
            "2022-12-29 19:00:43 (143 MB/s) - ‘faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.tar.gz’ saved [211996178/211996178]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Edit pipeline.config\n",
        "The pipeline.config file is in protocol buffer format, and we can use the config_util to change the settings we need."
      ],
      "metadata": {
        "id": "dmagfqHoAepA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from object_detection.utils import config_util\n",
        "\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.builders import model_builder"
      ],
      "metadata": {
        "id": "MZyXXBfx4tRZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_config = f\"./{MODEL}/pipeline.config\"\n",
        "model_dir = f\"./{MODEL}/checkpoint\"\n",
        "\n",
        "config = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
        "\n",
        "model_config = config['model']\n",
        "model_config.faster_rcnn.num_classes = 2\n",
        "\n",
        "train_config = config['train_config']\n",
        "\n",
        "#Starting checkpoin location\n",
        "train_config.fine_tune_checkpoint = f\"/content/models/research/object_detection/pre-trained-models/{MODEL}/checkpoint/ckpt-0\"\n",
        "\n",
        "# GPU memory limits batch size\n",
        "train_config.batch_size = 32\n",
        "train_config.use_bfloat16 = False\n",
        "\n",
        "\n",
        "#...................$$$$$$$$$$$$$$$.................\n",
        "# some models use \"fine_tune\" rather than \"detection\" for fine-tuning\n",
        "train_config.fine_tune_checkpoint_type = \"detection\"\n",
        "# detection      ===> use all the layers as pretrained layers\n",
        "# classification ===> use only the classification layer as pretrained and initialize the weights randomly\n",
        "\n",
        "\n",
        "# set auto-augmentation\n",
        "# del train_config.data_augmentation_options[:]\n",
        "\n",
        "# a1=train_config.data_augmentation_options.add()\n",
        "# a1.autoaugment_image.policy_name = \"v2\"\n",
        "\n",
        "# training dataset\n",
        "train_input_config = config['train_input_config']\n",
        "train_input_config.label_map_path = \"/content/data/labelmap.pbtxt\"\n",
        "train_input_config.tf_record_input_reader.input_path[:] = [\"/content/data/train/*.tfrecord\",\n",
        "                                                           \"/content/data/eval/*.tfrecord\"]  # have little data, so we use eval as train data\n",
        "# evaluation dataset                                                           \n",
        "eval_input_config = config['eval_input_config']\n",
        "eval_input_config.label_map_path = \"/content/data/labelmap.pbtxt\"\n",
        "eval_input_config.tf_record_input_reader.input_path[:] = [\"/content/data/test/*.tfrecord\"]\n",
        "\n",
        "# save pipeline.config file\n",
        "pipeline_proto = config_util.create_pipeline_proto_from_configs(config)\n",
        "config_util.save_pipeline_config(pipeline_config=pipeline_proto, directory=f'/content/models/research/object_detection/pre-trained-models/{MODEL}')"
      ],
      "metadata": {
        "id": "Zqgdhby1AlQ5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the contents of the pipeline.config file:\n"
      ],
      "metadata": {
        "id": "5s3RmNtPHFN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !cat /content/models/research/object_detection/pre-trained-models/{MODEL}/pipeline.config"
      ],
      "metadata": {
        "id": "4gJ21vV3G1N3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "ayADewGsHJMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/data/train_process/"
      ],
      "metadata": {
        "id": "_WIhifYaKaae"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/models/research\n",
        "!python object_detection/model_main_tf2.py \\\n",
        "    --num_train_steps=1000 \\\n",
        "    --pipeline_config_path=/content/models/research/object_detection/pre-trained-models/{MODEL}/pipeline.config \\\n",
        "    --model_dir=/content/data/train_process/ \\\n",
        "    --alsologtostderr\n",
        "    #  &> /content/data/train.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2YeXp_wHFyr",
        "outputId": "5eadf9e9-de5d-470f-b913-ae2c45f387d7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research\n",
            "2022-12-29 19:42:15.289346: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2022-12-29 19:42:15.289685: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2022-12-29 19:42:15.289715: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2022-12-29 19:42:21.486844: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "I1229 19:42:21.522635 140197933782912 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "INFO:tensorflow:Maybe overwriting train_steps: 1000\n",
            "I1229 19:42:21.529534 140197933782912 config_util.py:552] Maybe overwriting train_steps: 1000\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I1229 19:42:21.531046 140197933782912 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "W1229 19:42:21.602841 140197933782912 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "INFO:tensorflow:Reading unweighted datasets: ['/content/data/train/*.tfrecord', '/content/data/eval/*.tfrecord']\n",
            "I1229 19:42:21.618884 140197933782912 dataset_builder.py:162] Reading unweighted datasets: ['/content/data/train/*.tfrecord', '/content/data/eval/*.tfrecord']\n",
            "INFO:tensorflow:Reading record datasets for input file: ['/content/data/train/*.tfrecord', '/content/data/eval/*.tfrecord']\n",
            "I1229 19:42:21.623196 140197933782912 dataset_builder.py:79] Reading record datasets for input file: ['/content/data/train/*.tfrecord', '/content/data/eval/*.tfrecord']\n",
            "INFO:tensorflow:Number of filenames to read: 1\n",
            "I1229 19:42:21.623335 140197933782912 dataset_builder.py:80] Number of filenames to read: 1\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W1229 19:42:21.623422 140197933782912 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "W1229 19:42:21.634348 140197933782912 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W1229 19:42:21.670694 140197933782912 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
            "W1229 19:42:22.641860 140197933782912 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W1229 19:42:30.189365 140197933782912 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W1229 19:42:32.662682 140197933782912 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "/usr/local/lib/python3.8/dist-packages/keras/backend.py:451: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn(\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I1229 19:42:43.568324 140193461954304 convolutional_keras_box_predictor.py:152] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:459: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use ref() instead.\n",
            "W1229 19:43:06.755403 140193461954304 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:459: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use ref() instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "W1229 19:43:18.719189 140193461954304 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "2022-12-29 19:44:40.454904: W tensorflow/tsl/framework/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 28.71GiB (rounded to 30828134400)requested by op MultiLevelMatMulCropAndResize/MultiLevelRoIAlign/GatherV2_1\n",
            "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
            "Current allocation summary follows.\n",
            "Current allocation summary follows.\n",
            "2022-12-29 19:44:40.455599: W tensorflow/tsl/framework/bfc_allocator.cc:492] ________________________________________________________________________***____*********_____*******\n",
            "2022-12-29 19:44:40.455655: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at gather_op.cc:158 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[7526400,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
            "Traceback (most recent call last):\n",
            "  File \"object_detection/model_main_tf2.py\", line 114, in <module>\n",
            "    tf.compat.v1.app.run()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py\", line 36, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/absl/app.py\", line 308, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/absl/app.py\", line 254, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"object_detection/model_main_tf2.py\", line 105, in main\n",
            "    model_lib_v2.train_loop(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/object_detection/model_lib_v2.py\", line 605, in train_loop\n",
            "    load_fine_tune_checkpoint(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/object_detection/model_lib_v2.py\", line 401, in load_fine_tune_checkpoint\n",
            "    _ensure_model_is_built(model, input_dataset, unpad_groundtruth_tensors)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/object_detection/model_lib_v2.py\", line 176, in _ensure_model_is_built\n",
            "    strategy.run(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 1316, in run\n",
            "    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 2895, in call_for_each_replica\n",
            "    return self._call_for_each_replica(fn, args, kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 696, in _call_for_each_replica\n",
            "    return mirrored_run.call_for_each_replica(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py\", line 83, in call_for_each_replica\n",
            "    return wrapped(args, kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\", line 52, in quick_execute\n",
            "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
            "tensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:\n",
            "\n",
            "Detected at node 'MultiLevelMatMulCropAndResize/MultiLevelRoIAlign/GatherV2_1' defined at (most recent call last):\n",
            "    File \"/usr/lib/python3.8/threading.py\", line 890, in _bootstrap\n",
            "      self._bootstrap_inner()\n",
            "    File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
            "      self.run()\n",
            "    File \"/usr/local/lib/python3.8/dist-packages/object_detection/model_lib_v2.py\", line 171, in _dummy_computation_fn\n",
            "      return _compute_losses_and_predictions_dicts(model, features, labels,\n",
            "    File \"/usr/local/lib/python3.8/dist-packages/object_detection/model_lib_v2.py\", line 124, in _compute_losses_and_predictions_dicts\n",
            "      prediction_dict = model.predict(\n",
            "    File \"/usr/local/lib/python3.8/dist-packages/object_detection/meta_architectures/faster_rcnn_meta_arch.py\", line 822, in predict\n",
            "      if self._number_of_stages >= 2:\n",
            "    File \"/usr/local/lib/python3.8/dist-packages/object_detection/meta_architectures/faster_rcnn_meta_arch.py\", line 823, in predict\n",
            "      prediction_dict.update(\n",
            "    File \"/usr/local/lib/python3.8/dist-packages/object_detection/meta_architectures/faster_rcnn_meta_arch.py\", line 1000, in _predict_second_stage\n",
            "      prediction_dict = self._box_prediction(rpn_features_to_crop,\n",
            "    File \"/usr/local/lib/python3.8/dist-packages/object_detection/meta_architectures/faster_rcnn_meta_arch.py\", line 1062, in _box_prediction\n",
            "      flattened_proposal_feature_maps = (\n",
            "    File \"/usr/local/lib/python3.8/dist-packages/object_detection/meta_architectures/faster_rcnn_meta_arch.py\", line 2027, in _compute_second_stage_input_feature_maps\n",
            "      cropped_regions = self._flatten_first_two_dimensions(\n",
            "    File \"/usr/local/lib/python3.8/dist-packages/object_detection/utils/spatial_transform_ops.py\", line 521, in multilevel_matmul_crop_and_resize\n",
            "      return multilevel_roi_align(images,\n",
            "    File \"/usr/local/lib/python3.8/dist-packages/object_detection/utils/spatial_transform_ops.py\", line 358, in multilevel_roi_align\n",
            "      flattened_feature_values = _gather_valid_indices(flattened_features,\n",
            "    File \"/usr/local/lib/python3.8/dist-packages/object_detection/utils/spatial_transform_ops.py\", line 275, in _gather_valid_indices\n",
            "      return tf.gather(padded_tensor, indices + 1)\n",
            "Node: 'MultiLevelMatMulCropAndResize/MultiLevelRoIAlign/GatherV2_1'\n",
            "OOM when allocating tensor with shape[7526400,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
            "\t [[{{node MultiLevelMatMulCropAndResize/MultiLevelRoIAlign/GatherV2_1}}]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
            " [Op:__inference__dummy_computation_fn_99545]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tail -n 10 /content/data/train.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFcDU85FK5fn",
        "outputId": "426759ce-fbcb-4081-ac5a-cdbac883997f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      return multilevel_roi_align(images,\n",
            "    File \"/usr/local/lib/python3.8/dist-packages/object_detection/utils/spatial_transform_ops.py\", line 358, in multilevel_roi_align\n",
            "      flattened_feature_values = _gather_valid_indices(flattened_features,\n",
            "    File \"/usr/local/lib/python3.8/dist-packages/object_detection/utils/spatial_transform_ops.py\", line 275, in _gather_valid_indices\n",
            "      return tf.gather(padded_tensor, indices + 1)\n",
            "Node: 'MultiLevelMatMulCropAndResize/MultiLevelRoIAlign/GatherV2_1'\n",
            "OOM when allocating tensor with shape[7526400,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
            "\t [[{{node MultiLevelMatMulCropAndResize/MultiLevelRoIAlign/GatherV2_1}}]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
            " [Op:__inference__dummy_computation_fn_99545]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluaion"
      ],
      "metadata": {
        "id": "BnK8ITU1LRI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path=/content/models/research/object_detection/pre-trained-models/{MODEL}/pipeline.config \\\n",
        "    --model_dir=/content/data/train_process/ \\\n",
        "    --checkpoint_dir=/content/data/train_process/ \\\n",
        "    --eval_timeout=30 \\\n",
        "    --alsologtostderr\n",
        "\n",
        "#checkpoint_dir:  Only different to select it as Testing not training the"
      ],
      "metadata": {
        "id": "A8mgYoG9LFbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Copy the piplie.config for later use\n",
        "!cp /content/models/research/object_detection/pre-trained-models/{MODEL}/pipeline.config /content/training/."
      ],
      "metadata": {
        "id": "TDzR1SqcarbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Export trained model\n",
        "!python object_detection/exporter_main_v2.py \\\n",
        "    --input_type image_tensor \\\n",
        "    --pipeline_config_path=/kaggle/working/models/research/object_detection/pre-trained-models/{MODEL}/pipeline.config \\\n",
        "    --trained_checkpoint_dir /kaggle/working/training/ \\\n",
        "    --output_directory /kaggle/working/exported-models/effdet_d2"
      ],
      "metadata": {
        "id": "uuuwAW6uMEcg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}