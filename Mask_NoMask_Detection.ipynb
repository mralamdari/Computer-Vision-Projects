{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UFC1PDWCWZox",
        "gwrpmeblX-1u"
      ],
      "mount_file_id": "1W-EQw-9wFkcIxPj5VYRQt74NoHDZju5g",
      "authorship_tag": "ABX9TyOux+RieWYRbrH6PDaxRzTG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Projects/blob/main/Mask_NoMask_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "from roboflow import Roboflow\n",
        "import xml.etree.ElementTree as ET"
      ],
      "metadata": {
        "id": "Ogf9_gtrGdgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "t1DR9_jvZJAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = Roboflow(api_key=\"\")\n",
        "project = rf.workspace(\"joseph-nelson\").project(\"mask-wearing\")\n",
        "dataset = project.version(4).download(\"tensorflow\")\n",
        "os.rename('/content/Mask-Wearing-4', '/content/data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbSezrfrPavr",
        "outputId": "df1db388-31ca-46b9-987b-b83e6b0b8128"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in Mask-Wearing-4 to tensorflow: 100% [19457480 / 19457480] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to Mask-Wearing-4 in tensorflow:: 100%|██████████| 154/154 [00:00<00:00, 464.07it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XML to CSV\n",
        "Convert annotation files(xml format) to a csv file.\n",
        "(for manually annotated imges)\n"
      ],
      "metadata": {
        "id": "UFC1PDWCWZox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xml_to_csv(path):\n",
        "    xml_list = []\n",
        "    for xml_file in glob.glob(path + '/*.xml'):\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        for member in root.findall('object'):\n",
        "            value = (root.find('filename').text,\n",
        "                     int(root.find('size')[0].text),\n",
        "                     int(root.find('size')[1].text),\n",
        "                     member[0].text,\n",
        "                     int(member[4][0].text),\n",
        "                     int(member[4][1].text),\n",
        "                     int(member[4][2].text),\n",
        "                     int(member[4][3].text))\n",
        "            \n",
        "            xml_list.append(value)\n",
        "    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
        "    return xml_df"
      ],
      "metadata": {
        "id": "Tq7LdCW0Gene"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_path = '/content/'\n",
        "\n",
        "xml_df = xml_to_csv(images_path)\n",
        "xml_df.to_csv('/content/annotations.csv', index=None)    "
      ],
      "metadata": {
        "id": "b3YIegnoLa2j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label Map\n",
        "\n",
        "Used in TensorFlow Object Detection API"
      ],
      "metadata": {
        "id": "gwrpmeblX-1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labelmap_file = '''item {\n",
        "    id: 1\n",
        "    name: 'mask'\n",
        "}\n",
        "item {\n",
        "    id: 2\n",
        "    name: 'no-mask'\n",
        "}'''\n",
        "\n",
        "with open('/content/Mask-Wearing-4/labelmap.pbtxt', 'w+') as f:\n",
        "  f.write(labelmap_file)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "u2ZXsJFhRZ4j"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer Dataset to TFrecords"
      ],
      "metadata": {
        "id": "QDAyV56CZOeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Object Detection API\n"
      ],
      "metadata": {
        "id": "ijYTTY63cVKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "# Clone the tensorflow models repository if it doesn't already exist\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "  while \"models\" in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "  !git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "metadata": {
        "id": "fWu_eZr7cdOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Object Detection API\n",
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install ."
      ],
      "metadata": {
        "id": "v17vFLlkcgdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import argparse\n",
        "import collections\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "\n",
        "from object_detection.utils import dataset_util\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "def split(df, group):\n",
        "\tdata = collections.namedtuple('data', ['filename', 'object'])\n",
        "\tgb = df.groupby(group)\n",
        "\treturn [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "\n",
        "def create_tf_example(group, path, category_idx):\n",
        "\twith tf.io.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "\t\tencoded_jpg = fid.read()\n",
        "\tencoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "\timage = Image.open(encoded_jpg_io)\n",
        "\twidth, height = image.size\n",
        "\n",
        "\tfilename = group.filename.encode('utf8')\n",
        "\timage_format = b'jpg'\n",
        "\txmins = []\n",
        "\txmaxs = []\n",
        "\tymins = []\n",
        "\tymaxs = []\n",
        "\tclasses_text = []\n",
        "\tclasses = []\n",
        "\n",
        "\tfor index, row in group.object.iterrows():\n",
        "\t\txmins.append(float(row['xmin']) / width)\n",
        "\t\txmaxs.append(float(row['xmax']) / width)\n",
        "\t\tymins.append(float(row['ymin']) / height)\n",
        "\t\tymaxs.append(float(row['ymax']) / height)\n",
        "\t\tclasses_text.append(row['class'].encode('utf8'))\n",
        "\t\tclasses.append(category_idx[row['class']])\n",
        "\n",
        "\ttf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "\t\t'image/height': dataset_util.int64_feature(height),\n",
        "\t\t'image/width': dataset_util.int64_feature(width),\n",
        "\t\t'image/filename': dataset_util.bytes_feature(filename),\n",
        "\t\t'image/source_id': dataset_util.bytes_feature(filename),\n",
        "\t\t'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "\t\t'image/format': dataset_util.bytes_feature(image_format),\n",
        "\t\t'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "\t\t'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "\t\t'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "\t\t'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "\t\t'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "\t\t'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "\t}))\n",
        "\treturn tf_example"
      ],
      "metadata": {
        "id": "srCb-afeRhuS"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for f_name in ['train', 'test', 'valid']:\n",
        "\n",
        "  img_path = f'/content/data/{f_name}'\n",
        "  csv_path = f'/content/data/{f_name}/_annotations.csv'\n",
        "  tfrecord_path = f'/content/data/{f_name}/{f_name}.tfrecord'\n",
        "  labelmap_path  = '/content/data/labelmap.pbtxt'\n",
        "\n",
        "  label_map_dict = label_map_util.get_label_map_dict(labelmap_path)\n",
        "  writer = tf.io.TFRecordWriter(tfrecord_path)\n",
        "\n",
        "  examples = pd.read_csv(csv_path)\n",
        "  grouped = split(examples, 'filename')\n",
        "  for group in grouped:\n",
        "    tf_example = create_tf_example(group, img_path, label_map_dict)\n",
        "    writer.write(tf_example.SerializeToString())\n",
        "\n",
        "  writer.close()\n",
        "  print(f'Successfully created the TFRecords: {tfrecord_path}')"
      ],
      "metadata": {
        "id": "woE6k-l-bkIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "yqPJkSe2ttiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install a model from the model garden\n",
        "Choose from a large selection of object detection models in the [TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)."
      ],
      "metadata": {
        "id": "MrkQIt95_rtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"faster_rcnn_resnet50_v1_640x640_coco17_tpu-8\"\n",
        "\n",
        "%cd /content/models/research/object_detection\n",
        "!mkdir pre-trained-models\n",
        "%cd pre-trained-models\n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/{MODEL}.tar.gz\n",
        "!tar -xf {MODEL}.tar.gz\n",
        "!rm {MODEL}.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skA7tAJA_rTc",
        "outputId": "49eaa053-fb35-40c6-ecd5-5c0491f9aac1"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research/object_detection\n",
            "/content/models/research/object_detection/pre-trained-models\n",
            "--2022-12-29 17:48:09--  http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 108.177.111.128, 2607:f8b0:4001:c07::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|108.177.111.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 211996178 (202M) [application/x-tar]\n",
            "Saving to: ‘faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.tar.gz’\n",
            "\n",
            "faster_rcnn_resnet5 100%[===================>] 202.17M   128MB/s    in 1.6s    \n",
            "\n",
            "2022-12-29 17:48:11 (128 MB/s) - ‘faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.tar.gz’ saved [211996178/211996178]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Edit pipeline.config\n",
        "The pipeline.config file is in protocol buffer format, and we can use the config_util to change the settings we need."
      ],
      "metadata": {
        "id": "dmagfqHoAepA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from object_detection.utils import config_util\n",
        "\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.builders import model_builder"
      ],
      "metadata": {
        "id": "MZyXXBfx4tRZ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_config = f\"./{MODEL}/pipeline.config\"\n",
        "model_dir = f\"./{MODEL}/checkpoint\"\n",
        "\n",
        "config = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
        "\n",
        "model_config = config['model']\n",
        "model_config.faster_rcnn.num_classes = 2\n",
        "\n",
        "train_config = config['train_config']\n",
        "\n",
        "#Starting checkpoin location\n",
        "train_config.fine_tune_checkpoint = f\"/content/models/research/object_detection/pre-trained-models/{MODEL}/checkpoint/ckpt-0\"\n",
        "\n",
        "# GPU memory limits batch size\n",
        "train_config.batch_size = 32\n",
        "train_config.use_bfloat16 = False\n",
        "\n",
        "\n",
        "#...................$$$$$$$$$$$$$$$.................\n",
        "# some models use \"fine_tune\" rather than \"detection\" for fine-tuning\n",
        "train_config.fine_tune_checkpoint_type = \"detection\"\n",
        "# detection      ===> use all the layers as pretrained layers\n",
        "# classification ===> use only the classification layer as pretrained and initialize the weights randomly\n",
        "\n",
        "\n",
        "# set auto-augmentation\n",
        "del train_config.data_augmentation_options[:]\n",
        "\n",
        "a1=train_config.data_augmentation_options.add()\n",
        "a1.autoaugment_image.policy_name = \"v2\"\n",
        "\n",
        "# training dataset (80%)\n",
        "train_input_config = config['train_input_config']\n",
        "train_input_config.label_map_path = \"/content/data/labelmap.pbtxt\"\n",
        "train_input_config.tf_record_input_reader.input_path[:] = [\"/content/data/train/*.tfrecord\",\n",
        "                                                           \"/content/data/eval/*.tfrecord\"]  # have little data, so we use eval as train data\n",
        "# evaluation dataset (20%)                                                           \n",
        "eval_input_config = config['eval_input_config']\n",
        "eval_input_config.label_map_path = \"/content/data/labelmap.pbtxt\"\n",
        "eval_input_config.tf_record_input_reader.input_path[:] = [\"/content/data/test/*.tfrecord\"]\n",
        "\n",
        "# save pipeline.config file\n",
        "pipeline_proto = config_util.create_pipeline_proto_from_configs(config)\n",
        "config_util.save_pipeline_config(pipeline_config=pipeline_proto, \n",
        "                                 directory=f'/content/models/research/object_detection/pre-trained-models/{MODEL}')"
      ],
      "metadata": {
        "id": "Zqgdhby1AlQ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}