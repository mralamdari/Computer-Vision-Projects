{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Z_rpO7sX1a0L7XqXkgjDQJa93limDY6M",
      "authorship_tag": "ABX9TyM3/uUO43K3ffKB3P4HYmBi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Projects/blob/main/YOLO_v5_Wheet_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iiiATdLgeuHj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import tqdm\n",
        "import torch\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clone YOLOv5\n",
        "\n",
        "Clone the YOLOv5 repository, and move all the yolo files to your current working directory"
      ],
      "metadata": {
        "id": "44D6dYOOfils"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5\n",
        "!mv yolov5/* ./"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33iCddyafZmk",
        "outputId": "0f31beba-fb03-42a8-c338-83ef6390b8eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 15978, done.\u001b[K\n",
            "remote: Counting objects: 100% (147/147), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 15978 (delta 89), reused 113 (delta 70), pack-reused 15831\u001b[K\n",
            "Receiving objects: 100% (15978/15978), 14.60 MiB | 17.11 MiB/s, done.\n",
            "Resolving deltas: 100% (10962/10962), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "FPTbEpdyfqSN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle competitions download global-wheat-detection\n",
        "!unzip \\*.zip && rm *.zip\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "g-qmqkbHgKsz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/train.csv')\n",
        "bboxs = np.stack(df['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\n",
        "for i, column in enumerate(['x', 'y', 'w', 'h']):\n",
        "    df[column] = bboxs[:,i]\n",
        "df.drop(columns=['bbox'], inplace=True)\n",
        "df['x_center'] = df['x'] + df['w']/2\n",
        "df['y_center'] = df['y'] + df['h']/2\n",
        "df['classes'] = 0\n",
        "df = df[['image_id','x', 'y', 'w', 'h','x_center','y_center','classes']]\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "acVkLNE3hR8h",
        "outputId": "628ba98f-2ac5-4759-a6cc-004bf412b9f8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         image_id      x      y      w      h  x_center  y_center  classes\n",
              "0       b6ab77fd7  834.0  222.0   56.0   36.0     862.0     240.0        0\n",
              "1       b6ab77fd7  226.0  548.0  130.0   58.0     291.0     577.0        0\n",
              "2       b6ab77fd7  377.0  504.0   74.0  160.0     414.0     584.0        0\n",
              "3       b6ab77fd7  834.0   95.0  109.0  107.0     888.5     148.5        0\n",
              "4       b6ab77fd7   26.0  144.0  124.0  117.0      88.0     202.5        0\n",
              "...           ...    ...    ...    ...    ...       ...       ...      ...\n",
              "147788  5e0747034   64.0  619.0   84.0   95.0     106.0     666.5        0\n",
              "147789  5e0747034  292.0  549.0  107.0   82.0     345.5     590.0        0\n",
              "147790  5e0747034  134.0  228.0  141.0   71.0     204.5     263.5        0\n",
              "147791  5e0747034  430.0   13.0  184.0   79.0     522.0      52.5        0\n",
              "147792  5e0747034  875.0  740.0   94.0   61.0     922.0     770.5        0\n",
              "\n",
              "[147793 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0c352633-9e52-4fdd-9ba8-52d864a865f7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>w</th>\n",
              "      <th>h</th>\n",
              "      <th>x_center</th>\n",
              "      <th>y_center</th>\n",
              "      <th>classes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b6ab77fd7</td>\n",
              "      <td>834.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>862.0</td>\n",
              "      <td>240.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b6ab77fd7</td>\n",
              "      <td>226.0</td>\n",
              "      <td>548.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>291.0</td>\n",
              "      <td>577.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b6ab77fd7</td>\n",
              "      <td>377.0</td>\n",
              "      <td>504.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>414.0</td>\n",
              "      <td>584.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b6ab77fd7</td>\n",
              "      <td>834.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>109.0</td>\n",
              "      <td>107.0</td>\n",
              "      <td>888.5</td>\n",
              "      <td>148.5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b6ab77fd7</td>\n",
              "      <td>26.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>202.5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147788</th>\n",
              "      <td>5e0747034</td>\n",
              "      <td>64.0</td>\n",
              "      <td>619.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>666.5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147789</th>\n",
              "      <td>5e0747034</td>\n",
              "      <td>292.0</td>\n",
              "      <td>549.0</td>\n",
              "      <td>107.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>345.5</td>\n",
              "      <td>590.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147790</th>\n",
              "      <td>5e0747034</td>\n",
              "      <td>134.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>204.5</td>\n",
              "      <td>263.5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147791</th>\n",
              "      <td>5e0747034</td>\n",
              "      <td>430.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>184.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>522.0</td>\n",
              "      <td>52.5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147792</th>\n",
              "      <td>5e0747034</td>\n",
              "      <td>875.0</td>\n",
              "      <td>740.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>922.0</td>\n",
              "      <td>770.5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>147793 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0c352633-9e52-4fdd-9ba8-52d864a865f7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0c352633-9e52-4fdd-9ba8-52d864a865f7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0c352633-9e52-4fdd-9ba8-52d864a865f7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = list(set(df.image_id))  #Each picture contains lots of bounding boxes, so #BBs >> #Img_ids\n",
        "print(f'There are {len(index)} Images with {len(df.image_id)} Bounding Boxes')\n",
        "source = 'train'\n",
        "for img_name, bboxs_list in tqdm.tqdm(df.groupby('image_id')):\n",
        "    label_path = f'convertor/labels/' \n",
        "    os.makedirs(label_path, exist_ok=True)\n",
        "\n",
        "    with open(f'{label_path}/{img_name}.txt', 'w+') as f:\n",
        "        row = bboxs_list[['classes','x_center','y_center','w','h']].astype(float).values\n",
        "        row = row/1024\n",
        "        row = row.astype(str)\n",
        "        for j in range(len(row)):\n",
        "            text = ' '.join(row[j])\n",
        "            f.write(text)\n",
        "            f.write(\"\\n\")\n",
        "    \n",
        "    img_path = f'convertor/images/'\n",
        "    os.makedirs(img_path, exist_ok=True)\n",
        "    os.replace(f\"/content/train/{img_name}.jpg\", f'{img_path}{img_name}.jpg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqlkJs8BtfwO",
        "outputId": "0640989e-4a42-46e5-ebf7-7c4efeb5666d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 3373 Images with 147793 Bounding Boxes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3373/3373 [00:04<00:00, 740.97it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = 0.2\n",
        "img_path = 'convertor/images'\n",
        "label_path = f'convertor/labels' \n",
        "\n",
        "os.makedirs(img_path+'/train', exist_ok=True)\n",
        "os.makedirs(img_path+'/val', exist_ok=True)\n",
        "os.makedirs(label_path+'/train', exist_ok=True)\n",
        "os.makedirs(label_path+'/val', exist_ok=True)\n",
        "\n",
        "\n",
        "for i in os.listdir(img_path):\n",
        "  name = i[:-4]\n",
        "  if i == 'train' or i == 'val':\n",
        "    continue\n",
        "  if np.random.randn() < p:\n",
        "    os.rename(f'{img_path}/{name}.jpg', f'{img_path}/val/{name}.jpg')\n",
        "    os.rename(f'{label_path}/{name}.txt', f'{label_path}/val/{name}.txt')\n",
        "  else:\n",
        "    os.rename(f'{img_path}/{name}.jpg', f'{img_path}/train/{name}.jpg') \n",
        "    os.rename(f'{label_path}/{name}.txt', f'{label_path}/train/{name}.txt')  "
      ],
      "metadata": {
        "id": "0vZmgrAFHe3M"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Configuration Files\n",
        "\n",
        "For a yolo model, we need two config files with .yaml as extension; they contain:\n",
        "####1.the location of training & validation folders, the number of classes and class names\n",
        "\n",
        "####2.yolo model architecture"
      ],
      "metadata": {
        "id": "QBJE2cdJXPPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First YAML File"
      ],
      "metadata": {
        "id": "w9_nPtoJYtgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ./convertor/wheat0.yaml\n",
        "\n",
        "# COCO 2017 dataset http://cocodataset.org - first 128 training images\n",
        "# Download command:  python -c \"from yolov5.utils.google_utils import gdrive_download; gdrive_download('1n_oKgR81BJtqk75b00eAjdv03qVCQn2f','coco128.zip')\"\n",
        "# Train command: python train.py --data ./data/coco128.yaml\n",
        "# Dataset should be placed next to yolov5 folder:\n",
        "#   /parent_folder\n",
        "#     /coco128\n",
        "#     /yolov5\n",
        "\n",
        "\n",
        "# train and val datasets (image directory or *.txt file with image paths)\n",
        "train: ./convertor/images/train/\n",
        "val: ./convertor/images/val/\n",
        "\n",
        "# number of classes\n",
        "nc: 1\n",
        "\n",
        "# class names\n",
        "names: ['wheat']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jftKGwVEYt37",
        "outputId": "9790fdf5-f617-45a7-d56b-d63914e0ef21"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./convertor/wheat0.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second YAML File"
      ],
      "metadata": {
        "id": "F-jSQIxYYuIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ./convertor/yolov5x.yaml\n",
        "\n",
        "# parameters\n",
        "nc: 1  # number of classes\n",
        "depth_multiple: 1.33  # model depth multiple\n",
        "width_multiple: 1.25  # layer channel multiple\n",
        "\n",
        "# parameters\n",
        "nc: 1  # number of classes\n",
        "depth_multiple: 1.33  # model depth multiple\n",
        "width_multiple: 1.25  # layer channel multiple\n",
        "\n",
        "# anchors\n",
        "anchors:\n",
        "  - [116,90, 156,198, 373,326]  # P5/32\n",
        "  - [30,61, 62,45, 59,119]  # P4/16\n",
        "  - [10,13, 16,30, 33,23]  # P3/8\n",
        "\n",
        "# YOLOv5 backbone\n",
        "backbone:\n",
        "  # [from, number, module, args]\n",
        "  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n",
        "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
        "   [-1, 3, BottleneckCSP, [128]],\n",
        "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
        "   [-1, 9, BottleneckCSP, [256]],\n",
        "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
        "   [-1, 9, BottleneckCSP, [512]],\n",
        "   [-1, 1, Conv, [1024, 3, 2]], # 7-P5/32\n",
        "   [-1, 1, SPP, [1024, [5, 9, 13]]],\n",
        "  ]\n",
        "\n",
        "# YOLOv5 head\n",
        "head:\n",
        "  [[-1, 3, BottleneckCSP, [1024, False]],  # 9\n",
        "\n",
        "   [-1, 1, Conv, [512, 1, 1]],\n",
        "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
        "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
        "   [-1, 3, BottleneckCSP, [512, False]],  # 13\n",
        "\n",
        "   [-1, 1, Conv, [256, 1, 1]],\n",
        "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
        "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
        "   [-1, 3, BottleneckCSP, [256, False]],\n",
        "   [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1]],  # 18 (P3/8-small)\n",
        "\n",
        "   [-2, 1, Conv, [256, 3, 2]],\n",
        "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
        "   [-1, 3, BottleneckCSP, [512, False]],\n",
        "   [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1]],  # 22 (P4/16-medium)\n",
        "\n",
        "   [-2, 1, Conv, [512, 3, 2]],\n",
        "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
        "   [-1, 3, BottleneckCSP, [1024, False]],\n",
        "   [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1]],  # 26 (P5/32-large)\n",
        "\n",
        "   [[], 1, Detect, [nc, anchors]],  # Detect(P5, P4, P3)\n",
        "  ]"
      ],
      "metadata": {
        "id": "9oZLVD2vxhGg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ff2a6dd-d7c9-4f54-82e8-4fcceacb5b30"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./convertor/yolov5x.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Training\n",
        "\n",
        "!python /content/train.py --img 1024 --batch 16 --epochs 10 --data /content/convertor/wheat0.yaml --name yolov5x_fold0 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1V_jXbNadHo",
        "outputId": "2d30bd48-3cce-4eb7-d9b9-1ce0e54a335a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/content/convertor/wheat0.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=16, imgsz=1024, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=yolov5x_fold0, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mskipping check (not a git repository), for updates see https://github.com/ultralytics/yolov5\n",
            "YOLOv5 🚀 2023-6-11 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 71.9MB/s]\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:01<00:00, 9.69MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/convertor/labels/train... 1395 images, 0 backgrounds, 0 corrupt: 100% 1395/1395 [00:00<00:00, 1570.19it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/convertor/labels/train.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/convertor/labels/val... 1978 images, 0 backgrounds, 0 corrupt: 100% 1978/1978 [00:04<00:00, 401.27it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/convertor/labels/val.cache\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.73 anchors/target, 0.999 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Plotting labels to runs/train/yolov5x_fold0/labels.jpg... \n",
            "Image sizes 1024 train, 1024 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/yolov5x_fold0\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        0/9      8.71G    0.09003     0.3505          0        179       1024: 100% 88/88 [02:37<00:00,  1.79s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0% 0/62 [00:00<?, ?it/s]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   2% 1/62 [00:05<05:35,  5.51s/it]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 62/62 [01:46<00:00,  1.72s/it]\n",
            "                   all       1978      86668      0.601      0.635      0.633      0.241\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        1/9      10.7G    0.06279      0.332          0        184       1024: 100% 88/88 [02:17<00:00,  1.56s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 62/62 [01:06<00:00,  1.07s/it]\n",
            "                   all       1978      86668      0.367       0.72      0.379      0.143\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        2/9      10.7G    0.05727     0.3343          0        146       1024: 100% 88/88 [02:20<00:00,  1.59s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 62/62 [00:58<00:00,  1.06it/s]\n",
            "                   all       1978      86668      0.667      0.734      0.766      0.355\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        3/9      10.7G     0.0535     0.3259          0        277       1024: 100% 88/88 [02:16<00:00,  1.55s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 62/62 [00:57<00:00,  1.09it/s]\n",
            "                   all       1978      86668      0.896      0.844      0.907      0.442\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        4/9      10.7G    0.04767     0.3293          0         92       1024: 100% 88/88 [02:15<00:00,  1.54s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 62/62 [00:56<00:00,  1.09it/s]\n",
            "                   all       1978      86668      0.896      0.844       0.91       0.43\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        5/9      10.7G    0.04466     0.3158          0        170       1024: 100% 88/88 [02:14<00:00,  1.53s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 62/62 [00:57<00:00,  1.07it/s]\n",
            "                   all       1978      86668      0.896      0.857      0.914      0.474\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        6/9      10.7G    0.04274     0.3116          0        122       1024: 100% 88/88 [02:17<00:00,  1.56s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 62/62 [00:55<00:00,  1.12it/s]\n",
            "                   all       1978      86668      0.907      0.871      0.926      0.487\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        7/9      10.7G    0.04121     0.3166          0        163       1024: 100% 88/88 [02:18<00:00,  1.57s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 62/62 [00:55<00:00,  1.12it/s]\n",
            "                   all       1978      86668       0.91       0.88      0.932      0.505\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        8/9      10.7G    0.04011     0.3113          0        275       1024: 100% 88/88 [02:13<00:00,  1.52s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 62/62 [00:54<00:00,  1.14it/s]\n",
            "                   all       1978      86668      0.914       0.88      0.934      0.517\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "        9/9      10.7G    0.03934     0.3126          0        226       1024: 100% 88/88 [02:13<00:00,  1.51s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 62/62 [00:56<00:00,  1.11it/s]\n",
            "                   all       1978      86668      0.914      0.884      0.936      0.522\n",
            "\n",
            "10 epochs completed in 0.564 hours.\n",
            "Optimizer stripped from runs/train/yolov5x_fold0/weights/last.pt, 14.8MB\n",
            "Optimizer stripped from runs/train/yolov5x_fold0/weights/best.pt, 14.8MB\n",
            "\n",
            "Validating runs/train/yolov5x_fold0/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   6% 4/62 [00:24<07:36,  7.86s/it]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 62/62 [01:52<00:00,  1.81s/it]\n",
            "                   all       1978      86668      0.914      0.884      0.936      0.521\n",
            "Results saved to \u001b[1mruns/train/yolov5x_fold0\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get a list of experiments\n",
        "exp_list = os.listdir('/content/runs/train')\n",
        "\n",
        "# Get the latest exp.\n",
        "\n",
        "exp = exp_list[-1]\n",
        "\n",
        "exp"
      ],
      "metadata": {
        "id": "BIrNfqTGbFZI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6d559ee6-c014-4d9f-da42-70ec3f692132"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'yolov5x_fold0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Visualization\n",
        "\n",
        "!python /content/detect.py --weights runs/train/yolov5x_fold0/weights/best.pt --img 1024  --source /content/convertor/images/val7\n",
        "\n",
        "\n",
        "# Display the contents of the \"exp\" folder\n",
        "os.listdir(f'/content/runs/train/{exp}')\n",
        "\n",
        "# One batch of val images with true labels\n",
        "\n",
        "plt.figure(figsize = (15, 15))\n",
        "plt.imshow(plt.imread(f'runs/train/{exp}/val_batch1_pred.jpg'))"
      ],
      "metadata": {
        "id": "TFfp2c-gT8Wy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a229b1fe-9cfd-467a-d1c6-ef909982fcb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['runs/train/yolov5x_fold0/weights/best.pt'], source=/content/convertor/images/val7, data=data/coco128.yaml, imgsz=[1024, 1024], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 2023-6-11 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0zLCpK_SI_xj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}