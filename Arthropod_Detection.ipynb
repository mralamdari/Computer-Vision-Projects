{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1EkPO3-a7zWEBBox_OayTsTNDMvTNzzuS",
      "authorship_tag": "ABX9TyMX5prlPTFgdrginccenPDP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Projects/blob/main/Arthropod_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Arthropod Taxonomy Orders Object Detection Dataset](https://www.kaggle.com/datasets/mistag/arthropod-taxonomy-orders-object-detection-dataset/)"
      ],
      "metadata": {
        "id": "O3md6o47lY_f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc6VQxrK6Zz3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib as mpl\n",
        "import concurrent.futures\n",
        "from threading import Lock\n",
        "import json, math, random, os, pprint,sys\n",
        "from matplotlib import pyplot as plt\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "pp = pprint.PrettyPrinter()\n",
        "!pip install --quiet tf-models-official==2.9.2\n",
        "from tensorflow_models.vision import box_ops as boxutils\n",
        "import boxutils as box\n",
        "\n",
        "sys.path.append('PATH_TO_TENSORFLOW_OBJECT_DETECTION_FOLDER')# load all the metadata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR']='/content/drive/MyDrive/'\n",
        "!kaggle datasets download -d mistag/arthropod-taxonomy-orders-object-detection-dataset\n",
        "!unzip *.zip && rm *.zip"
      ],
      "metadata": {
        "id": "OEQKeqZl8mgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGES_PER_SHARD = 481\n",
        "TARGET_WIDTH = 1024\n",
        "\n",
        "CLASSES = ['Lepidoptera', 'Hymenoptera', 'Hemiptera', 'Odonata', 'Diptera', 'Araneae', 'Coleoptera']\n",
        "\n",
        "RAW_CLASSES = CLASSES + ['_truncated', '_blurred', '_occluded']"
      ],
      "metadata": {
        "id": "Msyvk4_ZZPex"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Progress:\n",
        "    \"\"\"Text mode progress bar.\n",
        "    Usage:\n",
        "            p = Progress(30)\n",
        "            p.step()\n",
        "            p.step()\n",
        "            p.step(start=True) # to restart form 0%\n",
        "    The progress bar displays a new header at each restart.\"\"\"\n",
        "    def __init__(self, maxi, size=100, msg=\"\"):\n",
        "        \"\"\"\n",
        "        :param maxi: the number of steps required to reach 100%\n",
        "        :param size: the number of characters taken on the screen by the progress bar\n",
        "        :param msg: the message displayed in the header of the progress bat\n",
        "        \"\"\"\n",
        "        self.maxi = maxi\n",
        "        self.p = self.__start_progress(maxi)()  # () to get the iterator from the generator\n",
        "        self.header_printed = False\n",
        "        self.msg = msg\n",
        "        self.size = size\n",
        "        self.lock = Lock()\n",
        "\n",
        "    def step(self, reset=False):\n",
        "        with self.lock:\n",
        "            if reset:\n",
        "                self.__init__(self.maxi, self.size, self.msg)\n",
        "            if not self.header_printed:\n",
        "                self.__print_header()\n",
        "            next(self.p)\n",
        "\n",
        "    def __print_header(self):\n",
        "        print()\n",
        "        format_string = \"0%{: ^\" + str(self.size - 6) + \"}100%\"\n",
        "        print(format_string.format(self.msg))\n",
        "        self.header_printed = True\n",
        "\n",
        "    def __start_progress(self, maxi):\n",
        "        def print_progress():\n",
        "            # Bresenham's algorithm. Yields the number of dots printed.\n",
        "            # This will always print 100 dots in max invocations.\n",
        "            dx = maxi\n",
        "            dy = self.size\n",
        "            d = dy - dx\n",
        "            for x in range(maxi):\n",
        "                k = 0\n",
        "                while d >= 0:\n",
        "                    print('=', end=\"\", flush=True)\n",
        "                    k += 1\n",
        "                    d -= dx\n",
        "                d += dy\n",
        "                yield k\n",
        "\n",
        "        return print_progress\n",
        "\n",
        "    \n",
        "def no_decorations(ax):\n",
        "    ax.axes.get_xaxis().set_visible(False)\n",
        "    ax.axes.get_yaxis().set_visible(False)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['bottom'].set_visible(False)\n",
        "    ax.spines['left'].set_visible(False)\n",
        "    \n",
        "\n",
        "def display_detections(images, offsets, resizes, detections, classnames, ground_truth_boxes=[]):\n",
        "    # scale and offset the detected boxes back to original image coordinates\n",
        "    boxes   = [[ (x,y,w,h)  for _, x, y, w, h, score, klass in detection_list] for detection_list in detections]\n",
        "    boxes   = [[ (x-ofs[1], y-ofs[0], w, h) for x,y,w,h in boxlist ] for boxlist, ofs in zip(boxes, offsets)]\n",
        "    boxes   = [[ (x*rsz, y*rsz, w*rsz, h*rsz) for x,y,w,h in boxlist ] for boxlist, rsz in zip(boxes, resizes)]\n",
        "    classes = [[ int(klass) for _, x, y, w, h, score, klass in detection_list] for detection_list in detections]\n",
        "    scores  = [[ score      for _, x, y, w, h, score, klass in detection_list] for detection_list in detections]\n",
        "    display_with_boxes(images, boxes, classes, scores, classnames, ground_truth_boxes)\n",
        "    \n",
        "    \n",
        "# images, boxes and classes must have the same number of elements\n",
        "# scores can be en empty list []. If it is not empty, it must also\n",
        "# have the same number of elements.\n",
        "# classnames is the list of possible classes (strings)\n",
        "def display_with_boxes(images, boxes, classes, scores, classnames, ground_truth_boxes=[]):\n",
        "    N = len(images)\n",
        "    sqrtN = int(math.ceil(math.sqrt(N)))\n",
        "    aspect = sum([im.shape[1]/im.shape[0] for im in images])/len(images) # mean aspect ratio of images\n",
        "    fig = plt.figure(figsize=(15,15/aspect), frameon=False)\n",
        "    \n",
        "    for k in range(N):\n",
        "        ax = plt.subplot(sqrtN, sqrtN, k+1)\n",
        "        no_decorations(ax)\n",
        "        plt.imshow(images[k])\n",
        "        \n",
        "        if ground_truth_boxes:\n",
        "            for box in ground_truth_boxes[k]:\n",
        "                x, y, w, h = (box[0], box[1], box[2]-box[0], box[3]-box[1]) # convert x1 y1 x2 y2 into xywh\n",
        "                #x, y, w, h = (box[0], box[1], box[2], box[3])\n",
        "                rect = mpl.patches.Rectangle((x, y),w,h,linewidth=4,edgecolor='#FFFFFFA0',facecolor='none')\n",
        "                ax.add_patch(rect)\n",
        "\n",
        "        for i, (box, klass) in enumerate(zip(boxes[k], classes[k])):\n",
        "            x, y, w, h = (box[0], box[1], box[2]-box[0], box[3]-box[1]) # convert x1 y1 x2 y2 into xywh\n",
        "            #x, y, w, h = (box[0], box[1], box[2], box[3])\n",
        "            #label = classnames[klass-1] # predicted classes are 1-based\n",
        "            label = classnames[klass]\n",
        "            if scores:\n",
        "                label += ' ' + str(int(scores[k][i]*100)) + '%' \n",
        "            rect = mpl.patches.Rectangle((x, y),w,h,linewidth=4,edgecolor='#00000080',facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "            rect = mpl.patches.Rectangle((x, y),w,h,linewidth=2,edgecolor='#FFFF00FF',facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "            plt.text(x, y, label, size=16, ha=\"left\", va=\"top\", color='#FFFF00FF',\n",
        "                     bbox=dict(boxstyle=\"round\", ec='#00000080', fc='#0000004E', linewidth=3) )\n",
        "            plt.text(x, y, label, size=16, ha=\"left\", va=\"top\", color='#FFFF00FF',\n",
        "                     bbox=dict(boxstyle=\"round\", ec='#FFFF00FF', fc='#0000004E', linewidth=1.5) )\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(wspace=0.02, hspace=0.02)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "w-S7WrgfBJaT"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/content/ArTaxOr/Araneae'\n",
        "json_filename_pattern = '/content/ArTaxOr/Araneae/annotations/*.json'\n",
        "jpeg_filename_pattern = '/content/ArTaxOr/Araneae/*.jpg'\n",
        "\n",
        "def load_json(filename, p):\n",
        "    p.step()\n",
        "    with tf.io.gfile.GFile(filename, 'r') as f:\n",
        "        return json.load(f)\n",
        "    \n",
        "def filename_key(filename):\n",
        "    path, filename = os.path.split(filename)\n",
        "    dirname = os.path.split(path)[1]\n",
        "    return filename\n",
        "    # return os.path.join(dirname, filename)\n",
        "    \n",
        "def load_metadata(filename_pattern, jpeg_filename_pattern):\n",
        "    print(\"Scanning directory...\", end=' ')\n",
        "    json_filenames = tf.io.gfile.glob(json_filename_pattern)\n",
        "    jpeg_filenames = tf.io.gfile.glob(jpeg_filename_pattern)\n",
        "    print(f\"found {len(json_filenames)} metadata files and {len(jpeg_filenames)} image files.\")\n",
        "    print(\"Loading metadata\")\n",
        "    p = Progress(len(json_filenames))\n",
        "    with concurrent.futures.ThreadPoolExecutor() as exe:\n",
        "        data = exe.map(lambda x: load_json(x,p), json_filenames)\n",
        "    # data as a dictionary for easier cross-referencing\n",
        "    data = {filename_key(d['asset']['path']):d for d in data}\n",
        "    return data, jpeg_filenames\n",
        "\n",
        "RAW_METADATA, JPEG_FILENAMES = load_metadata(json_filename_pattern, jpeg_filename_pattern)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tvvJWpPA5-a",
        "outputId": "35f70e9d-2b24-4d8e-8efd-c1780e30bd43"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanning directory... found 2418 metadata files and 2418 image files.\n",
            "Loading metadata\n",
            "\n",
            "0%                                                                                              100%\n",
            "===================================================================================================="
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = JPEG_FILENAMES[0]\n",
        "RAW_METADATA[name[-16:]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31CkjYPus83g",
        "outputId": "381e3cb7-d1b4-4ee7-c515-95c4d96ad492"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'asset': {'format': 'jpg',\n",
              "  'id': '1a34363b7db2be18a6c282c223c41bf2',\n",
              "  'name': '4bf03aa7b9c1.jpg',\n",
              "  'path': 'file:F:/ArTaxOr/Araneae/4bf03aa7b9c1.jpg',\n",
              "  'size': {'width': 2048, 'height': 1533},\n",
              "  'state': 2,\n",
              "  'type': 1},\n",
              " 'regions': [{'id': 'XRBvH-p4D',\n",
              "   'type': 'RECTANGLE',\n",
              "   'tags': ['Araneae'],\n",
              "   'boundingBox': {'height': 1138.0028735632184,\n",
              "    'width': 844.15770609319,\n",
              "    'left': 567.9713261648745,\n",
              "    'top': 154.18103448275863},\n",
              "   'points': [{'x': 567.9713261648745, 'y': 154.18103448275863},\n",
              "    {'x': 1412.1290322580646, 'y': 154.18103448275863},\n",
              "    {'x': 1412.1290322580646, 'y': 1292.183908045977},\n",
              "    {'x': 567.9713261648745, 'y': 1292.183908045977}]}],\n",
              " 'version': '2.1.0'}"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that there is a model garden safe way of computing image ids.\n",
        "# In the end, this does not matter. The ids are never used, but they must still be processed,\n",
        "# otherwise the Model Garden data loader will crash on them.\n",
        "\n",
        "# copied from Model Garden code\n",
        "# = official/vision/beta/dataloaders/utils process_source_id()\n",
        "\n",
        "def model_garden__str_to_int64(s):\n",
        "    return tf.strings.to_number(s, tf.int64).numpy()\n",
        "\n",
        "def compute_id_bytestring(s):\n",
        "    \n",
        "    computed_id = (int('0x' + s[:16], 16) ^ int('0x' + s[16:], 16)) & 0x0FFFFFFFFFFFFFFF\n",
        "    return str(computed_id).encode('utf-8')\n",
        "\n",
        "raw_ids = []\n",
        "cnv_ids = []\n",
        "cnv_cnv_ids = []\n",
        "\n",
        "for i, key in enumerate(RAW_METADATA):\n",
        "    iid = RAW_METADATA[key]['asset']['id']\n",
        "    raw_ids.append(iid)\n",
        "    iid = compute_id_bytestring(iid)\n",
        "    #print(\"computed:\", iid)\n",
        "    cnv_ids.append(iid)\n",
        "    iid = model_garden__str_to_int64(iid)\n",
        "    cnv_cnv_ids.append(iid)\n",
        "\n",
        "print(f\"Original ids: {len(raw_ids)} Uniques:{len(set(raw_ids))} Collisions:{len(raw_ids) - len(set(raw_ids))}\")\n",
        "print(f\"Original ids: {len(cnv_ids)} Uniques:{len(set(cnv_ids))} Collisions:{len(raw_ids) - len(set(cnv_ids))}\")\n",
        "print(f\"Computed ids as converted by Model Garden:{len(cnv_cnv_ids)}, Uniques: {len(set(cnv_cnv_ids))}, Collisions: {len(raw_ids) - len(set(cnv_cnv_ids))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4xyoVqD0tuU",
        "outputId": "45e13c05-50ba-4b89-9a10-66757f3e6ab0"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original ids: 2418 Uniques:2418 Collisions:0\n",
            "Original ids: 2418 Uniques:2418 Collisions:0\n",
            "Computed ids as converted by Model Garden:2418, Uniques: 2418, Collisions: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# collect weird aspec ratio images and very large images\n",
        "oddities = [d['asset']['path'] for k,d in RAW_METADATA.items() if d['asset']['size']['width']>5400]\n",
        "oddities += [d['asset']['path'] for k,d in RAW_METADATA.items() if d['asset']['size']['height']>5000]\n",
        "oddities2 = [d['asset']['path'] for k,d in RAW_METADATA.items() if d['asset']['size']['height']/d['asset']['size']['width']>1.9]\n",
        "oddities2 += [d['asset']['path'] for k,d in RAW_METADATA.items() if d['asset']['size']['width']/d['asset']['size']['height']>2.0]\n",
        "\n",
        "nice_pics = ['/ArTaxOr/Lepidoptera/e7d7b4678088.jpg','/ArTaxOr/Lepidoptera/e74f298859ff.jpg','/ArTaxOr/Coleoptera/92c9a15e7362.jpg',\n",
        "             '/ArTaxOr/Coleoptera/a1824522fddc.jpg','/ArTaxOr/Hymenoptera/7188c0cc8c9d.jpg','/ArTaxOr/Lepidoptera/b7197aead30b.jpg',\n",
        "             '/ArTaxOr/Lepidoptera/dfc9ece476e6.jpg','/ArTaxOr/Araneae/a1488eb130e3.jpg','/ArTaxOr/Coleoptera/39c0eabccc41.jpg']\n",
        "alt_pics = ['/ArTaxOr/Araneae/3c6491416c3f.jpg','/ArTaxOr/Araneae/81ff08857d15.jpg',\n",
        "            '/ArTaxOr/Hymenoptera/f8f10bc28f5b.jpg','/ArTaxOr/Lepidoptera/e314c31efafd.jpg']\n",
        "\n",
        "#filenames = tf.io.gfile.glob('../input/arthropod-taxonomy-orders-object-detection-dataset/ArTaxOr/*/*jpg')\n",
        "#filenames = nice_pics\n",
        "filenames = oddities+oddities2\n",
        "#filenames = alt_pics\n",
        "\n",
        "# display a couple of images from the raw_metadata with their bounding boxes\n",
        "images = []\n",
        "bboxes = []\n",
        "taglists = []\n",
        "random.shuffle(filenames)\n",
        "for filename in filenames[:4]:\n",
        "    print(DATA_PATH, filename_key(filename), filename)\n",
        "    filepath = os.path.join(DATA_PATH, filename_key(filename))\n",
        "    d = RAW_METADATA[filename_key(filename)]\n",
        "    images.append(tf.image.decode_jpeg(tf.io.read_file(filepath)))\n",
        "    bbxs = [region['boundingBox'] for region in d['regions']]\n",
        "    # xywh to yxyx conversion\n",
        "    bbxs = [[box['left'],box['top'], box['left']+box['width'], box['top']+box['height']] for box in bbxs]\n",
        "    #tags = [region['tags'] for region in d['regions']] # all tags\n",
        "    tags = [region['tags'][0] for region in d['regions']] # first tag only\n",
        "    tags = [CLASSES.index(t) for t in tags] \n",
        "    bboxes.append(bbxs)\n",
        "    taglists.append(tags)\n",
        "\n",
        "display_with_boxes(images, bboxes, taglists, None, CLASSES, ground_truth_boxes=[])"
      ],
      "metadata": {
        "id": "HOgvmClK27Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Nb entries:', len(RAW_METADATA))\n",
        "print('Nb images:', len(JPEG_FILENAMES))\n",
        "assert(len(RAW_METADATA) == len(JPEG_FILENAMES))\n",
        "\n",
        "formats = set([d['asset']['format'] for k,d in RAW_METADATA.items()])\n",
        "print('Image formats:', formats)\n",
        "assert(len(formats)==1 and list(formats)[0]=='jpg')\n",
        "\n",
        "states = set([d['asset']['state'] for k,d in RAW_METADATA.items()])\n",
        "print('Image states(?):', states)\n",
        "types = set([d['asset']['type'] for k,d in RAW_METADATA.items()])\n",
        "print('Image types(?):', types)\n",
        "\n",
        "widths = [d['asset']['size']['width'] for k,d in RAW_METADATA.items()]\n",
        "print(f'Images widths range from {min(widths)} to {max(widths)}')\n",
        "heights = [d['asset']['size']['height'] for k,d in RAW_METADATA.items()]\n",
        "print(f'Images heights range from {min(heights)} to {max(heights)}')\n",
        "\n",
        "aspect_ratios = [w/h for w,h in zip(widths, heights)]\n",
        "print(f'Images aspect ratios range from {min(aspect_ratios)} to {max(aspect_ratios)}')\n",
        "\n",
        "nbbox = [len(d['regions']) for k,d in RAW_METADATA.items()]\n",
        "print(f'Nb of bounding boxes from {min(nbbox)} to {max(nbbox)}, average {sum(nbbox)/len(nbbox):.3}')\n",
        "\n",
        "region_types = [set([t['type'] for t in d['regions']]) for k,d in RAW_METADATA.items()]\n",
        "region_types = set().union(*region_types)\n",
        "print('Region types:', region_types)\n",
        "assert(len(region_types)==1 and list(region_types)[0]=='RECTANGLE')\n",
        "\n",
        "region_tags = [[t['tags'] for t in d['regions']] for k,d in RAW_METADATA.items()]\n",
        "region_tags = [sum(l,[]) for l in region_tags] # concat lists of tags across multiple regions\n",
        "region_tags = set(sum(region_tags, [])) # concat all and make a set\n",
        "print('Region tags:', region_tags)\n",
        "\n",
        "bbox_width = [[t['boundingBox']['width'] for t in d['regions']] for k,d in RAW_METADATA.items()]\n",
        "bbox_width = sum(bbox_width, []) # flatten the list\n",
        "print(f'Bounding box widths range from {min(bbox_width)} to {max(bbox_width)}')\n",
        "\n",
        "bbox_height = [[t['boundingBox']['height'] for t in d['regions']] for k,d in RAW_METADATA.items()]\n",
        "bbox_height = sum(bbox_height, []) # flatten the list\n",
        "print(f'Bounding box height range from {min(bbox_height)} to {max(bbox_height)}')\n",
        "\n",
        "bbox_left = [[t['boundingBox']['left'] for t in d['regions']] for k,d in RAW_METADATA.items()]\n",
        "bbox_left = sum(bbox_left, []) # flatten the list\n",
        "print(f'Bounding box left range from {min(bbox_left)} to {max(bbox_left)}')\n",
        "\n",
        "bbox_top = [[t['boundingBox']['top'] for t in d['regions']] for k,d in RAW_METADATA.items()]\n",
        "bbox_top = sum(bbox_top, []) # flatten the list\n",
        "print(f'Bounding box top range from {min(bbox_top)} to {max(bbox_top)}')\n",
        "\n",
        "# compute aspect ratios in landscape mode\n",
        "landscape_aspect_ratios = [w/h if w>h else h/w for w,h in zip(widths, heights)]\n",
        "print(f'Images landscape aspect ratios range from {min(landscape_aspect_ratios)} to {max(landscape_aspect_ratios)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i33c85W39HQ",
        "outputId": "b7585529-f073-4e86-b245-3745f6c7f274"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nb entries: 2418\n",
            "Nb images: 2418\n",
            "Image formats: {'jpg'}\n",
            "Image states(?): {2}\n",
            "Image types(?): {1}\n",
            "Images widths range from 500 to 5500\n",
            "Images heights range from 375 to 4272\n",
            "Images aspect ratios range from 0.4044418468731736 to 2.3703703703703702\n",
            "Nb of bounding boxes from 1 to 34, average 1.11\n",
            "Region types: {'RECTANGLE'}\n",
            "Region tags: {'Araneae', 'Lepidoptera', '_occluded', '_blurred', 'Coleoptera', 'Diptera', '_truncated', 'Hemiptera', 'Hymenoptera'}\n",
            "Bounding box widths range from 29.42528735632184 to 3861.1984838042727\n",
            "Bounding box height range from 29.42528735632184 to 3453.654601861427\n",
            "Bounding box left range from 0 to 3980.0137835975193\n",
            "Bounding box top range from 0 to 3089.1127197518094\n",
            "Images landscape aspect ratios range from 1.0 to 2.472543352601156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2,ax3,ax4) = plt.subplots(1,4, figsize=(15,3))\n",
        "ax1.set_title(\"aspect ratios\")\n",
        "ax1.hist(aspect_ratios)\n",
        "ax2.set_title(\"landscape aspect ratios\")\n",
        "ax2.hist(landscape_aspect_ratios)\n",
        "ax3.set_title(\"widths\")\n",
        "ax3.hist(widths)\n",
        "ax4.set_title(\"heights\")\n",
        "ax4.hist(heights)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "JjZRmW7L4yKk",
        "outputId": "9ec23dc6-82f7-4e35-b6f6-b39b78b6b9e3"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x216 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def metadataset_from_image_filenames(filenames):\n",
        "    values = [RAW_METADATA[filename_key(f)] for f in filenames]\n",
        "\n",
        "    values = {'id': [filename_key(val['asset']['id']) for val in values],\n",
        "              'fname': [os.path.join(DATA_PATH, filename_key(val['asset']['path'])) for val in values],\n",
        "              'width':[val['asset']['size']['width'] for val in values],\n",
        "              'height':[val['asset']['size']['height'] for val in values],\n",
        "              # xywh to yxyx conversion ?\n",
        "              # internal box format: x1 y1 x2 y2 in pixel coordinates\n",
        "              'boxes': tf.ragged.constant([[(item['boundingBox']['left'],\n",
        "                                             item['boundingBox']['top'],\n",
        "                                             item['boundingBox']['left']+item['boundingBox']['width'],\n",
        "                                             item['boundingBox']['top']+item['boundingBox']['height']) \n",
        "                                            for item in val['regions']]\n",
        "                                           for val in values], ragged_rank=1), # must specify ragged rank, othewise tf.ragged.constant\n",
        "                                                                               # does not detect that the last dim is 4 [x1 y1 x2 y2]\n",
        "               'tags': tf.ragged.constant([[item['tags']\n",
        "                                            for item in val['regions']]\n",
        "                                           for val in values]),\n",
        "               'classes': tf.ragged.constant([[[CLASSES.index(tag) for tag in item['tags'] if tag in CLASSES]\n",
        "                                            for item in val['regions']]\n",
        "                                           for val in values])\n",
        "              }\n",
        "\n",
        "    metadataset = tf.data.Dataset.from_tensor_slices(values)\n",
        "    return metadataset"
      ],
      "metadata": {
        "id": "TROu85Ab6I57"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_img(metadata):\n",
        "    compressed_data = tf.io.read_file(metadata['fname'])\n",
        "    image = tf.image.decode_jpeg(compressed_data, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32) # convert from uint8 [0,255] float [0,1)\n",
        "    return image, metadata\n",
        "\n",
        "def keep_one_tag_per_box(tags):\n",
        "    # Keeping first tag only. Other tags are not bug names but qualifiesrs like '_truncated', '_blurred', '_occluded'\n",
        "    # Input: one tag list per box: ex: [['Odonata', '_blurred'], ['Hymenoptera']] for 2 boxes\n",
        "    # This works for 0 boxes (tested)\n",
        "    tags = tags[:, 0:1] # Cannot index into ragged dimension, must do this.\n",
        "    tags = tags.merge_dims(-2,-1) # equivalent to tf.squeeze(tags, axis=-1) for ragged tensor\n",
        "    return tags\n",
        "\n",
        "# Resize images to MAX_WIDTH if they are larger. Adjust bounding boxes accordingly.\n",
        "def resize_image_and_boxes(im, metadata):\n",
        "    boxes = tf.cast(metadata['boxes'], tf.float32) # the number of boxes is ragged but for one image it is constant\n",
        "    image_size = tf.stack([metadata['width'], metadata['height']])\n",
        "    \n",
        "    scale_factor = tf.cast(TARGET_WIDTH, tf.float32) / tf.cast(image_size[0], tf.float32)\n",
        "    scale_factor = tf.math.minimum(scale_factor, 1.0)\n",
        "    new_image_size = tf.stack([tf.math.round(scale_factor * tf.cast(image_size[0], tf.float32)), \n",
        "                                tf.math.round(scale_factor * tf.cast(image_size[1], tf.float32))])\n",
        "    # resize image\n",
        "    im = tf.image.resize(im, tf.stack([image_size[1], TARGET_WIDTH]), preserve_aspect_ratio=True)\n",
        "    \n",
        "    # resize boxes\n",
        "    boxes = box.scale_rois(boxes, tf.math.reciprocal(tf.cast(image_size, tf.float32)))\n",
        "    boxes = box.scale_rois(boxes, new_image_size)\n",
        "    #boxes = tf.cast(tf.round(boxes), tf.int32)\n",
        "    \n",
        "    # metadata\n",
        "    new_image_size = tf.cast(new_image_size, tf.int32)\n",
        "    classes = metadata['classes']\n",
        "    tags = metadata['tags']\n",
        "    ids = metadata['id'] \n",
        "    \n",
        "    return im, new_image_size, ids, boxes, tags, classes"
      ],
      "metadata": {
        "id": "eefp9xig6gWk"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filenames = JPEG_FILENAMES\n",
        "#filenames = alt_pics\n",
        "filenames = (oddities+oddities2).copy()\n",
        "random.shuffle(filenames)\n",
        "\n",
        "metadataset = metadataset_from_image_filenames(filenames)\n",
        "dataset = metadataset.map(decode_img, num_parallel_calls=AUTO)\n",
        "dataset = dataset.map(resize_image_and_boxes, num_parallel_calls=AUTO)\n",
        "\n",
        "dataset_iterator = iter(dataset.apply(tf.data.experimental.dense_to_ragged_batch(4)))"
      ],
      "metadata": {
        "id": "4OEPiMtX6k7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, new_image_size, names, boxes, tags, classes  = next(dataset_iterator)\n",
        "images = [im.numpy() for im in images] # must do this for ragged\n",
        "classes = [[classlist[0] for classlist in detection] for detection in classes] # keep first class only\n",
        "display_with_boxes(images, boxes, classes, None, CLASSES, ground_truth_boxes=[])"
      ],
      "metadata": {
        "id": "C7mfFDxK6np0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames_to_process = JPEG_FILENAMES.copy()\n",
        "\n",
        "OUTPUT_DIR = '../../arthropod-data-tfrec'\n",
        "OUTPUT_NAME_PREFIX = 'arthropods_'\n",
        "\n",
        "print(\"TFRecords output directory:\", OUTPUT_DIR) \n",
        "print(\"Images resizing target size:\", TARGET_WIDTH, 'px')\n",
        "print(\"Number of images:\", len(filenames_to_process))\n",
        "NB_SHARDS = -(-len(filenames_to_process)//IMAGES_PER_SHARD) # -- trick rounds up\n",
        "print(f\"Output sharded into {NB_SHARDS} files with {IMAGES_PER_SHARD} images per file, {len(filenames_to_process)-(NB_SHARDS-1)*IMAGES_PER_SHARD} images in last file\")"
      ],
      "metadata": {
        "id": "EC7AJ4HWBcES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TFRecord helper functions\n",
        "def _bytes_feature(value):\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value)) # WARNING: this expects a list of byte strings, not a list of bytes!\n",
        "def _float_feature(value):\n",
        "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
        "def _int_feature(value):\n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
        "\n",
        "def to_tfrecord(tfrec_filewriter, img_bytes, width, height, ids, boxes, labels):\n",
        "    feature = {\n",
        "        \"image/encoded\": _bytes_feature([img_bytes]), # compressed image bytes\n",
        "        \"image/source_id\": _bytes_feature([ids]),    # string\n",
        "        \"image/width\": _int_feature([width]),         # image width\n",
        "        \"image/height\": _int_feature([height]),       # image height\n",
        "        \"image/object/bbox/xmin\": _float_feature(boxes[:,0]),\n",
        "        \"image/object/bbox/ymin\": _float_feature(boxes[:,1]),\n",
        "        \"image/object/bbox/xmax\": _float_feature(boxes[:,2]),\n",
        "        \"image/object/bbox/ymax\": _float_feature(boxes[:,3]),\n",
        "        #\"nb_tags\": _int_feature(nb_tags),     # nb of tags per box\n",
        "        \"image/object/class/label\": _int_feature(labels) # flat list of labels\n",
        "    }\n",
        "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n",
        "def recompress_jpeg(image):\n",
        "    image = tf.image.convert_image_dtype(image, tf.uint8) # convert from float [0,1) to uint8 [0,255]\n",
        "    return tf.image.encode_jpeg(image, optimize_size=True, chroma_downsampling=False)\n",
        "\n",
        "# Model Garden specific format adjustments\n",
        "\n",
        "# Model Garden models expect normalized box coordinates in [0..1] range\n",
        "def normalize_boxes(boxes, image_size):\n",
        "    boxes = box.scale_rois(boxes, tf.math.reciprocal(tf.cast(image_size, tf.float32)))\n",
        "    return boxes\n",
        "\n",
        "# Model Garden models expect target classes to be 1-based because they reserve class 0 for backgrounds.\n",
        "def labels_1_based(labels):\n",
        "    return labels+1"
      ],
      "metadata": {
        "id": "q0Telv4YBi3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(filenames_to_process)\n",
        "metadataset = metadataset_from_image_filenames(filenames_to_process)\n",
        "dataset = metadataset.map(decode_img, num_parallel_calls=AUTO)\n",
        "dataset = dataset.map(resize_image_and_boxes, num_parallel_calls=AUTO)\n",
        "\n",
        "# apply the Model Garden specific format adjustments\n",
        "dataset = dataset.map(lambda image, size, ids, boxes, tags, classes:\n",
        "                      (recompress_jpeg(image), size, ids,\n",
        "                       normalize_boxes(boxes, size), # Model Garden models expect normalized box coordinates in [0..1] range\n",
        "                       labels_1_based(keep_one_tag_per_box(classes))), num_parallel_calls=AUTO)\n",
        "\n",
        "dataset = dataset.apply(tf.data.experimental.dense_to_ragged_batch(IMAGES_PER_SHARD)) # use the batch size as the shard size"
      ],
      "metadata": {
        "id": "g96gl5HrCUG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SHARDS = [4,8,10,15,17,20,27,31]  # these files will be labeled \"test\" other files \"train\"\n",
        "\n",
        "if os.path.isdir(OUTPUT_DIR) and len(os.listdir(OUTPUT_DIR))>0:\n",
        "    print(\"ERROR: the output directory exists and is not empty. Aborting. Please empty the output directory manually before proceeding.\")\n",
        "else:\n",
        "    if not os.path.isdir(OUTPUT_DIR):\n",
        "        os.mkdir(OUTPUT_DIR)\n",
        "\n",
        "    print(\"Writing TFRecords\")\n",
        "    for shard, (image, size, ids, boxes, classes) in enumerate(dataset):\n",
        "        shard_size = image.numpy().shape[0] # batch size is shard size\n",
        "        \n",
        "        # use a standard shard naming convention with the number of images and the number of shards\n",
        "        #filename = OUTPUT_NAME_PREFIX+\"{}x{}px_{:03d}_of_{:03d}-{:03d}.tfrec\".format(\n",
        "        #    TARGET_SIZE[0], TARGET_SIZE[1], shard+1, NB_SHARDS, shard_size)\n",
        "        filename = OUTPUT_NAME_PREFIX+\"w{}px_{:03d}_of_{:03d}-{:03d}.{}.tfrec\".format(\n",
        "            TARGET_WIDTH, shard+1, NB_SHARDS, shard_size, 'test' if shard in TEST_SHARDS else 'train')\n",
        "        \n",
        "        with tf.io.TFRecordWriter(os.path.join(OUTPUT_DIR, filename)) as file:\n",
        "            for i in range(shard_size):\n",
        "                \n",
        "                binary_image   = image[i].numpy()\n",
        "                binary_id      = compute_id_bytestring(ids[i].numpy().decode('utf-8')) # Model Garden data loader fails if id is too long\n",
        "                binary_width   = size[i].numpy()[0]\n",
        "                binary_height  = size[i].numpy()[1]\n",
        "                binary_boxes   = boxes[i].numpy()                  # coordinates in x1 y1 x2 y2 format\n",
        "                #binary_nb_tags = tags[i].row_lengths().numpy()    # ragged to: nb of labels per box\n",
        "                #binary_tags    = tags[i].flat_values.numpy()      # ragged to: flat list of labels\n",
        "                binary_labels  = classes[i].numpy()                # list of labels (one label per box)\n",
        "                \n",
        "                example = to_tfrecord(file, binary_image, binary_width, binary_height, binary_id, binary_boxes, binary_labels)\n",
        "                file.write(example.SerializeToString())\n",
        "        print(\"Wrote file {} containing {} records\".format(filename, shard_size))"
      ],
      "metadata": {
        "id": "Urq9dXb8Cbys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "thKpjDxeCkdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LEDdL-euCkXv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}