{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VdtRvVryHrSc",
        "NogqyrRUHhp7",
        "kpIBQk5GHkun"
      ],
      "mount_file_id": "1EkPO3-a7zWEBBox_OayTsTNDMvTNzzuS",
      "authorship_tag": "ABX9TyPKqNXVIYkQ/DOSRqLUvOdT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Projects/blob/main/Arthropod_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Arthropod Taxonomy Orders Object Detection Dataset](https://www.kaggle.com/datasets/mistag/arthropod-taxonomy-orders-object-detection-dataset/)\n",
        "\n",
        "\n",
        "\n",
        "###### Lepidoptera = butterfies and moths\n",
        "###### Hymenoptera = wasps, bees and ants\n",
        "###### Hemiptera = true bugs (cicadas, aphids, shield bugs, ...)\n",
        "###### Odonata = dragonflies\n",
        "###### Diptera = fies\n",
        "###### Araneae = spiders\n",
        "###### Coleoptera = beetles\n",
        "\n",
        "###### NOT IN DATASET\n",
        "###### Orthoptera = grasshoppers"
      ],
      "metadata": {
        "id": "VdtRvVryHrSc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Jc6VQxrK6Zz3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import pprint as pp\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib as mpl\n",
        "import concurrent.futures\n",
        "from threading import Lock\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "pp = pp.PrettyPrinter()\n",
        "\n",
        "!pip install --quiet tf-models-official==2.9.2\n",
        "sys.path.append('PATH_TO_TENSORFLOW_OBJECT_DETECTION_FOLDER')# load all the metadata\n",
        "\n",
        "import tensorflow_models as tfm\n",
        "from tensorflow_models.vision import box_ops as box"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR']='/content/drive/MyDrive/'\n",
        "!kaggle datasets download -d mistag/arthropod-taxonomy-orders-object-detection-dataset\n",
        "!unzip *.zip && rm *.zip"
      ],
      "metadata": {
        "id": "OEQKeqZl8mgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#part 1\n",
        "\n"
      ],
      "metadata": {
        "id": "O3md6o47lY_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PerProcessing"
      ],
      "metadata": {
        "id": "NogqyrRUHhp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGES_PER_SHARD = 481\n",
        "TARGET_WIDTH = 1024\n",
        "\n",
        "CLASSES = ['Lepidoptera', 'Hymenoptera', 'Hemiptera', 'Odonata', 'Diptera', 'Araneae', 'Coleoptera']\n",
        "\n",
        "RAW_CLASSES = CLASSES + ['_truncated', '_blurred', '_occluded']"
      ],
      "metadata": {
        "id": "Msyvk4_ZZPex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Progress:\n",
        "    \"\"\"Text mode progress bar.\n",
        "    Usage:\n",
        "            p = Progress(30)\n",
        "            p.step()\n",
        "            p.step()\n",
        "            p.step(start=True) # to restart form 0%\n",
        "    The progress bar displays a new header at each restart.\"\"\"\n",
        "    def __init__(self, maxi, size=100, msg=\"\"):\n",
        "        \"\"\"\n",
        "        :param maxi: the number of steps required to reach 100%\n",
        "        :param size: the number of characters taken on the screen by the progress bar\n",
        "        :param msg: the message displayed in the header of the progress bat\n",
        "        \"\"\"\n",
        "        self.maxi = maxi\n",
        "        self.p = self.__start_progress(maxi)()  # () to get the iterator from the generator\n",
        "        self.header_printed = False\n",
        "        self.msg = msg\n",
        "        self.size = size\n",
        "        self.lock = Lock()\n",
        "\n",
        "    def step(self, reset=False):\n",
        "        with self.lock:\n",
        "            if reset:\n",
        "                self.__init__(self.maxi, self.size, self.msg)\n",
        "            if not self.header_printed:\n",
        "                self.__print_header()\n",
        "            next(self.p)\n",
        "\n",
        "    def __print_header(self):\n",
        "        print()\n",
        "        format_string = \"0%{: ^\" + str(self.size - 6) + \"}100%\"\n",
        "        print(format_string.format(self.msg))\n",
        "        self.header_printed = True\n",
        "\n",
        "    def __start_progress(self, maxi):\n",
        "        def print_progress():\n",
        "            # Bresenham's algorithm. Yields the number of dots printed.\n",
        "            # This will always print 100 dots in max invocations.\n",
        "            dx = maxi\n",
        "            dy = self.size\n",
        "            d = dy - dx\n",
        "            for x in range(maxi):\n",
        "                k = 0\n",
        "                while d >= 0:\n",
        "                    print('=', end=\"\", flush=True)\n",
        "                    k += 1\n",
        "                    d -= dx\n",
        "                d += dy\n",
        "                yield k\n",
        "\n",
        "        return print_progress\n",
        "\n",
        "    \n",
        "def no_decorations(ax):\n",
        "    ax.axes.get_xaxis().set_visible(False)\n",
        "    ax.axes.get_yaxis().set_visible(False)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['bottom'].set_visible(False)\n",
        "    ax.spines['left'].set_visible(False)\n",
        "    \n",
        "\n",
        "def display_detections(images, offsets, resizes, detections, classnames, ground_truth_boxes=[]):\n",
        "    # scale and offset the detected boxes back to original image coordinates\n",
        "    boxes   = [[ (x,y,w,h)  for _, x, y, w, h, score, klass in detection_list] for detection_list in detections]\n",
        "    boxes   = [[ (x-ofs[1], y-ofs[0], w, h) for x,y,w,h in boxlist ] for boxlist, ofs in zip(boxes, offsets)]\n",
        "    boxes   = [[ (x*rsz, y*rsz, w*rsz, h*rsz) for x,y,w,h in boxlist ] for boxlist, rsz in zip(boxes, resizes)]\n",
        "    classes = [[ int(klass) for _, x, y, w, h, score, klass in detection_list] for detection_list in detections]\n",
        "    scores  = [[ score      for _, x, y, w, h, score, klass in detection_list] for detection_list in detections]\n",
        "    display_with_boxes(images, boxes, classes, scores, classnames, ground_truth_boxes)\n",
        "    \n",
        "    \n",
        "# images, boxes and classes must have the same number of elements\n",
        "# scores can be en empty list []. If it is not empty, it must also\n",
        "# have the same number of elements.\n",
        "# classnames is the list of possible classes (strings)\n",
        "def display_with_boxes(images, boxes, classes, scores, classnames, ground_truth_boxes=[]):\n",
        "    N = len(images)\n",
        "    sqrtN = int(np.ceil(np.sqrt(N)))\n",
        "    aspect = sum([im.shape[1]/im.shape[0] for im in images])/len(images) # mean aspect ratio of images\n",
        "    fig = plt.figure(figsize=(15,15/aspect), frameon=False)\n",
        "    \n",
        "    for k in range(N):\n",
        "        ax = plt.subplot(sqrtN, sqrtN, k+1)\n",
        "        no_decorations(ax)\n",
        "        plt.imshow(images[k])\n",
        "        \n",
        "        if ground_truth_boxes:\n",
        "            for box in ground_truth_boxes[k]:\n",
        "                x, y, w, h = (box[0], box[1], box[2]-box[0], box[3]-box[1]) # convert x1 y1 x2 y2 into xywh\n",
        "                #x, y, w, h = (box[0], box[1], box[2], box[3])\n",
        "                rect = mpl.patches.Rectangle((x, y),w,h,linewidth=4,edgecolor='#FFFFFFA0',facecolor='none')\n",
        "                ax.add_patch(rect)\n",
        "\n",
        "        for i, (box, klass) in enumerate(zip(boxes[k], classes[k])):\n",
        "            x, y, w, h = (box[0], box[1], box[2]-box[0], box[3]-box[1]) # convert x1 y1 x2 y2 into xywh\n",
        "            #x, y, w, h = (box[0], box[1], box[2], box[3])\n",
        "            #label = classnames[klass-1] # predicted classes are 1-based\n",
        "            label = classnames[klass]\n",
        "            if scores:\n",
        "                label += ' ' + str(int(scores[k][i]*100)) + '%' \n",
        "            rect = mpl.patches.Rectangle((x, y),w,h,linewidth=4,edgecolor='#00000080',facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "            rect = mpl.patches.Rectangle((x, y),w,h,linewidth=2,edgecolor='#FFFF00FF',facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "            plt.text(x, y, label, size=16, ha=\"left\", va=\"top\", color='#FFFF00FF',\n",
        "                     bbox=dict(boxstyle=\"round\", ec='#00000080', fc='#0000004E', linewidth=3) )\n",
        "            plt.text(x, y, label, size=16, ha=\"left\", va=\"top\", color='#FFFF00FF',\n",
        "                     bbox=dict(boxstyle=\"round\", ec='#FFFF00FF', fc='#0000004E', linewidth=1.5) )\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(wspace=0.02, hspace=0.02)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "w-S7WrgfBJaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/content/ArTaxOr/Araneae'\n",
        "json_filename_pattern = '/content/ArTaxOr/Araneae/annotations/*.json'\n",
        "jpeg_filename_pattern = '/content/ArTaxOr/Araneae/*.jpg'\n",
        "\n",
        "def load_json(filename, p):\n",
        "    p.step()\n",
        "    with tf.io.gfile.GFile(filename, 'r') as f:\n",
        "        return json.load(f)\n",
        "    \n",
        "def filename_key(filename):\n",
        "    path, filename = os.path.split(filename)\n",
        "    dirname = os.path.split(path)[1]\n",
        "    return filename\n",
        "    # return os.path.join(dirname, filename)\n",
        "    \n",
        "def load_metadata(filename_pattern, jpeg_filename_pattern):\n",
        "    print(\"Scanning directory...\", end=' ')\n",
        "    json_filenames = tf.io.gfile.glob(json_filename_pattern)\n",
        "    jpeg_filenames = tf.io.gfile.glob(jpeg_filename_pattern)\n",
        "    print(f\"found {len(json_filenames)} metadata files and {len(jpeg_filenames)} image files.\")\n",
        "    print(\"Loading metadata\")\n",
        "    p = Progress(len(json_filenames))\n",
        "    with concurrent.futures.ThreadPoolExecutor() as exe:\n",
        "        data = exe.map(lambda x: load_json(x,p), json_filenames)\n",
        "    # data as a dictionary for easier cross-referencing\n",
        "    data = {filename_key(d['asset']['path']):d for d in data}\n",
        "    return data, jpeg_filenames\n",
        "\n",
        "RAW_METADATA, JPEG_FILENAMES = load_metadata(json_filename_pattern, jpeg_filename_pattern)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tvvJWpPA5-a",
        "outputId": "0db70c4d-c90f-4d8c-987c-171b4ec8dc08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanning directory... found 2418 metadata files and 2418 image files.\n",
            "Loading metadata\n",
            "\n",
            "0%                                                                                              100%\n",
            "===================================================================================================="
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = JPEG_FILENAMES[0]\n",
        "RAW_METADATA[name[-16:]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31CkjYPus83g",
        "outputId": "34880684-a6b7-4eea-a0b7-8a82ce1dd69e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'asset': {'format': 'jpg',\n",
              "  'id': 'b25575a4d90e8922ecb815e684638555',\n",
              "  'name': '9cc9c7842d7b.jpg',\n",
              "  'path': 'file:F:/ArTaxOr/Araneae/9cc9c7842d7b.jpg',\n",
              "  'size': {'width': 5184, 'height': 3456},\n",
              "  'state': 2,\n",
              "  'type': 1},\n",
              " 'regions': [{'id': 'Sojpn0NLA',\n",
              "   'type': 'RECTANGLE',\n",
              "   'tags': ['Araneae'],\n",
              "   'boundingBox': {'height': 1272.3226473629782,\n",
              "    'width': 1848.3641379310345,\n",
              "    'left': 1805.4620689655173,\n",
              "    'top': 1277.1251292657703},\n",
              "   'points': [{'x': 1805.4620689655173, 'y': 1277.1251292657703},\n",
              "    {'x': 3653.826206896552, 'y': 1277.1251292657703},\n",
              "    {'x': 3653.826206896552, 'y': 2549.4477766287487},\n",
              "    {'x': 1805.4620689655173, 'y': 2549.4477766287487}]}],\n",
              " 'version': '2.1.0'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that there is a model garden safe way of computing image ids.\n",
        "# In the end, this does not matter. The ids are never used, but they must still be processed,\n",
        "# otherwise the Model Garden data loader will crash on them.\n",
        "\n",
        "# copied from Model Garden code\n",
        "# = official/vision/beta/dataloaders/utils process_source_id()\n",
        "\n",
        "def model_garden__str_to_int64(s):\n",
        "    return tf.strings.to_number(s, tf.int64).numpy()\n",
        "\n",
        "def compute_id_bytestring(s):\n",
        "    \n",
        "    computed_id = (int('0x' + s[:16], 16) ^ int('0x' + s[16:], 16)) & 0x0FFFFFFFFFFFFFFF\n",
        "    return str(computed_id).encode('utf-8')\n",
        "\n",
        "raw_ids = []\n",
        "cnv_ids = []\n",
        "cnv_cnv_ids = []\n",
        "\n",
        "for i, key in enumerate(RAW_METADATA):\n",
        "    iid = RAW_METADATA[key]['asset']['id']\n",
        "    raw_ids.append(iid)\n",
        "    iid = compute_id_bytestring(iid)\n",
        "    #print(\"computed:\", iid)\n",
        "    cnv_ids.append(iid)\n",
        "    iid = model_garden__str_to_int64(iid)\n",
        "    cnv_cnv_ids.append(iid)\n",
        "\n",
        "print(f\"Original ids: {len(raw_ids)} Uniques:{len(set(raw_ids))} Collisions:{len(raw_ids) - len(set(raw_ids))}\")\n",
        "print(f\"Original ids: {len(cnv_ids)} Uniques:{len(set(cnv_ids))} Collisions:{len(raw_ids) - len(set(cnv_ids))}\")\n",
        "print(f\"Computed ids as converted by Model Garden:{len(cnv_cnv_ids)}, Uniques: {len(set(cnv_cnv_ids))}, Collisions: {len(raw_ids) - len(set(cnv_cnv_ids))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4xyoVqD0tuU",
        "outputId": "92830f5d-e733-465a-893c-e85d2d9ea2e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original ids: 2418 Uniques:2418 Collisions:0\n",
            "Original ids: 2418 Uniques:2418 Collisions:0\n",
            "Computed ids as converted by Model Garden:2418, Uniques: 2418, Collisions: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# collect weird aspec ratio images and very large images\n",
        "oddities = [d['asset']['path'] for k,d in RAW_METADATA.items() if d['asset']['size']['width']>5400]\n",
        "oddities += [d['asset']['path'] for k,d in RAW_METADATA.items() if d['asset']['size']['height']>5000]\n",
        "oddities2 = [d['asset']['path'] for k,d in RAW_METADATA.items() if d['asset']['size']['height']/d['asset']['size']['width']>1.9]\n",
        "oddities2 += [d['asset']['path'] for k,d in RAW_METADATA.items() if d['asset']['size']['width']/d['asset']['size']['height']>2.0]\n",
        "\n",
        "nice_pics = ['/ArTaxOr/Lepidoptera/e7d7b4678088.jpg','/ArTaxOr/Lepidoptera/e74f298859ff.jpg','/ArTaxOr/Coleoptera/92c9a15e7362.jpg',\n",
        "             '/ArTaxOr/Coleoptera/a1824522fddc.jpg','/ArTaxOr/Hymenoptera/7188c0cc8c9d.jpg','/ArTaxOr/Lepidoptera/b7197aead30b.jpg',\n",
        "             '/ArTaxOr/Lepidoptera/dfc9ece476e6.jpg','/ArTaxOr/Araneae/a1488eb130e3.jpg','/ArTaxOr/Coleoptera/39c0eabccc41.jpg']\n",
        "alt_pics = ['/ArTaxOr/Araneae/3c6491416c3f.jpg','/ArTaxOr/Araneae/81ff08857d15.jpg',\n",
        "            '/ArTaxOr/Hymenoptera/f8f10bc28f5b.jpg','/ArTaxOr/Lepidoptera/e314c31efafd.jpg']\n",
        "\n",
        "#filenames = tf.io.gfile.glob('../input/arthropod-taxonomy-orders-object-detection-dataset/ArTaxOr/*/*jpg')\n",
        "#filenames = nice_pics\n",
        "filenames = oddities+oddities2\n",
        "#filenames = alt_pics\n",
        "\n",
        "# display a couple of images from the raw_metadata with their bounding boxes\n",
        "images = []\n",
        "bboxes = []\n",
        "taglists = []\n",
        "np.random.shuffle(filenames)\n",
        "for filename in filenames[:4]:\n",
        "    print(DATA_PATH, filename_key(filename), filename)\n",
        "    filepath = os.path.join(DATA_PATH, filename_key(filename))\n",
        "    d = RAW_METADATA[filename_key(filename)]\n",
        "    images.append(tf.image.decode_jpeg(tf.io.read_file(filepath)))\n",
        "    bbxs = [region['boundingBox'] for region in d['regions']]\n",
        "    # xywh to yxyx conversion\n",
        "    bbxs = [[box['left'],box['top'], box['left']+box['width'], box['top']+box['height']] for box in bbxs]\n",
        "    #tags = [region['tags'] for region in d['regions']] # all tags\n",
        "    tags = [region['tags'][0] for region in d['regions']] # first tag only\n",
        "    tags = [CLASSES.index(t) for t in tags] \n",
        "    bboxes.append(bbxs)\n",
        "    taglists.append(tags)\n",
        "\n",
        "display_with_boxes(images, bboxes, taglists, None, CLASSES, ground_truth_boxes=[])"
      ],
      "metadata": {
        "id": "HOgvmClK27Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Nb entries:', len(RAW_METADATA))\n",
        "print('Nb images:', len(JPEG_FILENAMES))\n",
        "assert(len(RAW_METADATA) == len(JPEG_FILENAMES))\n",
        "\n",
        "formats = set([d['asset']['format'] for k,d in RAW_METADATA.items()])\n",
        "print('Image formats:', formats)\n",
        "assert(len(formats)==1 and list(formats)[0]=='jpg')\n",
        "\n",
        "states = set([d['asset']['state'] for k,d in RAW_METADATA.items()])\n",
        "print('Image states(?):', states)\n",
        "types = set([d['asset']['type'] for k,d in RAW_METADATA.items()])\n",
        "print('Image types(?):', types)\n",
        "\n",
        "widths = [d['asset']['size']['width'] for k,d in RAW_METADATA.items()]\n",
        "print(f'Images widths range from {min(widths)} to {max(widths)}')\n",
        "heights = [d['asset']['size']['height'] for k,d in RAW_METADATA.items()]\n",
        "print(f'Images heights range from {min(heights)} to {max(heights)}')\n",
        "\n",
        "aspect_ratios = [w/h for w,h in zip(widths, heights)]\n",
        "print(f'Images aspect ratios range from {min(aspect_ratios)} to {max(aspect_ratios)}')\n",
        "\n",
        "nbbox = [len(d['regions']) for k,d in RAW_METADATA.items()]\n",
        "print(f'Nb of bounding boxes from {min(nbbox)} to {max(nbbox)}, average {sum(nbbox)/len(nbbox):.3}')\n",
        "\n",
        "region_types = [set([t['type'] for t in d['regions']]) for k,d in RAW_METADATA.items()]\n",
        "region_types = set().union(*region_types)\n",
        "print('Region types:', region_types)\n",
        "assert(len(region_types)==1 and list(region_types)[0]=='RECTANGLE')\n",
        "\n",
        "region_tags = [[t['tags'] for t in d['regions']] for k,d in RAW_METADATA.items()]\n",
        "region_tags = [sum(l,[]) for l in region_tags] # concat lists of tags across multiple regions\n",
        "region_tags = set(sum(region_tags, [])) # concat all and make a set\n",
        "print('Region tags:', region_tags)\n",
        "\n",
        "bbox_width = [[t['boundingBox']['width'] for t in d['regions']] for k,d in RAW_METADATA.items()]\n",
        "bbox_width = sum(bbox_width, []) # flatten the list\n",
        "print(f'Bounding box widths range from {min(bbox_width)} to {max(bbox_width)}')\n",
        "\n",
        "bbox_height = [[t['boundingBox']['height'] for t in d['regions']] for k,d in RAW_METADATA.items()]\n",
        "bbox_height = sum(bbox_height, []) # flatten the list\n",
        "print(f'Bounding box height range from {min(bbox_height)} to {max(bbox_height)}')\n",
        "\n",
        "bbox_left = [[t['boundingBox']['left'] for t in d['regions']] for k,d in RAW_METADATA.items()]\n",
        "bbox_left = sum(bbox_left, []) # flatten the list\n",
        "print(f'Bounding box left range from {min(bbox_left)} to {max(bbox_left)}')\n",
        "\n",
        "bbox_top = [[t['boundingBox']['top'] for t in d['regions']] for k,d in RAW_METADATA.items()]\n",
        "bbox_top = sum(bbox_top, []) # flatten the list\n",
        "print(f'Bounding box top range from {min(bbox_top)} to {max(bbox_top)}')\n",
        "\n",
        "# compute aspect ratios in landscape mode\n",
        "landscape_aspect_ratios = [w/h if w>h else h/w for w,h in zip(widths, heights)]\n",
        "print(f'Images landscape aspect ratios range from {min(landscape_aspect_ratios)} to {max(landscape_aspect_ratios)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i33c85W39HQ",
        "outputId": "02f4fe84-bc6d-41eb-ab7e-1f072353c1fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nb entries: 2418\n",
            "Nb images: 2418\n",
            "Image formats: {'jpg'}\n",
            "Image states(?): {2}\n",
            "Image types(?): {1}\n",
            "Images widths range from 500 to 5500\n",
            "Images heights range from 375 to 4272\n",
            "Images aspect ratios range from 0.4044418468731736 to 2.3703703703703702\n",
            "Nb of bounding boxes from 1 to 34, average 1.11\n",
            "Region types: {'RECTANGLE'}\n",
            "Region tags: {'_truncated', '_blurred', 'Lepidoptera', 'Araneae', '_occluded', 'Coleoptera', 'Hymenoptera', 'Diptera', 'Hemiptera'}\n",
            "Bounding box widths range from 29.42528735632184 to 3861.1984838042727\n",
            "Bounding box height range from 29.42528735632184 to 3453.654601861427\n",
            "Bounding box left range from 0 to 3980.0137835975193\n",
            "Bounding box top range from 0 to 3089.1127197518094\n",
            "Images landscape aspect ratios range from 1.0 to 2.472543352601156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2,ax3,ax4) = plt.subplots(1,4, figsize=(15,3))\n",
        "ax1.set_title(\"aspect ratios\")\n",
        "ax1.hist(aspect_ratios)\n",
        "ax2.set_title(\"landscape aspect ratios\")\n",
        "ax2.hist(landscape_aspect_ratios)\n",
        "ax3.set_title(\"widths\")\n",
        "ax3.hist(widths)\n",
        "ax4.set_title(\"heights\")\n",
        "ax4.hist(heights)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JjZRmW7L4yKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metadataset_from_image_filenames(filenames):\n",
        "    values = [RAW_METADATA[filename_key(f)] for f in filenames]\n",
        "\n",
        "    values = {'id': [filename_key(val['asset']['id']) for val in values],\n",
        "              'fname': [os.path.join(DATA_PATH, filename_key(val['asset']['path'])) for val in values],\n",
        "              'width':[val['asset']['size']['width'] for val in values],\n",
        "              'height':[val['asset']['size']['height'] for val in values],\n",
        "              # xywh to yxyx conversion ?\n",
        "              # internal box format: x1 y1 x2 y2 in pixel coordinates\n",
        "              'boxes': tf.ragged.constant([[(item['boundingBox']['left'],\n",
        "                                             item['boundingBox']['top'],\n",
        "                                             item['boundingBox']['left']+item['boundingBox']['width'],\n",
        "                                             item['boundingBox']['top']+item['boundingBox']['height']) \n",
        "                                            for item in val['regions']] for val in values], ragged_rank=1), # must specify ragged rank, othewise tf.ragged.constant\n",
        "                                                                               # does not detect that the last dim is 4 [x1 y1 x2 y2]\n",
        "               'tags': tf.ragged.constant([[item['tags'] for item in val['regions']] for val in values]),\n",
        "               'classes': tf.ragged.constant([[[CLASSES.index(tag) for tag in item['tags'] if tag in CLASSES] for item in val['regions']] for val in values])\n",
        "              }\n",
        "\n",
        "    metadataset = tf.data.Dataset.from_tensor_slices(values)\n",
        "    return metadataset"
      ],
      "metadata": {
        "id": "TROu85Ab6I57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_img(metadata):\n",
        "    compressed_data = tf.io.read_file(metadata['fname'])\n",
        "    image = tf.image.decode_jpeg(compressed_data, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32) # convert from uint8 [0,255] float [0,1)\n",
        "    return image, metadata\n",
        "\n",
        "def keep_one_tag_per_box(tags):\n",
        "    # Keeping first tag only. Other tags are not bug names but qualifiesrs like '_truncated', '_blurred', '_occluded'\n",
        "    # Input: one tag list per box: ex: [['Odonata', '_blurred'], ['Hymenoptera']] for 2 boxes\n",
        "    # This works for 0 boxes (tested)\n",
        "    tags = tags[:, 0:1] # Cannot index into ragged dimension, must do this.\n",
        "    tags = tags.merge_dims(-2,-1) # equivalent to tf.squeeze(tags, axis=-1) for ragged tensor\n",
        "    return tags\n",
        "\n",
        "# Resize images to MAX_WIDTH if they are larger. Adjust bounding boxes accordingly.\n",
        "def resize_image_and_boxes(im, metadata):\n",
        "    boxes = tf.cast(metadata['boxes'], tf.float32) # the number of boxes is ragged but for one image it is constant\n",
        "    image_size = tf.stack([metadata['width'], metadata['height']])\n",
        "    \n",
        "    scale_factor = tf.cast(TARGET_WIDTH, tf.float32) / tf.cast(image_size[0], tf.float32)\n",
        "    scale_factor = tf.math.minimum(scale_factor, 1.0)\n",
        "    new_image_size = tf.stack([tf.math.round(scale_factor * tf.cast(image_size[0], tf.float32)), \n",
        "                                tf.math.round(scale_factor * tf.cast(image_size[1], tf.float32))])\n",
        "    # resize image\n",
        "    im = tf.image.resize(im, tf.stack([image_size[1], TARGET_WIDTH]), preserve_aspect_ratio=True)\n",
        "    \n",
        "    # resize boxes\n",
        "    # boxes = box.normalize_boxes(boxes, tf.math.reciprocal(tf.cast(image_size, tf.float32)))\n",
        "    # boxes = box.normalize_boxes(boxes, new_image_size)\n",
        "    #boxes = tf.cast(tf.round(boxes), tf.int32)\n",
        "    boxes = box.normalize_boxes(boxes, tf.math.reciprocal(tf.cast(image_size, tf.float32)))\n",
        "    boxes = box.compute_outer_boxes(boxes, new_image_size)\n",
        "    # metadata\n",
        "    new_image_size = tf.cast(new_image_size, tf.int32)\n",
        "    classes = metadata['classes']\n",
        "    tags = metadata['tags']\n",
        "    ids = metadata['id'] \n",
        "    \n",
        "    return im, new_image_size, ids, boxes, tags, classes"
      ],
      "metadata": {
        "id": "eefp9xig6gWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filenames = JPEG_FILENAMES\n",
        "#filenames = alt_pics\n",
        "filenames = (oddities+oddities2).copy()\n",
        "np.random.shuffle(filenames)\n",
        "\n",
        "metadataset = metadataset_from_image_filenames(filenames)\n",
        "dataset = metadataset.map(decode_img, num_parallel_calls=AUTO)\n",
        "dataset = dataset.map(resize_image_and_boxes, num_parallel_calls=AUTO)\n",
        "\n",
        "dataset_iterator = iter(dataset.apply(tf.data.experimental.dense_to_ragged_batch(4)))"
      ],
      "metadata": {
        "id": "4OEPiMtX6k7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, new_image_size, names, boxes, tags, classes  = next(dataset_iterator)\n",
        "images = [im.numpy() for im in images] # must do this for ragged\n",
        "classes = [[classlist[0] for classlist in detection] for detection in classes] # keep first class only\n",
        "display_with_boxes(images, boxes, classes, None, CLASSES, ground_truth_boxes=[])"
      ],
      "metadata": {
        "id": "C7mfFDxK6np0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames_to_process = JPEG_FILENAMES.copy()\n",
        "\n",
        "OUTPUT_DIR = '../../arthropod-data-tfrec'\n",
        "OUTPUT_NAME_PREFIX = 'arthropods_'\n",
        "\n",
        "print(\"TFRecords output directory:\", OUTPUT_DIR) \n",
        "print(\"Images resizing target size:\", TARGET_WIDTH, 'px')\n",
        "print(\"Number of images:\", len(filenames_to_process))\n",
        "NB_SHARDS = -(-len(filenames_to_process)//IMAGES_PER_SHARD) # -- trick rounds up\n",
        "print(f\"Output sharded into {NB_SHARDS} files with {IMAGES_PER_SHARD} images per file, {len(filenames_to_process)-(NB_SHARDS-1)*IMAGES_PER_SHARD} images in last file\")"
      ],
      "metadata": {
        "id": "EC7AJ4HWBcES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a789c78a-7265-4986-b7d7-55a1cd7e683e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFRecords output directory: ../../arthropod-data-tfrec\n",
            "Images resizing target size: 1024 px\n",
            "Number of images: 2418\n",
            "Output sharded into 6 files with 481 images per file, 13 images in last file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TFRecord helper functions\n",
        "def _bytes_feature(value):\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value)) # WARNING: this expects a list of byte strings, not a list of bytes!\n",
        "def _float_feature(value):\n",
        "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
        "def _int_feature(value):\n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
        "\n",
        "def to_tfrecord(tfrec_filewriter, img_bytes, width, height, ids, boxes, labels):\n",
        "    feature = {\n",
        "        \"image/encoded\": _bytes_feature([img_bytes]), # compressed image bytes\n",
        "        \"image/source_id\": _bytes_feature([ids]),    # string\n",
        "        \"image/width\": _int_feature([width]),         # image width\n",
        "        \"image/height\": _int_feature([height]),       # image height\n",
        "        \"image/object/bbox/xmin\": _float_feature(boxes[:,0]),\n",
        "        \"image/object/bbox/ymin\": _float_feature(boxes[:,1]),\n",
        "        \"image/object/bbox/xmax\": _float_feature(boxes[:,2]),\n",
        "        \"image/object/bbox/ymax\": _float_feature(boxes[:,3]),\n",
        "        #\"nb_tags\": _int_feature(nb_tags),     # nb of tags per box\n",
        "        \"image/object/class/label\": _int_feature(labels) # flat list of labels\n",
        "    }\n",
        "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n",
        "def recompress_jpeg(image):\n",
        "    image = tf.image.convert_image_dtype(image, tf.uint8) # convert from float [0,1) to uint8 [0,255]\n",
        "    return tf.image.encode_jpeg(image, optimize_size=True, chroma_downsampling=False)\n",
        "\n",
        "# Model Garden specific format adjustments\n",
        "\n",
        "# Model Garden models expect normalized box coordinates in [0..1] range\n",
        "def normalize_boxes(boxes, image_size):\n",
        "    boxes = box.normalize_boxes(boxes, tf.math.reciprocal(tf.cast(image_size, tf.float32)))\n",
        "    return boxes\n",
        "\n",
        "# Model Garden models expect target classes to be 1-based because they reserve class 0 for backgrounds.\n",
        "def labels_1_based(labels):\n",
        "    return labels+1"
      ],
      "metadata": {
        "id": "q0Telv4YBi3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.shuffle(filenames_to_process)\n",
        "metadataset = metadataset_from_image_filenames(filenames_to_process)\n",
        "dataset = metadataset.map(decode_img, num_parallel_calls=AUTO)\n",
        "dataset = dataset.map(resize_image_and_boxes, num_parallel_calls=AUTO)\n",
        "\n",
        "# apply the Model Garden specific format adjustments\n",
        "dataset = dataset.map(lambda image, size, ids, boxes, tags, classes:\n",
        "                      (recompress_jpeg(image), size, ids,\n",
        "                       normalize_boxes(boxes, size), # Model Garden models expect normalized box coordinates in [0..1] range\n",
        "                       labels_1_based(keep_one_tag_per_box(classes))), num_parallel_calls=AUTO)\n",
        "\n",
        "dataset = dataset.apply(tf.data.experimental.dense_to_ragged_batch(IMAGES_PER_SHARD)) # use the batch size as the shard size"
      ],
      "metadata": {
        "id": "g96gl5HrCUG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SHARDS = [4,8,10,15,17,20,27,31]  # these files will be labeled \"test\" other files \"train\"\n",
        "\n",
        "if os.path.isdir(OUTPUT_DIR) and len(os.listdir(OUTPUT_DIR))>0:\n",
        "    print(\"ERROR: the output directory exists and is not empty. Aborting. Please empty the output directory manually before proceeding.\")\n",
        "else:\n",
        "    if not os.path.isdir(OUTPUT_DIR):\n",
        "        os.mkdir(OUTPUT_DIR)\n",
        "\n",
        "    print(\"Writing TFRecords\")\n",
        "    for shard, (image, size, ids, boxes, classes) in enumerate(dataset):\n",
        "        shard_size = image.numpy().shape[0] # batch size is shard size\n",
        "        \n",
        "        # use a standard shard naming convention with the number of images and the number of shards\n",
        "        #filename = OUTPUT_NAME_PREFIX+\"{}x{}px_{:03d}_of_{:03d}-{:03d}.tfrec\".format(\n",
        "        #    TARGET_SIZE[0], TARGET_SIZE[1], shard+1, NB_SHARDS, shard_size)\n",
        "        filename = OUTPUT_NAME_PREFIX+\"w{}px_{:03d}_of_{:03d}-{:03d}.{}.tfrec\".format(\n",
        "            TARGET_WIDTH, shard+1, NB_SHARDS, shard_size, 'test' if shard in TEST_SHARDS else 'train')\n",
        "        \n",
        "        with tf.io.TFRecordWriter(os.path.join(OUTPUT_DIR, filename)) as file:\n",
        "            for i in range(shard_size):\n",
        "                \n",
        "                binary_image   = image[i].numpy()\n",
        "                binary_id      = compute_id_bytestring(ids[i].numpy().decode('utf-8')) # Model Garden data loader fails if id is too long\n",
        "                binary_width   = size[i].numpy()[0]\n",
        "                binary_height  = size[i].numpy()[1]\n",
        "                binary_boxes   = boxes[i].numpy()                  # coordinates in x1 y1 x2 y2 format\n",
        "                #binary_nb_tags = tags[i].row_lengths().numpy()    # ragged to: nb of labels per box\n",
        "                #binary_tags    = tags[i].flat_values.numpy()      # ragged to: flat list of labels\n",
        "                binary_labels  = classes[i].numpy()                # list of labels (one label per box)\n",
        "                \n",
        "                example = to_tfrecord(file, binary_image, binary_width, binary_height, binary_id, binary_boxes, binary_labels)\n",
        "                file.write(example.SerializeToString())\n",
        "        print(\"Wrote file {} containing {} records\".format(filename, shard_size))"
      ],
      "metadata": {
        "id": "Urq9dXb8Cbys",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6172bfed-8bf8-4076-a969-0bc4b05676bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing TFRecords\n",
            "Wrote file arthropods_w1024px_001_of_006-481.train.tfrec containing 481 records\n",
            "Wrote file arthropods_w1024px_002_of_006-481.train.tfrec containing 481 records\n",
            "Wrote file arthropods_w1024px_003_of_006-481.train.tfrec containing 481 records\n",
            "Wrote file arthropods_w1024px_004_of_006-481.train.tfrec containing 481 records\n",
            "Wrote file arthropods_w1024px_005_of_006-481.test.tfrec containing 481 records\n",
            "Wrote file arthropods_w1024px_006_of_006-013.train.tfrec containing 13 records\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_tfrecord(example):\n",
        "    feature = {\n",
        "        \"image/encoded\": tf.io.FixedLenFeature([], tf.string), # compressed image bytes\n",
        "        \"image/source_id\": tf.io.FixedLenFeature([], tf.string),  # string\n",
        "        \"image/width\": tf.io.FixedLenFeature([], tf.int64),\n",
        "        \"image/height\": tf.io.FixedLenFeature([], tf.int64),\n",
        "        \"image/object/bbox/xmin\": tf.io.VarLenFeature(tf.float32),\n",
        "        \"image/object/bbox/ymin\": tf.io.VarLenFeature(tf.float32),\n",
        "        \"image/object/bbox/xmax\": tf.io.VarLenFeature(tf.float32),\n",
        "        \"image/object/bbox/ymax\": tf.io.VarLenFeature(tf.float32),\n",
        "        \"image/object/class/label\": tf.io.VarLenFeature(tf.int64) # one tag per box\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, feature)\n",
        "    source_id = example['image/source_id']\n",
        "    image = tf.image.decode_jpeg(example['image/encoded'], channels=3)\n",
        "    boxes_xmin = tf.sparse.to_dense(example['image/object/bbox/xmin'])\n",
        "    boxes_xmax = tf.sparse.to_dense(example['image/object/bbox/xmax'])\n",
        "    boxes_ymin = tf.sparse.to_dense(example['image/object/bbox/ymin'])\n",
        "    boxes_ymax = tf.sparse.to_dense(example['image/object/bbox/ymax'])\n",
        "    boxes = tf.stack([boxes_xmin, boxes_ymin, boxes_xmax, boxes_ymax], axis=-1)\n",
        "    labels = tf.sparse.to_dense(example['image/object/class/label'])\n",
        "    return image, source_id, boxes, labels\n",
        "    \n",
        "def load_tfrecord_dataset(filenames):\n",
        "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
        "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
        "\n",
        "    ignore_order = tf.data.Options()\n",
        "    ignore_order.experimental_deterministic = False\n",
        "\n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
        "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
        "    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "thKpjDxeCkdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = tf.io.gfile.glob(os.path.join(OUTPUT_DIR, '*.tfrec'))\n",
        "dataset = load_tfrecord_dataset(filenames)\n",
        "dataset_iterator = iter(dataset.apply(tf.data.experimental.dense_to_ragged_batch(4)))"
      ],
      "metadata": {
        "id": "LEDdL-euCkXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, source_id, boxes, labels = next(dataset_iterator)\n",
        "images = [im.numpy() for im in images] # must do this for ragged\n",
        "\n",
        "# must scale the rois back to pixels for visualizatoin\n",
        "image_shapes = [tf.cast((image.shape[1], image.shape[0]), tf.float32) for image in images]\n",
        "boxes = [box.normalize_boxes(bbox, image_shapes[i]) for i,bbox in enumerate(boxes)]\n",
        "\n",
        "# must shift labels back to 0-based for visualization\n",
        "labels -= 1\n",
        "#classes = [[CLASSES.index(label) for label in batchitem] for batchitem in labels]\n",
        "display_with_boxes(images, boxes, labels, None, CLASSES, ground_truth_boxes=[])"
      ],
      "metadata": {
        "id": "HNBRvcgKCzm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yixLsO-68wg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "kpIBQk5GHkun"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GCS bucket\n",
        "\n",
        "This bucket will receive:\n",
        "\n",
        "Tensorboard summaries that allow you to follow the training\n",
        "\n",
        "checkpoints\n",
        "\n",
        "the saved model after training"
      ],
      "metadata": {
        "id": "Wk5mCQ1aOx-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use your own GCS bucket here. GCS is required if training on TPU.\n",
        "# On GPU, a local folder will work.\n",
        "MODEL_ARTIFACT_BUCKET = 'gs://ml1-demo-martin/arthropod_jobs/'\n",
        "MODEL_DIR = MODEL_ARTIFACT_BUCKET + str(int(time.time()))\n",
        "\n",
        "# If you are running on Colaboratory, you must authenticate\n",
        "# for Colab to have write access to the bucket.\n",
        "\n",
        "# this is always set on Colab, the value is 0 or 1 depending on GPU presence\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()"
      ],
      "metadata": {
        "id": "2wZerY3eHnyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TPU / GPU detection\n"
      ],
      "metadata": {
        "id": "lh6tW-OWPUDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try: # detect TPUs\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except ValueError: # detect GPUs or multi-GPU machines\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "print(f'Replicas: {strategy.num_replicas_in_sync}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDewTPEvO0-z",
        "outputId": "663534c0-9ad6-4231-f973-ae49b80904e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replicas: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuration"
      ],
      "metadata": {
        "id": "WHJuc7UFPyFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DATA_PATH_PATTERN = 'gs://practical-ml-vision-book/arthropod_detection_tfr/size_w1024px/*.train.tfrec'\n",
        "VALID_DATA_PATH_PATTERN = 'gs://practical-ml-vision-book/arthropod_detection_tfr/size_w1024px/*.test.tfrec'\n",
        "SPINET_MOBILE_CHECKPOINT = 'gs://practical-ml-vision-book/arthropod_detection_tfr/spinenet49mobile_checkpoint'\n",
        "\n",
        "BATCH_SIZE = 32 * strategy.num_replicas_in_sync\n",
        "\n",
        "EPOCHS = 80\n",
        "\n",
        "CLASSES = ['Lepidoptera', 'Hymenoptera', 'Hemiptera', 'Odonata', 'Diptera', 'Araneae', 'Coleoptera']\n",
        "\n",
        "RAW_CLASSES = CLASSES + ['_truncated', '_blurred', '_occluded']"
      ],
      "metadata": {
        "id": "HTNPlj40O7wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
        "data_items_counter = lambda files: int(np.sum([int(re.compile(r\"-([0-9]*)\\.\").search(f).group(1)) for f in files]))\n",
        "\n",
        "TRAIN_FILENAMES = tf.io.gfile.glob(TRAIN_DATA_PATH_PATTERN)\n",
        "TRAIN_IMAGES_COUNT = data_items_counter(TRAIN_FILENAMES)\n",
        "STEPS_PER_EPOCH = TRAIN_IMAGES_COUNT // BATCH_SIZE\n",
        "\n",
        "VALID_FILENAMES = tf.io.gfile.glob(VALID_DATA_PATH_PATTERN)\n",
        "VALID_IMAGES_COUNT = data_items_counter(VALID_FILENAMES)\n",
        "VALID_STEPS = VALID_IMAGES_COUNT // BATCH_SIZE\n",
        "\n",
        "print(f\"\"\"Training dataset:\n",
        "    {len(TRAIN_FILENAMES)}: TFRecord files.)\n",
        "    {TRAIN_IMAGES_COUNT}: images)\n",
        "    Steps per epoch: {STEPS_PER_EPOCH})\n",
        ")\n",
        "Validation dataset:)\n",
        "    {len(VALID_FILENAMES)}: TFRecord files.)\n",
        "    {VALID_IMAGES_COUNT}: images)\n",
        "    Validation steps: {VALID_STEPS})\n",
        "\n",
        "Global batch size: {BATCH_SIZE}\n",
        "    \"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbFRn1hWP6OT",
        "outputId": "2cd0b1ef-77fe-4c26-cce5-f833542e5847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset:\n",
            "    24: TFRecord files.)\n",
            "    11544: images)\n",
            "    Steps per epoch: 360)\n",
            ")\n",
            "Validation dataset:)\n",
            "    8: TFRecord files.)\n",
            "    3832: images)\n",
            "    Validation steps: 119)\n",
            "\n",
            "Global batch size: 32\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model configuration"
      ],
      "metadata": {
        "id": "9dCvhd_WRRjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = [384, 384]\n",
        "\n",
        "# default parameters can be overriden in two ways:\n",
        "# 1) params.override({'task': {'model': {'backbone': backbone_cfg.as_dict()}}})\n",
        "# 2) params.task.model.backbone = backbone_cfg\n",
        "# params.override checks that the dictionary keys exist\n",
        "# the second options will silently add new keys\n",
        "\n",
        "params = tfm.core.exp_factory.get_exp_config('retinanet')\n",
        "\n",
        "params.task.model.num_classes = len(CLASSES)+1 # class 0 is reserved for backgrounds\n",
        "params.task.model.input_size = [*IMAGE_SIZE, 3] # this automatically configures the input reader to random crop training images\n",
        "params.task.init_checkpoint = SPINET_MOBILE_CHECKPOINT\n",
        "params.task.init_checkpoint_modules = 'backbone'\n",
        "params.task.model.backbone = tfm.vision.configs.backbones.Backbone(type='spinenet_mobile',\n",
        "                                                                   spinenet_mobile=tfm.vision.configs.backbones.SpineNetMobile())\n",
        "\n",
        "train_data_cfg=tfm.vision.configs.retinanet.DataConfig(\n",
        "    input_path=TRAIN_DATA_PATH_PATTERN,\n",
        "    is_training=True,\n",
        "    global_batch_size=BATCH_SIZE,\n",
        "    parser=tfm.vision.configs.retinanet.Parser(aug_rand_hflip=True, aug_scale_min=0.7, aug_scale_max=2.0))\n",
        "\n",
        "valid_data_cfg=tfm.vision.configs.retinanet.DataConfig(\n",
        "    input_path=VALID_DATA_PATH_PATTERN,\n",
        "    is_training=False,\n",
        "    global_batch_size=BATCH_SIZE)\n",
        "\n",
        "# params.override({'task': {'model': {'backbone': backbone_cfg.as_dict()}}})\n",
        "params.override({'task': {'train_data': train_data_cfg.as_dict(), 'validation_data': valid_data_cfg.as_dict()}})\n",
        "\n",
        "trainer_cfg=tfm.core.config_definitions.TrainerConfig(\n",
        "    train_steps=EPOCHS * STEPS_PER_EPOCH,\n",
        "    validation_steps=VALID_STEPS,\n",
        "    validation_interval=8*STEPS_PER_EPOCH,\n",
        "    steps_per_loop=STEPS_PER_EPOCH,\n",
        "    summary_interval=STEPS_PER_EPOCH,\n",
        "    checkpoint_interval=8*STEPS_PER_EPOCH)\n",
        "\n",
        "optim_cfg = tfm.optimization.OptimizationConfig({\n",
        "    'optimizer': {\n",
        "                  'type': 'sgd',\n",
        "                  'sgd': {'momentum': 0.9}},\n",
        "    'learning_rate': {'type': 'stepwise',\n",
        "                      'stepwise': {'boundaries': [15 * STEPS_PER_EPOCH,\n",
        "                                                  30 * STEPS_PER_EPOCH,\n",
        "                                                  45 * STEPS_PER_EPOCH,\n",
        "                                                  60 * STEPS_PER_EPOCH,\n",
        "                                                  75 * STEPS_PER_EPOCH],\n",
        "                                   'values': [0.016, #0.01,\n",
        "                                              0.008, #0.005,\n",
        "                                              0.004, #0.0025,\n",
        "                                              0.002, #0.001,\n",
        "                                              0.001, #0.0005,\n",
        "                                              0.0005]} #0.00025]}\n",
        "                     },\n",
        "    #'warmup': {'type': 'linear','linear': {'warmup_steps': 5*STEPS_PER_EPOCH, 'warmup_learning_rate': 0.00001}}\n",
        "})\n",
        "\n",
        "trainer_cfg.override({'optimizer_config': optim_cfg})\n",
        "params.override({'trainer': trainer_cfg})\n",
        "\n",
        "pp.pprint(params.as_dict())"
      ],
      "metadata": {
        "id": "0k4bWzqdQL2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = tfm.core.task_factory.get_task(params.task, logging_dir=MODEL_DIR)\n",
        "\n",
        "# this works too:\n",
        "#task = official.vision.beta.tasks.retinanet.RetinaNetTask(params.task)\n",
        "\n",
        "# this returns a RetinaNetModel\n",
        "#task.build_model()\n",
        "# note: none of the expected model functionalities work: model.fit(), model.predict(), model.save()\n",
        "\n",
        "# this returns the training dataset\n",
        "#train_dataset = task.build_inputs(train_data_cfg)\n",
        "# note: the dataset already includes FPN level and anchor pairing and is therefore not very readable\n",
        "\n",
        "# this returns the validation dataset\n",
        "#valid_dataset = task.build_inputs(valid_data_cfg)\n",
        "# note: the dataset already includes FPN level and anchor pairing and is therefore not very readable\n",
        "\n",
        "# this code allows you to see if the TFRecord fields are read correctly\n",
        "#ds = tf.data.TFRecordDataset(tf.io.gfile.glob(TRAIN_DATA_PATH_PATTERN))\n",
        "#dec = official.vision.beta.dataloaders.tf_example_decoder.TfExampleDecoder()\n",
        "#ds = ds.map(dec.decode)\n",
        "\n",
        "# training and validatoin data parsing happens in:\n",
        "# official.vision.beta.dataloaders.retinanet_input.Parser._parse_train_data\n",
        "# official.vision.beta.dataloaders.retinanet_input.Parser._parse_eval_data\n",
        "# official.vision.beta.dataloaders.Parser.parse() # dispatches between _parse_train_data and _parse_eval_data"
      ],
      "metadata": {
        "id": "ABQ6_J6WRU83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "mUMtdkosSHmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(MODEL_DIR)\n",
        "model,_ = tfm.core.train_lib.run_experiment(\n",
        "    distribution_strategy=strategy,\n",
        "    task=task,\n",
        "    mode=\"train_and_eval\", # 'train', 'eval', 'train_and_eval' or 'continuous_eval'\n",
        "    params=params,\n",
        "    model_dir=MODEL_DIR)"
      ],
      "metadata": {
        "id": "A2D8JVOCR-Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export the model"
      ],
      "metadata": {
        "id": "zG6RxegUSOne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfm.vision.serving.export_saved_model_lib.export_inference_graph(\n",
        "      input_type='image_tensor',\n",
        "      batch_size=4,\n",
        "      input_image_size=IMAGE_SIZE,\n",
        "      params=params,\n",
        "      checkpoint_path=MODEL_DIR,\n",
        "      export_dir=MODEL_DIR,\n",
        "      export_checkpoint_subdir='saved_chkpt',\n",
        "      export_saved_model_subdir='saved_model')"
      ],
      "metadata": {
        "id": "oXAxcs3uSJ1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Part 2](https://www.kaggle.com/code/mistag/starter-arthropod-taxonomy-orders-data-exploring)"
      ],
      "metadata": {
        "id": "vPKPYklNHk92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Data Exploring](https://www.kaggle.com/code/mistag/starter-arthropod-taxonomy-orders-data-exploring)\n",
        "\n",
        "The Arthropod Taxonomy Orders dataset is a collection of highres images annotated with labels from the taxanomy rank order. Annotations have been made with VoTT. VoTT stores all metadata in json files. In this kernel we will import all the metadata into DataFrames and export metadata to a variety of formats for object detection model training.\n",
        "The dataset is distributed under CC BY-NC-SA 4.0"
      ],
      "metadata": {
        "id": "uh4YUVG9NG1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "QHsv6SBlHli_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "pfiles = glob.glob('/content/ArTaxOr/**/*.vott', recursive=True)\n",
        "\n",
        "df = pd.DataFrame()\n",
        "for p in pfiles:\n",
        "  with open(p) as f:\n",
        "    pdata = json.load(f)\n",
        "    df = df.append(pd.DataFrame(list(pdata['assets'].values())), ignore_index=True)\n",
        "\n",
        "df['path'] = df['path'].str.replace('file:F/', '')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "MA9dMUdJNyi7",
        "outputId": "fbb7fba5-3a95-441e-d437-6f7622d049d0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  format                                id              name  \\\n",
              "0    jpg  2550e885b02db84a8f31e36faf4f72c1  002da3868a1a.jpg   \n",
              "1    jpg  3641c4e79eafbba7613f27490d871cb6  004de63c1ac1.jpg   \n",
              "2    jpg  b13010787800b033da886c1624f86ffc  00930401f173.jpg   \n",
              "3    jpg  3f0d901456dc366d15ae553ece84e5a8  00eebccd5430.jpg   \n",
              "4    jpg  1fe209a391e180c0114793fbf00b2690  01424bd88646.jpg   \n",
              "\n",
              "                                          path  \\\n",
              "0  file:F:/ArTaxOr/Coleoptera/002da3868a1a.jpg   \n",
              "1  file:F:/ArTaxOr/Coleoptera/004de63c1ac1.jpg   \n",
              "2  file:F:/ArTaxOr/Coleoptera/00930401f173.jpg   \n",
              "3  file:F:/ArTaxOr/Coleoptera/00eebccd5430.jpg   \n",
              "4  file:F:/ArTaxOr/Coleoptera/01424bd88646.jpg   \n",
              "\n",
              "                              size  state  type  \n",
              "0   {'width': 907, 'height': 1208}      2     1  \n",
              "1  {'width': 3968, 'height': 2648}      2     1  \n",
              "2  {'width': 2048, 'height': 1536}      2     1  \n",
              "3  {'width': 2048, 'height': 1536}      2     1  \n",
              "4    {'width': 864, 'height': 565}      2     1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2c118cb2-ecee-4844-b15c-f2284907b7fc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>format</th>\n",
              "      <th>id</th>\n",
              "      <th>name</th>\n",
              "      <th>path</th>\n",
              "      <th>size</th>\n",
              "      <th>state</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>jpg</td>\n",
              "      <td>2550e885b02db84a8f31e36faf4f72c1</td>\n",
              "      <td>002da3868a1a.jpg</td>\n",
              "      <td>file:F:/ArTaxOr/Coleoptera/002da3868a1a.jpg</td>\n",
              "      <td>{'width': 907, 'height': 1208}</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>jpg</td>\n",
              "      <td>3641c4e79eafbba7613f27490d871cb6</td>\n",
              "      <td>004de63c1ac1.jpg</td>\n",
              "      <td>file:F:/ArTaxOr/Coleoptera/004de63c1ac1.jpg</td>\n",
              "      <td>{'width': 3968, 'height': 2648}</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>jpg</td>\n",
              "      <td>b13010787800b033da886c1624f86ffc</td>\n",
              "      <td>00930401f173.jpg</td>\n",
              "      <td>file:F:/ArTaxOr/Coleoptera/00930401f173.jpg</td>\n",
              "      <td>{'width': 2048, 'height': 1536}</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>jpg</td>\n",
              "      <td>3f0d901456dc366d15ae553ece84e5a8</td>\n",
              "      <td>00eebccd5430.jpg</td>\n",
              "      <td>file:F:/ArTaxOr/Coleoptera/00eebccd5430.jpg</td>\n",
              "      <td>{'width': 2048, 'height': 1536}</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jpg</td>\n",
              "      <td>1fe209a391e180c0114793fbf00b2690</td>\n",
              "      <td>01424bd88646.jpg</td>\n",
              "      <td>file:F:/ArTaxOr/Coleoptera/01424bd88646.jpg</td>\n",
              "      <td>{'width': 864, 'height': 565}</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c118cb2-ecee-4844-b15c-f2284907b7fc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2c118cb2-ecee-4844-b15c-f2284907b7fc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2c118cb2-ecee-4844-b15c-f2284907b7fc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tags = pd.DataFrame(list(pdata['tags']))\n",
        "pattern = r'[A-Z]'\n",
        "labels = tags[tags.name.str.match(pattern)]\n",
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "tCBoBkXWUgtJ",
        "outputId": "ba3328db-eff2-46ce-fd31-9378077bb903"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          name    color\n",
              "0  Lepidoptera  #5db300\n",
              "1   Coleoptera  #e81123\n",
              "2  Hymenoptera  #6917aa\n",
              "3      Diptera  #015cda\n",
              "4      Araneae  #4894fe\n",
              "5    Hemiptera  #257ffe\n",
              "6      Odonata  #257ffe"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f081cf63-9af6-4358-8579-304f3ecc0f0e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>color</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Lepidoptera</td>\n",
              "      <td>#5db300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Coleoptera</td>\n",
              "      <td>#e81123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hymenoptera</td>\n",
              "      <td>#6917aa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Diptera</td>\n",
              "      <td>#015cda</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Araneae</td>\n",
              "      <td>#4894fe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Hemiptera</td>\n",
              "      <td>#257ffe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Odonata</td>\n",
              "      <td>#257ffe</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f081cf63-9af6-4358-8579-304f3ecc0f0e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f081cf63-9af6-4358-8579-304f3ecc0f0e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f081cf63-9af6-4358-8579-304f3ecc0f0e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HNdMpKItVrll"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}