{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VdtRvVryHrSc",
        "NogqyrRUHhp7",
        "kpIBQk5GHkun"
      ],
      "mount_file_id": "1EkPO3-a7zWEBBox_OayTsTNDMvTNzzuS",
      "authorship_tag": "ABX9TyNbjiGW/ak3MzclTale7sua",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Projects/blob/main/Arthropod_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Arthropod Taxonomy Orders Object Detection Dataset](https://www.kaggle.com/datasets/mistag/arthropod-taxonomy-orders-object-detection-dataset/)\n",
        "\n",
        "\n",
        "\n",
        "###### Lepidoptera = butterfies and moths\n",
        "###### Hymenoptera = wasps, bees and ants\n",
        "###### Hemiptera = true bugs (cicadas, aphids, shield bugs, ...)\n",
        "###### Odonata = dragonflies\n",
        "###### Diptera = fies\n",
        "###### Araneae = spiders\n",
        "###### Coleoptera = beetles\n",
        "\n",
        "###### NOT IN DATASET\n",
        "###### Orthoptera = grasshoppers"
      ],
      "metadata": {
        "id": "VdtRvVryHrSc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Jc6VQxrK6Zz3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import pprint as pp\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib as mpl\n",
        "import concurrent.futures\n",
        "from threading import Lock\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "pp = pp.PrettyPrinter()\n",
        "\n",
        "!pip install --quiet tf-models-official==2.9.2\n",
        "sys.path.append('PATH_TO_TENSORFLOW_OBJECT_DETECTION_FOLDER')# load all the metadata\n",
        "\n",
        "import tensorflow_models as tfm\n",
        "from tensorflow_models.vision import box_ops as box"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR']='/content/drive/MyDrive/'\n",
        "!kaggle datasets download -d mistag/arthropod-taxonomy-orders-object-detection-dataset\n",
        "!unzip *.zip && rm *.zip"
      ],
      "metadata": {
        "id": "OEQKeqZl8mgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#part 1\n",
        "\n"
      ],
      "metadata": {
        "id": "O3md6o47lY_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PerProcessing"
      ],
      "metadata": {
        "id": "NogqyrRUHhp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGES_PER_SHARD = 481\n",
        "TARGET_WIDTH = 1024\n",
        "\n",
        "CLASSES = ['Lepidoptera', 'Hymenoptera', 'Hemiptera', 'Odonata', 'Diptera', 'Araneae', 'Coleoptera']\n",
        "\n",
        "RAW_CLASSES = CLASSES + ['_truncated', '_blurred', '_occluded']"
      ],
      "metadata": {
        "id": "Msyvk4_ZZPex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Progress:\n",
        "    \"\"\"Text mode progress bar.\n",
        "    Usage:\n",
        "            p = Progress(30)\n",
        "            p.step()\n",
        "            p.step()\n",
        "            p.step(start=True) # to restart form 0%\n",
        "    The progress bar displays a new header at each restart.\"\"\"\n",
        "    def __init__(self, maxi, size=100, msg=\"\"):\n",
        "        \"\"\"\n",
        "        :param maxi: the number of steps required to reach 100%\n",
        "        :param size: the number of characters taken on the screen by the progress bar\n",
        "        :param msg: the message displayed in the header of the progress bat\n",
        "        \"\"\"\n",
        "        self.maxi = maxi\n",
        "        self.p = self.__start_progress(maxi)()  # () to get the iterator from the generator\n",
        "        self.header_printed = False\n",
        "        self.msg = msg\n",
        "        self.size = size\n",
        "        self.lock = Lock()\n",
        "\n",
        "    def step(self, reset=False):\n",
        "        with self.lock:\n",
        "            if reset:\n",
        "                self.__init__(self.maxi, self.size, self.msg)\n",
        "            if not self.header_printed:\n",
        "                self.__print_header()\n",
        "            next(self.p)\n",
        "\n",
        "    def __print_header(self):\n",
        "        print()\n",
        "        format_string = \"0%{: ^\" + str(self.size - 6) + \"}100%\"\n",
        "        print(format_string.format(self.msg))\n",
        "        self.header_printed = True\n",
        "\n",
        "    def __start_progress(self, maxi):\n",
        "        def print_progress():\n",
        "            # Bresenham's algorithm. Yields the number of dots printed.\n",
        "            # This will always print 100 dots in max invocations.\n",
        "            dx = maxi\n",
        "            dy = self.size\n",
        "            d = dy - dx\n",
        "            for x in range(maxi):\n",
        "                k = 0\n",
        "                while d >= 0:\n",
        "                    print('=', end=\"\", flush=True)\n",
        "                    k += 1\n",
        "                    d -= dx\n",
        "                d += dy\n",
        "                yield k\n",
        "\n",
        "        return print_progress\n",
        "\n",
        "    \n",
        "def no_decorations(ax):\n",
        "    ax.axes.get_xaxis().set_visible(False)\n",
        "    ax.axes.get_yaxis().set_visible(False)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['bottom'].set_visible(False)\n",
        "    ax.spines['left'].set_visible(False)\n",
        "    \n",
        "\n",
        "def display_detections(images, offsets, resizes, detections, classnames, ground_truth_boxes=[]):\n",
        "    # scale and offset the detected boxes back to original image coordinates\n",
        "    boxes   = [[ (x,y,w,h)  for _, x, y, w, h, score, klass in detection_list] for detection_list in detections]\n",
        "    boxes   = [[ (x-ofs[1], y-ofs[0], w, h) for x,y,w,h in boxlist ] for boxlist, ofs in zip(boxes, offsets)]\n",
        "    boxes   = [[ (x*rsz, y*rsz, w*rsz, h*rsz) for x,y,w,h in boxlist ] for boxlist, rsz in zip(boxes, resizes)]\n",
        "    classes = [[ int(klass) for _, x, y, w, h, score, klass in detection_list] for detection_list in detections]\n",
        "    scores  = [[ score      for _, x, y, w, h, score, klass in detection_list] for detection_list in detections]\n",
        "    display_with_boxes(images, boxes, classes, scores, classnames, ground_truth_boxes)\n",
        "    \n",
        "    \n",
        "# images, boxes and classes must have the same number of elements\n",
        "# scores can be en empty list []. If it is not empty, it must also\n",
        "# have the same number of elements.\n",
        "# classnames is the list of possible classes (strings)\n",
        "def display_with_boxes(images, boxes, classes, scores, classnames, ground_truth_boxes=[]):\n",
        "    N = len(images)\n",
        "    sqrtN = int(np.ceil(np.sqrt(N)))\n",
        "    aspect = sum([im.shape[1]/im.shape[0] for im in images])/len(images) # mean aspect ratio of images\n",
        "    fig = plt.figure(figsize=(15,15/aspect), frameon=False)\n",
        "    \n",
        "    for k in range(N):\n",
        "        ax = plt.subplot(sqrtN, sqrtN, k+1)\n",
        "        no_decorations(ax)\n",
        "        plt.imshow(images[k])\n",
        "        \n",
        "        if ground_truth_boxes:\n",
        "            for box in ground_truth_boxes[k]:\n",
        "                x, y, w, h = (box[0], box[1], box[2]-box[0], box[3]-box[1]) # convert x1 y1 x2 y2 into xywh\n",
        "                #x, y, w, h = (box[0], box[1], box[2], box[3])\n",
        "                rect = mpl.patches.Rectangle((x, y),w,h,linewidth=4,edgecolor='#FFFFFFA0',facecolor='none')\n",
        "                ax.add_patch(rect)\n",
        "\n",
        "        for i, (box, klass) in enumerate(zip(boxes[k], classes[k])):\n",
        "            x, y, w, h = (box[0], box[1], box[2]-box[0], box[3]-box[1]) # convert x1 y1 x2 y2 into xywh\n",
        "            #x, y, w, h = (box[0], box[1], box[2], box[3])\n",
        "            #label = classnames[klass-1] # predicted classes are 1-based\n",
        "            label = classnames[klass]\n",
        "            if scores:\n",
        "                label += ' ' + str(int(scores[k][i]*100)) + '%' \n",
        "            rect = mpl.patches.Rectangle((x, y),w,h,linewidth=4,edgecolor='#00000080',facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "            rect = mpl.patches.Rectangle((x, y),w,h,linewidth=2,edgecolor='#FFFF00FF',facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "            plt.text(x, y, label, size=16, ha=\"left\", va=\"top\", color='#FFFF00FF',\n",
        "                     bbox=dict(boxstyle=\"round\", ec='#00000080', fc='#0000004E', linewidth=3) )\n",
        "            plt.text(x, y, label, size=16, ha=\"left\", va=\"top\", color='#FFFF00FF',\n",
        "                     bbox=dict(boxstyle=\"round\", ec='#FFFF00FF', fc='#0000004E', linewidth=1.5) )\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(wspace=0.02, hspace=0.02)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "w-S7WrgfBJaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/content/ArTaxOr/Araneae'\n",
        "json_filename_pattern = '/content/ArTaxOr/Araneae/annotations/*.json'\n",
        "jpeg_filename_pattern = '/content/ArTaxOr/Araneae/*.jpg'\n",
        "\n",
        "def load_json(filename, p):\n",
        "    p.step()\n",
        "    with tf.io.gfile.GFile(filename, 'r') as f:\n",
        "        return json.load(f)\n",
        "    \n",
        "def filename_key(filename):\n",
        "    path, filename = os.path.split(filename)\n",
        "    dirname = os.path.split(path)[1]\n",
        "    return filename\n",
        "    # return os.path.join(dirname, filename)\n",
        "    \n",
        "def load_metadata(filename_pattern, jpeg_filename_pattern):\n",
        "    print(\"Scanning directory...\", end=' ')\n",
        "    json_filenames = tf.io.gfile.glob(json_filename_pattern)\n",
        "    jpeg_filenames = tf.io.gfile.glob(jpeg_filename_pattern)\n",
        "    print(f\"found {len(json_filenames)} metadata files and {len(jpeg_filenames)} image files.\")\n",
        "    print(\"Loading metadata\")\n",
        "    p = Progress(len(json_filenames))\n",
        "    with concurrent.futures.ThreadPoolExecutor() as exe:\n",
        "        data = exe.map(lambda x: load_json(x,p), json_filenames)\n",
        "    # data as a dictionary for easier cross-referencing\n",
        "    data = {filename_key(d['asset']['path']):d for d in data}\n",
        "    return data, jpeg_filenames\n",
        "\n",
        "RAW_METADATA, JPEG_FILENAMES = load_metadata(json_filename_pattern, jpeg_filename_pattern)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tvvJWpPA5-a",
        "outputId": "0db70c4d-c90f-4d8c-987c-171b4ec8dc08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanning directory... found 2418 metadata files and 2418 image files.\n",
            "Loading metadata\n",
            "\n",
            "0%                                                                                              100%\n",
            "===================================================================================================="
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = JPEG_FILENAMES[0]\n",
        "RAW_METADATA[name[-16:]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31CkjYPus83g",
        "outputId": "34880684-a6b7-4eea-a0b7-8a82ce1dd69e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'asset': {'format': 'jpg',\n",
              "  'id': 'b25575a4d90e8922ecb815e684638555',\n",
              "  'name': '9cc9c7842d7b.jpg',\n",
              "  'path': 'file:F:/ArTaxOr/Araneae/9cc9c7842d7b.jpg',\n",
              "  'size': {'width': 5184, 'height': 3456},\n",
              "  'state': 2,\n",
              "  'type': 1},\n",
              " 'regions': [{'id': 'Sojpn0NLA',\n",
              "   'type': 'RECTANGLE',\n",
              "   'tags': ['Araneae'],\n",
              "   'boundingBox': {'height': 1272.3226473629782,\n",
              "    'width': 1848.3641379310345,\n",
              "    'left': 1805.4620689655173,\n",
              "    'top': 1277.1251292657703},\n",
              "   'points': [{'x': 1805.4620689655173, 'y': 1277.1251292657703},\n",
              "    {'x': 3653.826206896552, 'y': 1277.1251292657703},\n",
              "    {'x': 3653.826206896552, 'y': 2549.4477766287487},\n",
              "    {'x': 1805.4620689655173, 'y': 2549.4477766287487}]}],\n",
              " 'version': '2.1.0'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that there is a model garden safe way of computing image ids.\n",
        "# In the end, this does not matter. The ids are never used, but they must still be processed,\n",
        "# otherwise the Model Garden data loader will crash on them.\n",
        "\n",
        "# copied from Model Garden code\n",
        "# = official/vision/beta/dataloaders/utils process_source_id()\n",
        "\n",
        "def model_garden__str_to_int64(s):\n",
        "    return tf.strings.to_number(s, tf.int64).numpy()\n",
        "\n",
        "def compute_id_bytestring(s):\n",
        "    \n",
        "    computed_id = (int('0x' + s[:16], 16) ^ int('0x' + s[16:], 16)) & 0x0FFFFFFFFFFFFFFF\n",
        "    return str(computed_id).encode('utf-8')\n",
        "\n",
        "raw_ids = []\n",
        "cnv_ids = []\n",
        "cnv_cnv_ids = []\n",
        "\n",
        "for i, key in enumerate(RAW_METADATA):\n",
        "    iid = RAW_METADATA[key]['asset']['id']\n",
        "    raw_ids.append(iid)\n",
        "    iid = compute_id_bytestring(iid)\n",
        "    #print(\"computed:\", iid)\n",
        "    cnv_ids.append(iid)\n",
        "    iid = model_garden__str_to_int64(iid)\n",
        "    cnv_cnv_ids.append(iid)\n",
        "\n",
        "print(f\"Original ids: {len(raw_ids)} Uniques:{len(set(raw_ids))} Collisions:{len(raw_ids) - len(set(raw_ids))}\")\n",
        "print(f\"Original ids: {len(cnv_ids)} Uniques:{len(set(cnv_ids))} Collisions:{len(raw_ids) - len(set(cnv_ids))}\")\n",
        "print(f\"Computed ids as converted by Model Garden:{len(cnv_cnv_ids)}, Uniques: {len(set(cnv_cnv_ids))}, Collisions: {len(raw_ids) - len(set(cnv_cnv_ids))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4xyoVqD0tuU",
        "outputId": "92830f5d-e733-465a-893c-e85d2d9ea2e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original ids: 2418 Uniques:2418 Collisions:0\n",
            "Original ids: 2418 Uniques:2418 Collisions:0\n",
            "Computed ids as converted by Model Garden:2418, Uniques: 2418, Collisions: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# collect weird aspec ratio images and very large images\n",
        "oddities = [d['asset']['path'] for k,d in RAW_METADATA.items() if d['asset']['size']['width']>5400]\n",
        "oddities += [d['asset']['path'] for k,d in RAW_METADATA.items() if d['asset']['size']['height']>5000]\n",
        "oddities2 = [d['asset']['path'] for k,d in RAW_METADATA.items() if d['asset']['size']['height']/d['asset']['size']['width']>1.9]\n",
        "oddities2 += [d['asset']['path'] for k,d in RAW_METADATA.items() if d['asset']['size']['width']/d['asset']['size']['height']>2.0]\n",
        "\n",
        "nice_pics = ['/ArTaxOr/Lepidoptera/e7d7b4678088.jpg','/ArTaxOr/Lepidoptera/e74f298859ff.jpg','/ArTaxOr/Coleoptera/92c9a15e7362.jpg',\n",
        "             '/ArTaxOr/Coleoptera/a1824522fddc.jpg','/ArTaxOr/Hymenoptera/7188c0cc8c9d.jpg','/ArTaxOr/Lepidoptera/b7197aead30b.jpg',\n",
        "             '/ArTaxOr/Lepidoptera/dfc9ece476e6.jpg','/ArTaxOr/Araneae/a1488eb130e3.jpg','/ArTaxOr/Coleoptera/39c0eabccc41.jpg']\n",
        "alt_pics = ['/ArTaxOr/Araneae/3c6491416c3f.jpg','/ArTaxOr/Araneae/81ff08857d15.jpg',\n",
        "            '/ArTaxOr/Hymenoptera/f8f10bc28f5b.jpg','/ArTaxOr/Lepidoptera/e314c31efafd.jpg']\n",
        "\n",
        "#filenames = tf.io.gfile.glob('../input/arthropod-taxonomy-orders-object-detection-dataset/ArTaxOr/*/*jpg')\n",
        "#filenames = nice_pics\n",
        "filenames = oddities+oddities2\n",
        "#filenames = alt_pics\n",
        "\n",
        "# display a couple of images from the raw_metadata with their bounding boxes\n",
        "images = []\n",
        "bboxes = []\n",
        "taglists = []\n",
        "np.random.shuffle(filenames)\n",
        "for filename in filenames[:4]:\n",
        "    print(DATA_PATH, filename_key(filename), filename)\n",
        "    filepath = os.path.join(DATA_PATH, filename_key(filename))\n",
        "    d = RAW_METADATA[filename_key(filename)]\n",
        "    images.append(tf.image.decode_jpeg(tf.io.read_file(filepath)))\n",
        "    bbxs = [region['boundingBox'] for region in d['regions']]\n",
        "    # xywh to yxyx conversion\n",
        "    bbxs = [[box['left'],box['top'], box['left']+box['width'], box['top']+box['height']] for box in bbxs]\n",
        "    #tags = [region['tags'] for region in d['regions']] # all tags\n",
        "    tags = [region['tags'][0] for region in d['regions']] # first tag only\n",
        "    tags = [CLASSES.index(t) for t in tags] \n",
        "    bboxes.append(bbxs)\n",
        "    taglists.append(tags)\n",
        "\n",
        "display_with_boxes(images, bboxes, taglists, None, CLASSES, ground_truth_boxes=[])"
      ],
      "metadata": {
        "id": "HOgvmClK27Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Nb entries:', len(RAW_METADATA))\n",
        "print('Nb images:', len(JPEG_FILENAMES))\n",
        "assert(len(RAW_METADATA) == len(JPEG_FILENAMES))\n",
        "\n",
        "formats = set([d['asset']['format'] for k,d in RAW_METADATA.items()])\n",
        "print('Image formats:', formats)\n",
        "assert(len(formats)==1 and list(formats)[0]=='jpg')\n",
        "\n",
        "states = set([d['asset']['state'] for k,d in RAW_METADATA.items()])\n",
        "print('Image states(?):', states)\n",
        "types = set([d['asset']['type'] for k,d in RAW_METADATA.items()])\n",
        "print('Image types(?):', types)\n",
        "\n",
        "widths = [d['asset']['size']['width'] for k,d in RAW_METADATA.items()]\n",
        "print(f'Images widths range from {min(widths)} to {max(widths)}')\n",
        "heights = [d['asset']['size']['height'] for k,d in RAW_METADATA.items()]\n",
        "print(f'Images heights range from {min(heights)} to {max(heights)}')\n",
        "\n",
        "aspect_ratios = [w/h for w,h in zip(widths, heights)]\n",
        "print(f'Images aspect ratios range from {min(aspect_ratios)} to {max(aspect_ratios)}')\n",
        "\n",
        "nbbox = [len(d['regions']) for k,d in RAW_METADATA.items()]\n",
        "print(f'Nb of bounding boxes from {min(nbbox)} to {max(nbbox)}, average {sum(nbbox)/len(nbbox):.3}')\n",
        "\n",
        "region_types = [set([t['type'] for t in d['regions']]) for k,d in RAW_METADATA.items()]\n",
        "region_types = set().union(*region_types)\n",
        "print('Region types:', region_types)\n",
        "assert(len(region_types)==1 and list(region_types)[0]=='RECTANGLE')\n",
        "\n",
        "region_tags = [[t['tags'] for t in d['regions']] for k,d in RAW_METADATA.items()]\n",
        "region_tags = [sum(l,[]) for l in region_tags] # concat lists of tags across multiple regions\n",
        "region_tags = set(sum(region_tags, [])) # concat all and make a set\n",
        "print('Region tags:', region_tags)\n",
        "\n",
        "bbox_width = [[t['boundingBox']['width'] for t in d['regions']] for k,d in RAW_METADATA.items()]\n",
        "bbox_width = sum(bbox_width, []) # flatten the list\n",
        "print(f'Bounding box widths range from {min(bbox_width)} to {max(bbox_width)}')\n",
        "\n",
        "bbox_height = [[t['boundingBox']['height'] for t in d['regions']] for k,d in RAW_METADATA.items()]\n",
        "bbox_height = sum(bbox_height, []) # flatten the list\n",
        "print(f'Bounding box height range from {min(bbox_height)} to {max(bbox_height)}')\n",
        "\n",
        "bbox_left = [[t['boundingBox']['left'] for t in d['regions']] for k,d in RAW_METADATA.items()]\n",
        "bbox_left = sum(bbox_left, []) # flatten the list\n",
        "print(f'Bounding box left range from {min(bbox_left)} to {max(bbox_left)}')\n",
        "\n",
        "bbox_top = [[t['boundingBox']['top'] for t in d['regions']] for k,d in RAW_METADATA.items()]\n",
        "bbox_top = sum(bbox_top, []) # flatten the list\n",
        "print(f'Bounding box top range from {min(bbox_top)} to {max(bbox_top)}')\n",
        "\n",
        "# compute aspect ratios in landscape mode\n",
        "landscape_aspect_ratios = [w/h if w>h else h/w for w,h in zip(widths, heights)]\n",
        "print(f'Images landscape aspect ratios range from {min(landscape_aspect_ratios)} to {max(landscape_aspect_ratios)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i33c85W39HQ",
        "outputId": "02f4fe84-bc6d-41eb-ab7e-1f072353c1fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nb entries: 2418\n",
            "Nb images: 2418\n",
            "Image formats: {'jpg'}\n",
            "Image states(?): {2}\n",
            "Image types(?): {1}\n",
            "Images widths range from 500 to 5500\n",
            "Images heights range from 375 to 4272\n",
            "Images aspect ratios range from 0.4044418468731736 to 2.3703703703703702\n",
            "Nb of bounding boxes from 1 to 34, average 1.11\n",
            "Region types: {'RECTANGLE'}\n",
            "Region tags: {'_truncated', '_blurred', 'Lepidoptera', 'Araneae', '_occluded', 'Coleoptera', 'Hymenoptera', 'Diptera', 'Hemiptera'}\n",
            "Bounding box widths range from 29.42528735632184 to 3861.1984838042727\n",
            "Bounding box height range from 29.42528735632184 to 3453.654601861427\n",
            "Bounding box left range from 0 to 3980.0137835975193\n",
            "Bounding box top range from 0 to 3089.1127197518094\n",
            "Images landscape aspect ratios range from 1.0 to 2.472543352601156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2,ax3,ax4) = plt.subplots(1,4, figsize=(15,3))\n",
        "ax1.set_title(\"aspect ratios\")\n",
        "ax1.hist(aspect_ratios)\n",
        "ax2.set_title(\"landscape aspect ratios\")\n",
        "ax2.hist(landscape_aspect_ratios)\n",
        "ax3.set_title(\"widths\")\n",
        "ax3.hist(widths)\n",
        "ax4.set_title(\"heights\")\n",
        "ax4.hist(heights)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JjZRmW7L4yKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metadataset_from_image_filenames(filenames):\n",
        "    values = [RAW_METADATA[filename_key(f)] for f in filenames]\n",
        "\n",
        "    values = {'id': [filename_key(val['asset']['id']) for val in values],\n",
        "              'fname': [os.path.join(DATA_PATH, filename_key(val['asset']['path'])) for val in values],\n",
        "              'width':[val['asset']['size']['width'] for val in values],\n",
        "              'height':[val['asset']['size']['height'] for val in values],\n",
        "              # xywh to yxyx conversion ?\n",
        "              # internal box format: x1 y1 x2 y2 in pixel coordinates\n",
        "              'boxes': tf.ragged.constant([[(item['boundingBox']['left'],\n",
        "                                             item['boundingBox']['top'],\n",
        "                                             item['boundingBox']['left']+item['boundingBox']['width'],\n",
        "                                             item['boundingBox']['top']+item['boundingBox']['height']) \n",
        "                                            for item in val['regions']] for val in values], ragged_rank=1), # must specify ragged rank, othewise tf.ragged.constant\n",
        "                                                                               # does not detect that the last dim is 4 [x1 y1 x2 y2]\n",
        "               'tags': tf.ragged.constant([[item['tags'] for item in val['regions']] for val in values]),\n",
        "               'classes': tf.ragged.constant([[[CLASSES.index(tag) for tag in item['tags'] if tag in CLASSES] for item in val['regions']] for val in values])\n",
        "              }\n",
        "\n",
        "    metadataset = tf.data.Dataset.from_tensor_slices(values)\n",
        "    return metadataset"
      ],
      "metadata": {
        "id": "TROu85Ab6I57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_img(metadata):\n",
        "    compressed_data = tf.io.read_file(metadata['fname'])\n",
        "    image = tf.image.decode_jpeg(compressed_data, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32) # convert from uint8 [0,255] float [0,1)\n",
        "    return image, metadata\n",
        "\n",
        "def keep_one_tag_per_box(tags):\n",
        "    # Keeping first tag only. Other tags are not bug names but qualifiesrs like '_truncated', '_blurred', '_occluded'\n",
        "    # Input: one tag list per box: ex: [['Odonata', '_blurred'], ['Hymenoptera']] for 2 boxes\n",
        "    # This works for 0 boxes (tested)\n",
        "    tags = tags[:, 0:1] # Cannot index into ragged dimension, must do this.\n",
        "    tags = tags.merge_dims(-2,-1) # equivalent to tf.squeeze(tags, axis=-1) for ragged tensor\n",
        "    return tags\n",
        "\n",
        "# Resize images to MAX_WIDTH if they are larger. Adjust bounding boxes accordingly.\n",
        "def resize_image_and_boxes(im, metadata):\n",
        "    boxes = tf.cast(metadata['boxes'], tf.float32) # the number of boxes is ragged but for one image it is constant\n",
        "    image_size = tf.stack([metadata['width'], metadata['height']])\n",
        "    \n",
        "    scale_factor = tf.cast(TARGET_WIDTH, tf.float32) / tf.cast(image_size[0], tf.float32)\n",
        "    scale_factor = tf.math.minimum(scale_factor, 1.0)\n",
        "    new_image_size = tf.stack([tf.math.round(scale_factor * tf.cast(image_size[0], tf.float32)), \n",
        "                                tf.math.round(scale_factor * tf.cast(image_size[1], tf.float32))])\n",
        "    # resize image\n",
        "    im = tf.image.resize(im, tf.stack([image_size[1], TARGET_WIDTH]), preserve_aspect_ratio=True)\n",
        "    \n",
        "    # resize boxes\n",
        "    # boxes = box.normalize_boxes(boxes, tf.math.reciprocal(tf.cast(image_size, tf.float32)))\n",
        "    # boxes = box.normalize_boxes(boxes, new_image_size)\n",
        "    #boxes = tf.cast(tf.round(boxes), tf.int32)\n",
        "    boxes = box.normalize_boxes(boxes, tf.math.reciprocal(tf.cast(image_size, tf.float32)))\n",
        "    boxes = box.compute_outer_boxes(boxes, new_image_size)\n",
        "    # metadata\n",
        "    new_image_size = tf.cast(new_image_size, tf.int32)\n",
        "    classes = metadata['classes']\n",
        "    tags = metadata['tags']\n",
        "    ids = metadata['id'] \n",
        "    \n",
        "    return im, new_image_size, ids, boxes, tags, classes"
      ],
      "metadata": {
        "id": "eefp9xig6gWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filenames = JPEG_FILENAMES\n",
        "#filenames = alt_pics\n",
        "filenames = (oddities+oddities2).copy()\n",
        "np.random.shuffle(filenames)\n",
        "\n",
        "metadataset = metadataset_from_image_filenames(filenames)\n",
        "dataset = metadataset.map(decode_img, num_parallel_calls=AUTO)\n",
        "dataset = dataset.map(resize_image_and_boxes, num_parallel_calls=AUTO)\n",
        "\n",
        "dataset_iterator = iter(dataset.apply(tf.data.experimental.dense_to_ragged_batch(4)))"
      ],
      "metadata": {
        "id": "4OEPiMtX6k7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, new_image_size, names, boxes, tags, classes  = next(dataset_iterator)\n",
        "images = [im.numpy() for im in images] # must do this for ragged\n",
        "classes = [[classlist[0] for classlist in detection] for detection in classes] # keep first class only\n",
        "display_with_boxes(images, boxes, classes, None, CLASSES, ground_truth_boxes=[])"
      ],
      "metadata": {
        "id": "C7mfFDxK6np0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames_to_process = JPEG_FILENAMES.copy()\n",
        "\n",
        "OUTPUT_DIR = '../../arthropod-data-tfrec'\n",
        "OUTPUT_NAME_PREFIX = 'arthropods_'\n",
        "\n",
        "print(\"TFRecords output directory:\", OUTPUT_DIR) \n",
        "print(\"Images resizing target size:\", TARGET_WIDTH, 'px')\n",
        "print(\"Number of images:\", len(filenames_to_process))\n",
        "NB_SHARDS = -(-len(filenames_to_process)//IMAGES_PER_SHARD) # -- trick rounds up\n",
        "print(f\"Output sharded into {NB_SHARDS} files with {IMAGES_PER_SHARD} images per file, {len(filenames_to_process)-(NB_SHARDS-1)*IMAGES_PER_SHARD} images in last file\")"
      ],
      "metadata": {
        "id": "EC7AJ4HWBcES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a789c78a-7265-4986-b7d7-55a1cd7e683e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFRecords output directory: ../../arthropod-data-tfrec\n",
            "Images resizing target size: 1024 px\n",
            "Number of images: 2418\n",
            "Output sharded into 6 files with 481 images per file, 13 images in last file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TFRecord helper functions\n",
        "def _bytes_feature(value):\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value)) # WARNING: this expects a list of byte strings, not a list of bytes!\n",
        "def _float_feature(value):\n",
        "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
        "def _int_feature(value):\n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
        "\n",
        "def to_tfrecord(tfrec_filewriter, img_bytes, width, height, ids, boxes, labels):\n",
        "    feature = {\n",
        "        \"image/encoded\": _bytes_feature([img_bytes]), # compressed image bytes\n",
        "        \"image/source_id\": _bytes_feature([ids]),    # string\n",
        "        \"image/width\": _int_feature([width]),         # image width\n",
        "        \"image/height\": _int_feature([height]),       # image height\n",
        "        \"image/object/bbox/xmin\": _float_feature(boxes[:,0]),\n",
        "        \"image/object/bbox/ymin\": _float_feature(boxes[:,1]),\n",
        "        \"image/object/bbox/xmax\": _float_feature(boxes[:,2]),\n",
        "        \"image/object/bbox/ymax\": _float_feature(boxes[:,3]),\n",
        "        #\"nb_tags\": _int_feature(nb_tags),     # nb of tags per box\n",
        "        \"image/object/class/label\": _int_feature(labels) # flat list of labels\n",
        "    }\n",
        "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n",
        "def recompress_jpeg(image):\n",
        "    image = tf.image.convert_image_dtype(image, tf.uint8) # convert from float [0,1) to uint8 [0,255]\n",
        "    return tf.image.encode_jpeg(image, optimize_size=True, chroma_downsampling=False)\n",
        "\n",
        "# Model Garden specific format adjustments\n",
        "\n",
        "# Model Garden models expect normalized box coordinates in [0..1] range\n",
        "def normalize_boxes(boxes, image_size):\n",
        "    boxes = box.normalize_boxes(boxes, tf.math.reciprocal(tf.cast(image_size, tf.float32)))\n",
        "    return boxes\n",
        "\n",
        "# Model Garden models expect target classes to be 1-based because they reserve class 0 for backgrounds.\n",
        "def labels_1_based(labels):\n",
        "    return labels+1"
      ],
      "metadata": {
        "id": "q0Telv4YBi3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.shuffle(filenames_to_process)\n",
        "metadataset = metadataset_from_image_filenames(filenames_to_process)\n",
        "dataset = metadataset.map(decode_img, num_parallel_calls=AUTO)\n",
        "dataset = dataset.map(resize_image_and_boxes, num_parallel_calls=AUTO)\n",
        "\n",
        "# apply the Model Garden specific format adjustments\n",
        "dataset = dataset.map(lambda image, size, ids, boxes, tags, classes:\n",
        "                      (recompress_jpeg(image), size, ids,\n",
        "                       normalize_boxes(boxes, size), # Model Garden models expect normalized box coordinates in [0..1] range\n",
        "                       labels_1_based(keep_one_tag_per_box(classes))), num_parallel_calls=AUTO)\n",
        "\n",
        "dataset = dataset.apply(tf.data.experimental.dense_to_ragged_batch(IMAGES_PER_SHARD)) # use the batch size as the shard size"
      ],
      "metadata": {
        "id": "g96gl5HrCUG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SHARDS = [4,8,10,15,17,20,27,31]  # these files will be labeled \"test\" other files \"train\"\n",
        "\n",
        "if os.path.isdir(OUTPUT_DIR) and len(os.listdir(OUTPUT_DIR))>0:\n",
        "    print(\"ERROR: the output directory exists and is not empty. Aborting. Please empty the output directory manually before proceeding.\")\n",
        "else:\n",
        "    if not os.path.isdir(OUTPUT_DIR):\n",
        "        os.mkdir(OUTPUT_DIR)\n",
        "\n",
        "    print(\"Writing TFRecords\")\n",
        "    for shard, (image, size, ids, boxes, classes) in enumerate(dataset):\n",
        "        shard_size = image.numpy().shape[0] # batch size is shard size\n",
        "        \n",
        "        # use a standard shard naming convention with the number of images and the number of shards\n",
        "        #filename = OUTPUT_NAME_PREFIX+\"{}x{}px_{:03d}_of_{:03d}-{:03d}.tfrec\".format(\n",
        "        #    TARGET_SIZE[0], TARGET_SIZE[1], shard+1, NB_SHARDS, shard_size)\n",
        "        filename = OUTPUT_NAME_PREFIX+\"w{}px_{:03d}_of_{:03d}-{:03d}.{}.tfrec\".format(\n",
        "            TARGET_WIDTH, shard+1, NB_SHARDS, shard_size, 'test' if shard in TEST_SHARDS else 'train')\n",
        "        \n",
        "        with tf.io.TFRecordWriter(os.path.join(OUTPUT_DIR, filename)) as file:\n",
        "            for i in range(shard_size):\n",
        "                \n",
        "                binary_image   = image[i].numpy()\n",
        "                binary_id      = compute_id_bytestring(ids[i].numpy().decode('utf-8')) # Model Garden data loader fails if id is too long\n",
        "                binary_width   = size[i].numpy()[0]\n",
        "                binary_height  = size[i].numpy()[1]\n",
        "                binary_boxes   = boxes[i].numpy()                  # coordinates in x1 y1 x2 y2 format\n",
        "                #binary_nb_tags = tags[i].row_lengths().numpy()    # ragged to: nb of labels per box\n",
        "                #binary_tags    = tags[i].flat_values.numpy()      # ragged to: flat list of labels\n",
        "                binary_labels  = classes[i].numpy()                # list of labels (one label per box)\n",
        "                \n",
        "                example = to_tfrecord(file, binary_image, binary_width, binary_height, binary_id, binary_boxes, binary_labels)\n",
        "                file.write(example.SerializeToString())\n",
        "        print(\"Wrote file {} containing {} records\".format(filename, shard_size))"
      ],
      "metadata": {
        "id": "Urq9dXb8Cbys",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6172bfed-8bf8-4076-a969-0bc4b05676bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing TFRecords\n",
            "Wrote file arthropods_w1024px_001_of_006-481.train.tfrec containing 481 records\n",
            "Wrote file arthropods_w1024px_002_of_006-481.train.tfrec containing 481 records\n",
            "Wrote file arthropods_w1024px_003_of_006-481.train.tfrec containing 481 records\n",
            "Wrote file arthropods_w1024px_004_of_006-481.train.tfrec containing 481 records\n",
            "Wrote file arthropods_w1024px_005_of_006-481.test.tfrec containing 481 records\n",
            "Wrote file arthropods_w1024px_006_of_006-013.train.tfrec containing 13 records\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_tfrecord(example):\n",
        "    feature = {\n",
        "        \"image/encoded\": tf.io.FixedLenFeature([], tf.string), # compressed image bytes\n",
        "        \"image/source_id\": tf.io.FixedLenFeature([], tf.string),  # string\n",
        "        \"image/width\": tf.io.FixedLenFeature([], tf.int64),\n",
        "        \"image/height\": tf.io.FixedLenFeature([], tf.int64),\n",
        "        \"image/object/bbox/xmin\": tf.io.VarLenFeature(tf.float32),\n",
        "        \"image/object/bbox/ymin\": tf.io.VarLenFeature(tf.float32),\n",
        "        \"image/object/bbox/xmax\": tf.io.VarLenFeature(tf.float32),\n",
        "        \"image/object/bbox/ymax\": tf.io.VarLenFeature(tf.float32),\n",
        "        \"image/object/class/label\": tf.io.VarLenFeature(tf.int64) # one tag per box\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, feature)\n",
        "    source_id = example['image/source_id']\n",
        "    image = tf.image.decode_jpeg(example['image/encoded'], channels=3)\n",
        "    boxes_xmin = tf.sparse.to_dense(example['image/object/bbox/xmin'])\n",
        "    boxes_xmax = tf.sparse.to_dense(example['image/object/bbox/xmax'])\n",
        "    boxes_ymin = tf.sparse.to_dense(example['image/object/bbox/ymin'])\n",
        "    boxes_ymax = tf.sparse.to_dense(example['image/object/bbox/ymax'])\n",
        "    boxes = tf.stack([boxes_xmin, boxes_ymin, boxes_xmax, boxes_ymax], axis=-1)\n",
        "    labels = tf.sparse.to_dense(example['image/object/class/label'])\n",
        "    return image, source_id, boxes, labels\n",
        "    \n",
        "def load_tfrecord_dataset(filenames):\n",
        "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
        "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
        "\n",
        "    ignore_order = tf.data.Options()\n",
        "    ignore_order.experimental_deterministic = False\n",
        "\n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
        "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
        "    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "thKpjDxeCkdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = tf.io.gfile.glob(os.path.join(OUTPUT_DIR, '*.tfrec'))\n",
        "dataset = load_tfrecord_dataset(filenames)\n",
        "dataset_iterator = iter(dataset.apply(tf.data.experimental.dense_to_ragged_batch(4)))"
      ],
      "metadata": {
        "id": "LEDdL-euCkXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, source_id, boxes, labels = next(dataset_iterator)\n",
        "images = [im.numpy() for im in images] # must do this for ragged\n",
        "\n",
        "# must scale the rois back to pixels for visualizatoin\n",
        "image_shapes = [tf.cast((image.shape[1], image.shape[0]), tf.float32) for image in images]\n",
        "boxes = [box.normalize_boxes(bbox, image_shapes[i]) for i,bbox in enumerate(boxes)]\n",
        "\n",
        "# must shift labels back to 0-based for visualization\n",
        "labels -= 1\n",
        "#classes = [[CLASSES.index(label) for label in batchitem] for batchitem in labels]\n",
        "display_with_boxes(images, boxes, labels, None, CLASSES, ground_truth_boxes=[])"
      ],
      "metadata": {
        "id": "HNBRvcgKCzm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yixLsO-68wg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "kpIBQk5GHkun"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GCS bucket\n",
        "\n",
        "This bucket will receive:\n",
        "\n",
        "Tensorboard summaries that allow you to follow the training\n",
        "\n",
        "checkpoints\n",
        "\n",
        "the saved model after training"
      ],
      "metadata": {
        "id": "Wk5mCQ1aOx-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use your own GCS bucket here. GCS is required if training on TPU.\n",
        "# On GPU, a local folder will work.\n",
        "MODEL_ARTIFACT_BUCKET = 'gs://ml1-demo-martin/arthropod_jobs/'\n",
        "MODEL_DIR = MODEL_ARTIFACT_BUCKET + str(int(time.time()))\n",
        "\n",
        "# If you are running on Colaboratory, you must authenticate\n",
        "# for Colab to have write access to the bucket.\n",
        "\n",
        "# this is always set on Colab, the value is 0 or 1 depending on GPU presence\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()"
      ],
      "metadata": {
        "id": "2wZerY3eHnyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TPU / GPU detection\n"
      ],
      "metadata": {
        "id": "lh6tW-OWPUDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try: # detect TPUs\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except ValueError: # detect GPUs or multi-GPU machines\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "print(f'Replicas: {strategy.num_replicas_in_sync}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDewTPEvO0-z",
        "outputId": "663534c0-9ad6-4231-f973-ae49b80904e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replicas: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuration"
      ],
      "metadata": {
        "id": "WHJuc7UFPyFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DATA_PATH_PATTERN = 'gs://practical-ml-vision-book/arthropod_detection_tfr/size_w1024px/*.train.tfrec'\n",
        "VALID_DATA_PATH_PATTERN = 'gs://practical-ml-vision-book/arthropod_detection_tfr/size_w1024px/*.test.tfrec'\n",
        "SPINET_MOBILE_CHECKPOINT = 'gs://practical-ml-vision-book/arthropod_detection_tfr/spinenet49mobile_checkpoint'\n",
        "\n",
        "BATCH_SIZE = 32 * strategy.num_replicas_in_sync\n",
        "\n",
        "EPOCHS = 80\n",
        "\n",
        "CLASSES = ['Lepidoptera', 'Hymenoptera', 'Hemiptera', 'Odonata', 'Diptera', 'Araneae', 'Coleoptera']\n",
        "\n",
        "RAW_CLASSES = CLASSES + ['_truncated', '_blurred', '_occluded']"
      ],
      "metadata": {
        "id": "HTNPlj40O7wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
        "data_items_counter = lambda files: int(np.sum([int(re.compile(r\"-([0-9]*)\\.\").search(f).group(1)) for f in files]))\n",
        "\n",
        "TRAIN_FILENAMES = tf.io.gfile.glob(TRAIN_DATA_PATH_PATTERN)\n",
        "TRAIN_IMAGES_COUNT = data_items_counter(TRAIN_FILENAMES)\n",
        "STEPS_PER_EPOCH = TRAIN_IMAGES_COUNT // BATCH_SIZE\n",
        "\n",
        "VALID_FILENAMES = tf.io.gfile.glob(VALID_DATA_PATH_PATTERN)\n",
        "VALID_IMAGES_COUNT = data_items_counter(VALID_FILENAMES)\n",
        "VALID_STEPS = VALID_IMAGES_COUNT // BATCH_SIZE\n",
        "\n",
        "print(f\"\"\"Training dataset:\n",
        "    {len(TRAIN_FILENAMES)}: TFRecord files.)\n",
        "    {TRAIN_IMAGES_COUNT}: images)\n",
        "    Steps per epoch: {STEPS_PER_EPOCH})\n",
        ")\n",
        "Validation dataset:)\n",
        "    {len(VALID_FILENAMES)}: TFRecord files.)\n",
        "    {VALID_IMAGES_COUNT}: images)\n",
        "    Validation steps: {VALID_STEPS})\n",
        "\n",
        "Global batch size: {BATCH_SIZE}\n",
        "    \"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbFRn1hWP6OT",
        "outputId": "2cd0b1ef-77fe-4c26-cce5-f833542e5847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset:\n",
            "    24: TFRecord files.)\n",
            "    11544: images)\n",
            "    Steps per epoch: 360)\n",
            ")\n",
            "Validation dataset:)\n",
            "    8: TFRecord files.)\n",
            "    3832: images)\n",
            "    Validation steps: 119)\n",
            "\n",
            "Global batch size: 32\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model configuration"
      ],
      "metadata": {
        "id": "9dCvhd_WRRjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = [384, 384]\n",
        "\n",
        "# default parameters can be overriden in two ways:\n",
        "# 1) params.override({'task': {'model': {'backbone': backbone_cfg.as_dict()}}})\n",
        "# 2) params.task.model.backbone = backbone_cfg\n",
        "# params.override checks that the dictionary keys exist\n",
        "# the second options will silently add new keys\n",
        "\n",
        "params = tfm.core.exp_factory.get_exp_config('retinanet')\n",
        "\n",
        "params.task.model.num_classes = len(CLASSES)+1 # class 0 is reserved for backgrounds\n",
        "params.task.model.input_size = [*IMAGE_SIZE, 3] # this automatically configures the input reader to random crop training images\n",
        "params.task.init_checkpoint = SPINET_MOBILE_CHECKPOINT\n",
        "params.task.init_checkpoint_modules = 'backbone'\n",
        "params.task.model.backbone = tfm.vision.configs.backbones.Backbone(type='spinenet_mobile',\n",
        "                                                                   spinenet_mobile=tfm.vision.configs.backbones.SpineNetMobile())\n",
        "\n",
        "train_data_cfg=tfm.vision.configs.retinanet.DataConfig(\n",
        "    input_path=TRAIN_DATA_PATH_PATTERN,\n",
        "    is_training=True,\n",
        "    global_batch_size=BATCH_SIZE,\n",
        "    parser=tfm.vision.configs.retinanet.Parser(aug_rand_hflip=True, aug_scale_min=0.7, aug_scale_max=2.0))\n",
        "\n",
        "valid_data_cfg=tfm.vision.configs.retinanet.DataConfig(\n",
        "    input_path=VALID_DATA_PATH_PATTERN,\n",
        "    is_training=False,\n",
        "    global_batch_size=BATCH_SIZE)\n",
        "\n",
        "# params.override({'task': {'model': {'backbone': backbone_cfg.as_dict()}}})\n",
        "params.override({'task': {'train_data': train_data_cfg.as_dict(), 'validation_data': valid_data_cfg.as_dict()}})\n",
        "\n",
        "trainer_cfg=tfm.core.config_definitions.TrainerConfig(\n",
        "    train_steps=EPOCHS * STEPS_PER_EPOCH,\n",
        "    validation_steps=VALID_STEPS,\n",
        "    validation_interval=8*STEPS_PER_EPOCH,\n",
        "    steps_per_loop=STEPS_PER_EPOCH,\n",
        "    summary_interval=STEPS_PER_EPOCH,\n",
        "    checkpoint_interval=8*STEPS_PER_EPOCH)\n",
        "\n",
        "optim_cfg = tfm.optimization.OptimizationConfig({\n",
        "    'optimizer': {\n",
        "                  'type': 'sgd',\n",
        "                  'sgd': {'momentum': 0.9}},\n",
        "    'learning_rate': {'type': 'stepwise',\n",
        "                      'stepwise': {'boundaries': [15 * STEPS_PER_EPOCH,\n",
        "                                                  30 * STEPS_PER_EPOCH,\n",
        "                                                  45 * STEPS_PER_EPOCH,\n",
        "                                                  60 * STEPS_PER_EPOCH,\n",
        "                                                  75 * STEPS_PER_EPOCH],\n",
        "                                   'values': [0.016, #0.01,\n",
        "                                              0.008, #0.005,\n",
        "                                              0.004, #0.0025,\n",
        "                                              0.002, #0.001,\n",
        "                                              0.001, #0.0005,\n",
        "                                              0.0005]} #0.00025]}\n",
        "                     },\n",
        "    #'warmup': {'type': 'linear','linear': {'warmup_steps': 5*STEPS_PER_EPOCH, 'warmup_learning_rate': 0.00001}}\n",
        "})\n",
        "\n",
        "trainer_cfg.override({'optimizer_config': optim_cfg})\n",
        "params.override({'trainer': trainer_cfg})\n",
        "\n",
        "pp.pprint(params.as_dict())"
      ],
      "metadata": {
        "id": "0k4bWzqdQL2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = tfm.core.task_factory.get_task(params.task, logging_dir=MODEL_DIR)\n",
        "\n",
        "# this works too:\n",
        "#task = official.vision.beta.tasks.retinanet.RetinaNetTask(params.task)\n",
        "\n",
        "# this returns a RetinaNetModel\n",
        "#task.build_model()\n",
        "# note: none of the expected model functionalities work: model.fit(), model.predict(), model.save()\n",
        "\n",
        "# this returns the training dataset\n",
        "#train_dataset = task.build_inputs(train_data_cfg)\n",
        "# note: the dataset already includes FPN level and anchor pairing and is therefore not very readable\n",
        "\n",
        "# this returns the validation dataset\n",
        "#valid_dataset = task.build_inputs(valid_data_cfg)\n",
        "# note: the dataset already includes FPN level and anchor pairing and is therefore not very readable\n",
        "\n",
        "# this code allows you to see if the TFRecord fields are read correctly\n",
        "#ds = tf.data.TFRecordDataset(tf.io.gfile.glob(TRAIN_DATA_PATH_PATTERN))\n",
        "#dec = official.vision.beta.dataloaders.tf_example_decoder.TfExampleDecoder()\n",
        "#ds = ds.map(dec.decode)\n",
        "\n",
        "# training and validatoin data parsing happens in:\n",
        "# official.vision.beta.dataloaders.retinanet_input.Parser._parse_train_data\n",
        "# official.vision.beta.dataloaders.retinanet_input.Parser._parse_eval_data\n",
        "# official.vision.beta.dataloaders.Parser.parse() # dispatches between _parse_train_data and _parse_eval_data"
      ],
      "metadata": {
        "id": "ABQ6_J6WRU83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "mUMtdkosSHmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(MODEL_DIR)\n",
        "model,_ = tfm.core.train_lib.run_experiment(\n",
        "    distribution_strategy=strategy,\n",
        "    task=task,\n",
        "    mode=\"train_and_eval\", # 'train', 'eval', 'train_and_eval' or 'continuous_eval'\n",
        "    params=params,\n",
        "    model_dir=MODEL_DIR)"
      ],
      "metadata": {
        "id": "A2D8JVOCR-Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export the model"
      ],
      "metadata": {
        "id": "zG6RxegUSOne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfm.vision.serving.export_saved_model_lib.export_inference_graph(\n",
        "      input_type='image_tensor',\n",
        "      batch_size=4,\n",
        "      input_image_size=IMAGE_SIZE,\n",
        "      params=params,\n",
        "      checkpoint_path=MODEL_DIR,\n",
        "      export_dir=MODEL_DIR,\n",
        "      export_checkpoint_subdir='saved_chkpt',\n",
        "      export_saved_model_subdir='saved_model')"
      ],
      "metadata": {
        "id": "oXAxcs3uSJ1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Part 2](https://www.kaggle.com/code/mistag/starter-arthropod-taxonomy-orders-data-exploring)"
      ],
      "metadata": {
        "id": "vPKPYklNHk92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Data Exploring](https://www.kaggle.com/code/mistag/starter-arthropod-taxonomy-orders-data-exploring)\n",
        "\n",
        "The Arthropod Taxonomy Orders dataset is a collection of highres images annotated with labels from the taxanomy rank order. Annotations have been made with VoTT. VoTT stores all metadata in json files. In this kernel we will import all the metadata into DataFrames and export metadata to a variety of formats for object detection model training.\n",
        "The dataset is distributed under CC BY-NC-SA 4.0"
      ],
      "metadata": {
        "id": "uh4YUVG9NG1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "QHsv6SBlHli_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "pfiles = glob.glob('/content/ArTaxOr/**/*.vott', recursive=True)\n",
        "\n",
        "df = pd.DataFrame()\n",
        "for p in pfiles:\n",
        "  print(p)\n",
        "  with open(p) as f:\n",
        "    pdata = json.load(f)\n",
        "    print(pdata)\n",
        "    df = df.append(pd.DataFrame(list(pdata['assets'].values())), ignore_index=True)\n",
        "\n",
        "df['path'] = df['path'].str.replace('file:F:/', '')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA9dMUdJNyi7",
        "outputId": "2be23202-b9c2-4081-d522-cc15eff299a3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "OTtv5Gz0btMp",
        "outputId": "778b8cfe-c10b-4257-af5f-691c007afcfc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      format                                id              name  \\\n",
              "0        jpg  d0469bd539bde1b15b0acf2cd0d87f9d  0019ce6cda02.jpg   \n",
              "1        jpg  679107280d2f692152b436167c164018  003eb3db1665.jpg   \n",
              "2        jpg  f662e6835a6640dce287f3cf5f1373c6  00594c648f4d.jpg   \n",
              "3        jpg  de01228a5521101e5fd9164a1a6cf8cc  007e1a3a7667.jpg   \n",
              "4        jpg  be2462acc54305b1a0a88c69cc17c273  00c50b891171.jpg   \n",
              "...      ...                               ...               ...   \n",
              "15371    jpg  24e2e1ca2ddb72f55f54f3a5d2021dc8  ff1eb74031d7.jpg   \n",
              "15372    jpg  7895d0320b08b0766896543ab39e45e1  ff23e039add3.jpg   \n",
              "15373    jpg  104281193a8a3f23df7afda5769bb52b  ff28265dd0e0.jpg   \n",
              "15374    jpg  8c5758620dab8c8920c5a71e3d97e0e1  ff96a92dfc5d.jpg   \n",
              "15375    jpg  d40547b9e691836d86d7c44cdd6c1bdd  fffd85b67165.jpg   \n",
              "\n",
              "                                       path                             size  \\\n",
              "0          ArTaxOr/Araneae/0019ce6cda02.jpg  {'width': 1100, 'height': 1588}   \n",
              "1          ArTaxOr/Araneae/003eb3db1665.jpg   {'width': 1280, 'height': 960}   \n",
              "2          ArTaxOr/Araneae/00594c648f4d.jpg  {'width': 2048, 'height': 1370}   \n",
              "3          ArTaxOr/Araneae/007e1a3a7667.jpg  {'width': 1128, 'height': 1349}   \n",
              "4          ArTaxOr/Araneae/00c50b891171.jpg  {'width': 1344, 'height': 2048}   \n",
              "...                                     ...                              ...   \n",
              "15371  ArTaxOr/Hymenoptera/ff1eb74031d7.jpg  {'width': 1728, 'height': 1405}   \n",
              "15372  ArTaxOr/Hymenoptera/ff23e039add3.jpg  {'width': 2048, 'height': 1959}   \n",
              "15373  ArTaxOr/Hymenoptera/ff28265dd0e0.jpg  {'width': 2048, 'height': 1365}   \n",
              "15374  ArTaxOr/Hymenoptera/ff96a92dfc5d.jpg  {'width': 1902, 'height': 1504}   \n",
              "15375  ArTaxOr/Hymenoptera/fffd85b67165.jpg  {'width': 2048, 'height': 1339}   \n",
              "\n",
              "       state  type  \n",
              "0          2     1  \n",
              "1          2     1  \n",
              "2          2     1  \n",
              "3          2     1  \n",
              "4          2     1  \n",
              "...      ...   ...  \n",
              "15371      2     1  \n",
              "15372      2     1  \n",
              "15373      2     1  \n",
              "15374      2     1  \n",
              "15375      2     1  \n",
              "\n",
              "[15376 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ee0cf959-e1ea-431b-b2cd-770dfe601abb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>format</th>\n",
              "      <th>id</th>\n",
              "      <th>name</th>\n",
              "      <th>path</th>\n",
              "      <th>size</th>\n",
              "      <th>state</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>jpg</td>\n",
              "      <td>d0469bd539bde1b15b0acf2cd0d87f9d</td>\n",
              "      <td>0019ce6cda02.jpg</td>\n",
              "      <td>ArTaxOr/Araneae/0019ce6cda02.jpg</td>\n",
              "      <td>{'width': 1100, 'height': 1588}</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>jpg</td>\n",
              "      <td>679107280d2f692152b436167c164018</td>\n",
              "      <td>003eb3db1665.jpg</td>\n",
              "      <td>ArTaxOr/Araneae/003eb3db1665.jpg</td>\n",
              "      <td>{'width': 1280, 'height': 960}</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>jpg</td>\n",
              "      <td>f662e6835a6640dce287f3cf5f1373c6</td>\n",
              "      <td>00594c648f4d.jpg</td>\n",
              "      <td>ArTaxOr/Araneae/00594c648f4d.jpg</td>\n",
              "      <td>{'width': 2048, 'height': 1370}</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>jpg</td>\n",
              "      <td>de01228a5521101e5fd9164a1a6cf8cc</td>\n",
              "      <td>007e1a3a7667.jpg</td>\n",
              "      <td>ArTaxOr/Araneae/007e1a3a7667.jpg</td>\n",
              "      <td>{'width': 1128, 'height': 1349}</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jpg</td>\n",
              "      <td>be2462acc54305b1a0a88c69cc17c273</td>\n",
              "      <td>00c50b891171.jpg</td>\n",
              "      <td>ArTaxOr/Araneae/00c50b891171.jpg</td>\n",
              "      <td>{'width': 1344, 'height': 2048}</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15371</th>\n",
              "      <td>jpg</td>\n",
              "      <td>24e2e1ca2ddb72f55f54f3a5d2021dc8</td>\n",
              "      <td>ff1eb74031d7.jpg</td>\n",
              "      <td>ArTaxOr/Hymenoptera/ff1eb74031d7.jpg</td>\n",
              "      <td>{'width': 1728, 'height': 1405}</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15372</th>\n",
              "      <td>jpg</td>\n",
              "      <td>7895d0320b08b0766896543ab39e45e1</td>\n",
              "      <td>ff23e039add3.jpg</td>\n",
              "      <td>ArTaxOr/Hymenoptera/ff23e039add3.jpg</td>\n",
              "      <td>{'width': 2048, 'height': 1959}</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15373</th>\n",
              "      <td>jpg</td>\n",
              "      <td>104281193a8a3f23df7afda5769bb52b</td>\n",
              "      <td>ff28265dd0e0.jpg</td>\n",
              "      <td>ArTaxOr/Hymenoptera/ff28265dd0e0.jpg</td>\n",
              "      <td>{'width': 2048, 'height': 1365}</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15374</th>\n",
              "      <td>jpg</td>\n",
              "      <td>8c5758620dab8c8920c5a71e3d97e0e1</td>\n",
              "      <td>ff96a92dfc5d.jpg</td>\n",
              "      <td>ArTaxOr/Hymenoptera/ff96a92dfc5d.jpg</td>\n",
              "      <td>{'width': 1902, 'height': 1504}</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15375</th>\n",
              "      <td>jpg</td>\n",
              "      <td>d40547b9e691836d86d7c44cdd6c1bdd</td>\n",
              "      <td>fffd85b67165.jpg</td>\n",
              "      <td>ArTaxOr/Hymenoptera/fffd85b67165.jpg</td>\n",
              "      <td>{'width': 2048, 'height': 1339}</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15376 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee0cf959-e1ea-431b-b2cd-770dfe601abb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ee0cf959-e1ea-431b-b2cd-770dfe601abb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ee0cf959-e1ea-431b-b2cd-770dfe601abb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = pd.DataFrame(list(pdata['tags']))[:-3]\n",
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "tCBoBkXWUgtJ",
        "outputId": "55ff53f8-75f2-404c-b5b0-53c0d9cc0303"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          name    color\n",
              "0  Lepidoptera  #5db300\n",
              "1   Coleoptera  #e81123\n",
              "2  Hymenoptera  #6917aa\n",
              "3      Diptera  #015cda\n",
              "4      Araneae  #4894fe\n",
              "5    Hemiptera  #257ffe\n",
              "6      Odonata  #257ffe"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-145a3bba-9576-40f2-a294-cd00847b4761\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>color</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Lepidoptera</td>\n",
              "      <td>#5db300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Coleoptera</td>\n",
              "      <td>#e81123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hymenoptera</td>\n",
              "      <td>#6917aa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Diptera</td>\n",
              "      <td>#015cda</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Araneae</td>\n",
              "      <td>#4894fe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Hemiptera</td>\n",
              "      <td>#257ffe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Odonata</td>\n",
              "      <td>#257ffe</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-145a3bba-9576-40f2-a294-cd00847b4761')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-145a3bba-9576-40f2-a294-cd00847b4761 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-145a3bba-9576-40f2-a294-cd00847b4761');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = 'ArTaxOr/Diptera/fff86b0ae807.jpg'\n",
        "index = p[::-1].find('/')+1\n",
        "p[:-index]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "TwxU2QuBjzvE",
        "outputId": "3f283308-6c7f-47da-8057-1b0595ddf831"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ArTaxOr/Diptera'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "anno=pd.DataFrame(columns=['label', 'label_idx', 'xres', 'yres', 'height', 'width', 'left', 'top', \n",
        "                           'right', 'bottom', 'area', 'xcenter', 'ycenter', 'blurred',\n",
        "                           'occluded', 'truncated', 'file', 'id'])\n",
        "\n",
        "\n",
        "#get all the image's annotation details (object data) from a json file and store it in a dataframe\n",
        "for i in range(len(df)):  #15376 images(with details)\n",
        "  p = df['path'][i]\n",
        "  index = p[::-1].find('/')+1\n",
        "  json_file=f'/content/{p[:-index]}/annotations/{df[\"id\"][i]}-asset.json'\n",
        "\n",
        "  if os.path.isfile(json_file):\n",
        "    with open(json_file) as f:\n",
        "      adata = json.load(f)\n",
        "    xres=adata['asset']['size']['width']\n",
        "    yres=adata['asset']['size']['height'] \n",
        "    for j in range(len(adata['regions'])):\n",
        "            h=adata['regions'][j]['boundingBox']['height']/yres\n",
        "            w=adata['regions'][j]['boundingBox']['width']/xres\n",
        "            tags=adata['regions'][j]['tags']\n",
        "            anno=anno.append({'label': tags[0],\n",
        "                              'label_idx': labels[labels.name==tags[0]].index[0],\n",
        "                              'xres': xres,\n",
        "                              'yres': yres,\n",
        "                              'height': h,\n",
        "                              'width': w,                              \n",
        "                              'left': adata['regions'][j]['boundingBox']['left']/xres,\n",
        "                              'top': adata['regions'][j]['boundingBox']['top']/yres,\n",
        "                              'right': adata['regions'][j]['boundingBox']['left']/xres+w,\n",
        "                              'bottom': adata['regions'][j]['boundingBox']['top']/yres+h, \n",
        "                              'area': h*w,\n",
        "                              'xcenter': adata['regions'][j]['boundingBox']['left']/xres+0.5*w,\n",
        "                              'ycenter': adata['regions'][j]['boundingBox']['top']/yres+0.5*h,\n",
        "                              'blurred': int(any(ele == '_blurred' for ele in tags)),\n",
        "                              'occluded': int(any(ele == '_occluded' for ele in tags)),\n",
        "                              'truncated': int(any(ele == '_truncated' for ele in tags)),\n",
        "                              'file': adata['asset']['path'].replace('file:F:/',''),\n",
        "                              'id': adata['asset']['id'],}, ignore_index=True)"
      ],
      "metadata": {
        "id": "HNdMpKItVrll"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anno.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "tvYUXy6PvcEQ",
        "outputId": "6de6c3ac-fc22-4dfd-c768-09d05fa0bb51"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             label label_idx  xres  yres    height     width      left  \\\n",
              "7358   Lepidoptera         0  2400  1600  0.760083  0.842867  0.082012   \n",
              "10148   Coleoptera         1  4272  2848  0.167528  0.084769  0.470021   \n",
              "8885    Coleoptera         1  2048  1536  0.389847  0.290230  0.351652   \n",
              "2894       Diptera         3  2048  1536  0.954023  0.786638  0.091595   \n",
              "16108    Hemiptera         5  2048  2048  0.088123  0.072797  0.401341   \n",
              "\n",
              "            top     right    bottom      area   xcenter   ycenter blurred  \\\n",
              "7358   0.102039  0.924879  0.862122  0.640649  0.503446  0.482081       0   \n",
              "10148  0.387458  0.554790  0.554986  0.014201  0.512405  0.471222       0   \n",
              "8885   0.330460  0.641882  0.720307  0.113145  0.496767  0.525383       0   \n",
              "2894   0.041188  0.878233  0.995211  0.750471  0.484914  0.518199       0   \n",
              "16108  0.631226  0.474138  0.719349  0.006415  0.437739  0.675287       0   \n",
              "\n",
              "      occluded truncated                                  file  \\\n",
              "7358         0         0  ArTaxOr/Lepidoptera/b361f81b8799.jpg   \n",
              "10148        1         0   ArTaxOr/Coleoptera/b54441af82f6.jpg   \n",
              "8885         0         0   ArTaxOr/Coleoptera/3c7d856983d8.jpg   \n",
              "2894         0         0      ArTaxOr/Diptera/1714b863b6fb.jpg   \n",
              "16108        0         0    ArTaxOr/Hemiptera/d6e5904eafae.jpg   \n",
              "\n",
              "                                     id  \n",
              "7358   4091178799843c1adfe724319b01278a  \n",
              "10148  b9d56653fe2d1c3b676c9ba69cd3db06  \n",
              "8885   facf205d602820e8f6dde9db82041a74  \n",
              "2894   92bb511e49287d881e70c2ecab2753c3  \n",
              "16108  5609435267fcfbe8993e738cf0c13297  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bc41d8e8-0b7f-4ffb-9745-1bce55ae525a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>label_idx</th>\n",
              "      <th>xres</th>\n",
              "      <th>yres</th>\n",
              "      <th>height</th>\n",
              "      <th>width</th>\n",
              "      <th>left</th>\n",
              "      <th>top</th>\n",
              "      <th>right</th>\n",
              "      <th>bottom</th>\n",
              "      <th>area</th>\n",
              "      <th>xcenter</th>\n",
              "      <th>ycenter</th>\n",
              "      <th>blurred</th>\n",
              "      <th>occluded</th>\n",
              "      <th>truncated</th>\n",
              "      <th>file</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7358</th>\n",
              "      <td>Lepidoptera</td>\n",
              "      <td>0</td>\n",
              "      <td>2400</td>\n",
              "      <td>1600</td>\n",
              "      <td>0.760083</td>\n",
              "      <td>0.842867</td>\n",
              "      <td>0.082012</td>\n",
              "      <td>0.102039</td>\n",
              "      <td>0.924879</td>\n",
              "      <td>0.862122</td>\n",
              "      <td>0.640649</td>\n",
              "      <td>0.503446</td>\n",
              "      <td>0.482081</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>ArTaxOr/Lepidoptera/b361f81b8799.jpg</td>\n",
              "      <td>4091178799843c1adfe724319b01278a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10148</th>\n",
              "      <td>Coleoptera</td>\n",
              "      <td>1</td>\n",
              "      <td>4272</td>\n",
              "      <td>2848</td>\n",
              "      <td>0.167528</td>\n",
              "      <td>0.084769</td>\n",
              "      <td>0.470021</td>\n",
              "      <td>0.387458</td>\n",
              "      <td>0.554790</td>\n",
              "      <td>0.554986</td>\n",
              "      <td>0.014201</td>\n",
              "      <td>0.512405</td>\n",
              "      <td>0.471222</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>ArTaxOr/Coleoptera/b54441af82f6.jpg</td>\n",
              "      <td>b9d56653fe2d1c3b676c9ba69cd3db06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8885</th>\n",
              "      <td>Coleoptera</td>\n",
              "      <td>1</td>\n",
              "      <td>2048</td>\n",
              "      <td>1536</td>\n",
              "      <td>0.389847</td>\n",
              "      <td>0.290230</td>\n",
              "      <td>0.351652</td>\n",
              "      <td>0.330460</td>\n",
              "      <td>0.641882</td>\n",
              "      <td>0.720307</td>\n",
              "      <td>0.113145</td>\n",
              "      <td>0.496767</td>\n",
              "      <td>0.525383</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>ArTaxOr/Coleoptera/3c7d856983d8.jpg</td>\n",
              "      <td>facf205d602820e8f6dde9db82041a74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2894</th>\n",
              "      <td>Diptera</td>\n",
              "      <td>3</td>\n",
              "      <td>2048</td>\n",
              "      <td>1536</td>\n",
              "      <td>0.954023</td>\n",
              "      <td>0.786638</td>\n",
              "      <td>0.091595</td>\n",
              "      <td>0.041188</td>\n",
              "      <td>0.878233</td>\n",
              "      <td>0.995211</td>\n",
              "      <td>0.750471</td>\n",
              "      <td>0.484914</td>\n",
              "      <td>0.518199</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>ArTaxOr/Diptera/1714b863b6fb.jpg</td>\n",
              "      <td>92bb511e49287d881e70c2ecab2753c3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16108</th>\n",
              "      <td>Hemiptera</td>\n",
              "      <td>5</td>\n",
              "      <td>2048</td>\n",
              "      <td>2048</td>\n",
              "      <td>0.088123</td>\n",
              "      <td>0.072797</td>\n",
              "      <td>0.401341</td>\n",
              "      <td>0.631226</td>\n",
              "      <td>0.474138</td>\n",
              "      <td>0.719349</td>\n",
              "      <td>0.006415</td>\n",
              "      <td>0.437739</td>\n",
              "      <td>0.675287</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>ArTaxOr/Hemiptera/d6e5904eafae.jpg</td>\n",
              "      <td>5609435267fcfbe8993e738cf0c13297</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bc41d8e8-0b7f-4ffb-9745-1bce55ae525a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bc41d8e8-0b7f-4ffb-9745-1bce55ae525a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bc41d8e8-0b7f-4ffb-9745-1bce55ae525a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.relplot(x=\"width\", y=\"height\", hue=\"label\", col=\"label\", data=anno)"
      ],
      "metadata": {
        "id": "g1OwH8uBvv8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.jointplot(x='width', y='height', data=anno.loc[anno['label'] == 'Diptera'])"
      ],
      "metadata": {
        "id": "ZrfImW5uZyW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(rc={'figure.figsize': (12,6)})\n",
        "sns.violinplot(x=anno['label'], y=anno['area'])"
      ],
      "metadata": {
        "id": "vvZl1B14aIE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph=sns.countplot(data=anno, x='label')\n",
        "graph.set_xticklabels(graph.get_xticklabels(), rotation=90)\n",
        "\n",
        "for p in graph.patches:\n",
        "  height = p.get_height()\n",
        "  graph.text(p.get_x() + p.get_width()/2, height + 0.1, height, ha='center')"
      ],
      "metadata": {
        "id": "V-8gzBGdbCtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph=sns.countplot(data=anno, x='blurred')\n",
        "graph.set_xticklabels(graph.get_xticklabels(),rotation=90)\n",
        "for p in graph.patches:\n",
        "    height = p.get_height()\n",
        "    graph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "hNxAAUNhkPfH",
        "outputId": "175b5a48-408f-4d85-d8be-c358535fc089"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wuVHit5UnaYe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}