{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1fFvD4IBjjOPOMVqGre0vbJNKdEs2_j-I",
      "authorship_tag": "ABX9TyNGYTClI8PuLJphmO5gKIrp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Projects/blob/main/Unsupervised_Similar_Images_Finder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I**n this project we are going to learn about how to fins similar images of and select the best one based on it's size(resolution) and store it in the main folder and send the other copies to the rubish folder"
      ],
      "metadata": {
        "id": "-0D0IZ6TLce9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V7z9WChab5r7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import tqdm\n",
        "import shutil\n",
        "import skimage\n",
        "import IPython\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "!pip install patool\n",
        "import patoolib\n",
        "\n",
        "IPython.display.clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Read the zip file from the drive and unzip it"
      ],
      "metadata": {
        "id": "tuJDnBFoMDKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patoolib.extract_archive(\"/content/drive/MyDrive/reddit_files.rar\", outdir=\"/content/data\")\n",
        "\n",
        "Path = './data/images/'\n",
        "os.rename(f\"./data/{os.listdir('./data')[0]}\", Path)\n",
        "\n",
        "IPython.display.clear_output()\n",
        "\n",
        "data = os.listdir(Path)\n",
        "len(data)"
      ],
      "metadata": {
        "id": "W0MNWE9WeJRi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeaf4475-c58d-42f3-b495-6f00b39659be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2009"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: create a path for the videos and rubbish data and store the video clips in the video folder and then select the images andsave it as data variable"
      ],
      "metadata": {
        "id": "X2D6N6tdMPLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Path = './data/images/'\n",
        "rubish_path = './Rubish/'\n",
        "data = glob.glob(f'{Path}/*g')\n",
        "videos = glob.glob(f'{Path}/*.mp4')\n",
        "os.makedirs(f'{Path}/Videos/', exist_ok=True)\n",
        "os.makedirs(rubish_path, exist_ok=True)\n",
        "for i, video_path in enumerate(videos):\n",
        "    os.replace(video_path, f'{Path}/Videos/{i}.mp4')"
      ],
      "metadata": {
        "id": "ZpA5E21_eTpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3:\n",
        "create a Mean-Pooling layer\n",
        "\n",
        "Reason:\n",
        "an image is a big matrix and since we only need to compare the images from some of their pixels, we need to at first convert 3d image to a 1-dim image and then get the mean part of each block of the image"
      ],
      "metadata": {
        "id": "CCakG5BjMgp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def avg_pooling(img, pad_size=20):\n",
        "  image = abs(img[:, :, 0] - img[:, :, 1] - img[:, :, 2])\n",
        "  max_pool_img = skimage.measure.block_reduce(image, (pad_size,pad_size), np.mean)\n",
        "  return max_pool_img"
      ],
      "metadata": {
        "id": "BN9JZ-KkeTm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4:\n",
        "we have to read the images and since we are using min-pooling, we have to resize them in a same size, for example:(1000, 1000) and then pass it through avg-pooling then flatten the image, so we have less values per image"
      ],
      "metadata": {
        "id": "fIcuosOtNJqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arr = []\n",
        "height_arr, weight_arr = [], []\n",
        "for d in tqdm.tqdm(data):\n",
        "  try:\n",
        "      img = cv2.imread(d)\n",
        "      height_arr.append(img.shape[0])\n",
        "      weight_arr.append(img.shape[1])\n",
        "      img = cv2.resize(img, (1000, 1000))\n",
        "      img = avg_pooling(img, pad_size=50)\n",
        "      img = np.ravel(img)\n",
        "      arr.append(img)\n",
        "  except:\n",
        "    print(d)\n",
        "\n",
        "print('\\n')\n",
        "print('Height Mean:',np.mean(height_arr), 'Height Min:',np.min(height_arr), 'Height Max:',np.max(height_arr))\n",
        "print('Weight Mean:',np.mean(weight_arr), 'Weight Min:',np.min(weight_arr), 'Weight Max:',np.max(weight_arr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m709rblCeo5g",
        "outputId": "b43d6fff-5839-4b84-a3e4-49f5609ba5ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1970/1970 [01:29<00:00, 22.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Height Mean: 1343.7598984771573 Height Min: 194 Height Max: 8598\n",
            "Weight Mean: 1111.6837563451777 Weight Min: 149 Weight Max: 6144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5:\n",
        "In this step we go through the images that we stored in arr and get the average of the difference of each image values from every other images in the directory, and only select the ones that have mean average error < 15\n",
        "\n",
        "at last we sort the selected images in temp_data and sort it by temp_size because we want the images with higher resolution(bigger sizes) and store the sorted image pathes in to mae_err"
      ],
      "metadata": {
        "id": "W-PfWitCNvXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mae_arr = []\n",
        "for i, A in tqdm.tqdm(enumerate(arr)):\n",
        "  temp_data = []\n",
        "  temp_size = []\n",
        "  mean_mae_arr = [(abs(A - b)).mean() for b in arr]\n",
        "  for j, mae in enumerate(mean_mae_arr):\n",
        "    if mae < 15:\n",
        "      img = cv2.imread(data[j])\n",
        "      img_size = img.shape[0] * img.shape[1]\n",
        "      temp_data.append(data[j])\n",
        "      temp_size.append(img_size)\n",
        "\n",
        "  temp_data = np.array(temp_data)[np.argsort(temp_size)[::-1]]\n",
        "  mae_arr.append(temp_data)\n",
        "\n",
        "mae_arr = np.array(mae_arr)\n",
        "mae_arr.shape"
      ],
      "metadata": {
        "id": "DOb36t67eo2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6:\n",
        "In the last step, we store the first images of each path_list and get rid of the othe copies"
      ],
      "metadata": {
        "id": "RlgyW0odO7Aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for id, path_list in tqdm.tqdm(enumerate(mae_arr)):\n",
        "  for n, p in enumerate(path_list):\n",
        "    if os.path.exists(p):\n",
        "      if n==0:\n",
        "        os.replace(p, f'{Path}/img_{id}.jpg')\n",
        "      else:\n",
        "        os.replace(p, f'{rubish_path}/img_{id}_{n}.jpg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m1oOE1y7wWJ",
        "outputId": "22c15a83-e9f8-4eac-aa6b-a34d8797bc4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1970it [00:00, 18019.87it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functional model"
      ],
      "metadata": {
        "id": "aOH3QckMDk6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def similar_finder(Path, rubish_path):\n",
        "  #Part 1\n",
        "  data = glob.glob(f'{Path}/*g')\n",
        "  videos = glob.glob(f'{Path}/*.mp4')\n",
        "  os.makedirs(f'{Path}/Videos/', exist_ok=True)\n",
        "  os.makedirs(rubish_path, exist_ok=True)\n",
        "  for i, video_path in enumerate(videos):\n",
        "      os.replace(video_path, f'{Path}/Videos/{i}.mp4')\n",
        "\n",
        "  #Part 2\n",
        "  img_arr = []\n",
        "  for d in tqdm.tqdm(data):\n",
        "    try:\n",
        "        img = cv2.imread(d)\n",
        "        img = cv2.resize(img, (1000, 1000))\n",
        "        #Mean Average Pooling\n",
        "        img = skimage.measure.block_reduce(abs(img[:, :, 0] - img[:, :, 1] - img[:, :, 2]), (50, 50), np.mean)\n",
        "        img = np.ravel(img)\n",
        "        img_arr.append(img)\n",
        "    except:\n",
        "      print('Error File: ', d)\n",
        "\n",
        "  #Part 3\n",
        "  mae_arr = []\n",
        "  for i, A in tqdm.tqdm(enumerate(img_arr)):\n",
        "    temp_path = []\n",
        "    temp_size = []\n",
        "    mean_mae_arr = [(abs(A - b)).mean() for b in img_arr]\n",
        "    for j, mae in enumerate(mean_mae_arr):\n",
        "      if mae < 15:\n",
        "        img = cv2.imread(data[j])\n",
        "        img_size = img.shape[0] * img.shape[1]\n",
        "        temp_path.append(data[j])\n",
        "        temp_size.append(img_size)\n",
        "\n",
        "    temp_path = np.array(temp_path)[np.argsort(temp_size)[::-1]]\n",
        "    mae_arr.append(temp_path)\n",
        "\n",
        "  #Part 4\n",
        "  for id, path_list in tqdm.tqdm(enumerate(mae_arr)):\n",
        "    for n, p in enumerate(path_list):\n",
        "\n",
        "      if os.path.exists(p):\n",
        "        if n==0:\n",
        "          os.replace(p, f'{Path}/img_{id}.jpg')\n",
        "        else:\n",
        "          os.replace(p, f'{rubish_path}/img_{id}_{n}.jpg')"
      ],
      "metadata": {
        "id": "k3YpQmE8TFBv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Path = './data/images/'\n",
        "rubish_path = './Rubish/'\n",
        "\n",
        "similar_finder(Path = './data/images/', rubish_path = './Rubish/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtSGajeUIblC",
        "outputId": "4aa4c6cd-e1ab-4841-dfc4-faf45528bac0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1970/1970 [01:18<00:00, 25.10it/s]\n",
            "1970it [02:54, 11.31it/s]\n",
            "1970it [00:00, 19511.43it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kKC2Ea9TI-i2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}