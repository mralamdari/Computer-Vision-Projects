{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cLpy_FXSeHi5",
        "m6s-7TAX3cuE"
      ],
      "mount_file_id": "1fFvD4IBjjOPOMVqGre0vbJNKdEs2_j-I",
      "authorship_tag": "ABX9TyO8r1Li6Qmb4RsZrCWGSCRD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Projects/blob/main/Unsupervised_Similar_Images_Finder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I**n this project we are going to learn about how to fins similar images of and select the best one based on it's size(resolution) and store it in the main folder and send the other copies to the rubish folder"
      ],
      "metadata": {
        "id": "-0D0IZ6TLce9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "V7z9WChab5r7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import tqdm\n",
        "import shutil\n",
        "import skimage\n",
        "import IPython\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "!pip install patool\n",
        "import patoolib\n",
        "\n",
        "IPython.display.clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Read the zip file from the drive and unzip it"
      ],
      "metadata": {
        "id": "tuJDnBFoMDKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patoolib.extract_archive(\"/content/drive/MyDrive/reddit_files.rar\", outdir=\"/content/data\")\n",
        "\n",
        "Path = './data/images/'\n",
        "os.rename(f\"./data/{os.listdir('./data')[0]}\", Path)\n",
        "\n",
        "IPython.display.clear_output()\n",
        "\n",
        "data = os.listdir(Path)\n",
        "len(data)"
      ],
      "metadata": {
        "id": "W0MNWE9WeJRi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72b4868a-70e7-4bfc-ac42-d15e33b3fe1b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2009"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: create a path for the videos and rubbish data and store the video clips in the video folder and then select the images andsave it as data variable"
      ],
      "metadata": {
        "id": "X2D6N6tdMPLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Path = './data/images/'\n",
        "rubish_path = './Rubish/'\n",
        "data = glob.glob(f'{Path}/*g')\n",
        "videos = glob.glob(f'{Path}/*.mp4')\n",
        "os.makedirs(f'{Path}/Videos/', exist_ok=True)\n",
        "os.makedirs(rubish_path, exist_ok=True)\n",
        "for i, video_path in enumerate(videos):\n",
        "    os.replace(video_path, f'{Path}/Videos/{i}.mp4')"
      ],
      "metadata": {
        "id": "ZpA5E21_eTpo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3:\n",
        "create a Mean-Pooling layer\n",
        "\n",
        "Reason:\n",
        "an image is a big matrix and since we only need to compare the images from some of their pixels, we need to at first convert 3d image to a 1-dim image and then get the mean part of each block of the image"
      ],
      "metadata": {
        "id": "CCakG5BjMgp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def avg_pooling(img, pad_size=20):\n",
        "  image = abs(img[:, :, 0] - img[:, :, 1] - img[:, :, 2])\n",
        "  max_pool_img = skimage.measure.block_reduce(image, (pad_size,pad_size), np.mean)\n",
        "  return max_pool_img"
      ],
      "metadata": {
        "id": "BN9JZ-KkeTm9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4:\n",
        "we have to read the images and since we are using min-pooling, we have to resize them in a same size, for example:(1000, 1000) and then pass it through avg-pooling then flatten the image, so we have less values per image"
      ],
      "metadata": {
        "id": "fIcuosOtNJqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arr = []\n",
        "height_arr, weight_arr = [], []\n",
        "for d in tqdm.tqdm(data):\n",
        "  try:\n",
        "      img = cv2.imread(d)\n",
        "      height_arr.append(img.shape[0])\n",
        "      weight_arr.append(img.shape[1])\n",
        "      img = cv2.resize(img, (1000, 1000))\n",
        "      img = avg_pooling(img, pad_size=50)\n",
        "      img = np.ravel(img)\n",
        "      arr.append(img)\n",
        "  except:\n",
        "    print(d)\n",
        "\n",
        "print('\\n')\n",
        "print('Height Mean:',np.mean(height_arr), 'Height Min:',np.min(height_arr), 'Height Max:',np.max(height_arr))\n",
        "print('Weight Mean:',np.mean(weight_arr), 'Weight Min:',np.min(weight_arr), 'Weight Max:',np.max(weight_arr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m709rblCeo5g",
        "outputId": "b43d6fff-5839-4b84-a3e4-49f5609ba5ba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1970/1970 [01:29<00:00, 22.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Height Mean: 1343.7598984771573 Height Min: 194 Height Max: 8598\n",
            "Weight Mean: 1111.6837563451777 Weight Min: 149 Weight Max: 6144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5:\n",
        "In this step we go through the images that we stored in arr and get the average of the difference of each image values from every other images in the directory, and only select the ones that have mean average error < 15\n",
        "\n",
        "at last we sort the selected images in temp_data and sort it by temp_size because we want the images with higher resolution(bigger sizes) and store the sorted image pathes in to mae_err"
      ],
      "metadata": {
        "id": "W-PfWitCNvXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mae_arr = []\n",
        "for i, A in tqdm.tqdm(enumerate(arr)):\n",
        "  temp_data = []\n",
        "  temp_size = []\n",
        "  mean_mae_arr = [(abs(A - b)).mean() for b in arr]\n",
        "  for j, mae in enumerate(mean_mae_arr):\n",
        "    if mae < 15:\n",
        "      img = cv2.imread(data[j])\n",
        "      img_size = img.shape[0] * img.shape[1]\n",
        "      temp_data.append(data[j])\n",
        "      temp_size.append(img_size)\n",
        "\n",
        "  temp_data = np.array(temp_data)[np.argsort(temp_size)[::-1]]\n",
        "  mae_arr.append(temp_data)\n",
        "\n",
        "mae_arr = np.array(mae_arr)\n",
        "mae_arr.shape"
      ],
      "metadata": {
        "id": "DOb36t67eo2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6:\n",
        "In the last step, we store the first images of each path_list and get rid of the othe copies"
      ],
      "metadata": {
        "id": "RlgyW0odO7Aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for id, path_list in tqdm.tqdm(enumerate(mae_arr)):\n",
        "  for n, p in enumerate(path_list):\n",
        "    if os.path.exists(p):\n",
        "      if n==0:\n",
        "        os.replace(p, f'{Path}/img_{id}.jpg')\n",
        "      else:\n",
        "        os.replace(p, f'{rubish_path}/img_{id}_{n}.jpg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m1oOE1y7wWJ",
        "outputId": "22c15a83-e9f8-4eac-aa6b-a34d8797bc4f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1970it [00:00, 18019.87it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#test 1"
      ],
      "metadata": {
        "id": "QC_0eB0nJVmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mae_arr = []\n",
        "for i, A in tqdm.tqdm(enumerate(arr)):\n",
        "  temp = []\n",
        "  mean_mae_arr = [(abs(A - b)).mean() for b in arr]\n",
        "  for j, mae in enumerate(mean_mae_arr):\n",
        "    if mae < 15:\n",
        "      img = cv2.imread(p)\n",
        "      img_size = img.shape[0] * img.shape[1]\n",
        "      temp.append(data[j])\n",
        "  mae_arr.append(temp)\n",
        "\n",
        "mae_arr = np.array(mae_arr)\n",
        "mae_arr.shape"
      ],
      "metadata": {
        "id": "VAF1heixJaka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = glob.glob('/content/data/images/*g')\n",
        "for i in new_data:\n",
        "  p1, p2, p3 = i.split('_')\n",
        "  if '0.jpg' == p3:\n",
        "    os.replace(i, f'./data/images/Images/img_{p2}.jpg')"
      ],
      "metadata": {
        "id": "GevZ7zayJU0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for id, path_list in tqdm.tqdm(enumerate(mae_arr)):\n",
        "#   best_img, best_size = path_list[0], 0\n",
        "#   for n, p in enumerate(path_list):\n",
        "#       if os.path.exists(p):\n",
        "#           img = cv2.imread(p)\n",
        "#           img_size = img.shape[0] * img.shape[1]\n",
        "#           if img_size >= best_size:\n",
        "#               best_img  = p\n",
        "#               best_size = img_size\n",
        "#           else:\n",
        "#               os.replace(p, f'./Rubish/img_{id}.jpg')\n",
        "#   if os.path.exists(p):\n",
        "#       os.replace(best_img, f'{Path}/Images/img_{id}.jpg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFuKClaKskEo",
        "outputId": "ec42ba3f-d27f-47f2-cdb5-775ab7c2e6a2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1970it [00:00, 26063.74it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mae_arr = []\n",
        "# arr2 = []\n",
        "# arr3 = []\n",
        "# for i, A in tqdm.tqdm(enumerate(arr)):\n",
        "#   temp = []\n",
        "#   mean_mae_arr = np.array([(abs(A - b)).mean() for b in arr])\n",
        "#   # mean_mae_arr = categorize(mean_mae_arr)\n",
        "#   mean_mae = np.mean(mean_mae_arr)\n",
        "#   arr2.append(mean_mae_arr)\n",
        "#   arr3.append(mean_mae)\n",
        "#   for j, mae in enumerate(arr):\n",
        "#     temp.append(data[j])\n",
        "#   mae_arr.append(temp)\n",
        "\n",
        "# arr2 = np.array(arr2)\n",
        "\n",
        "# temp_arr = []\n",
        "# score_arr= []\n",
        "# for score, path in zip(arr2, mae_arr):\n",
        "#   if len(np.unique(score)) > 2:\n",
        "#     t1, t2 = [], []\n",
        "#     for s, p in zip(score, path):\n",
        "#       if s < 50:\n",
        "#         t1.append(s)\n",
        "#         t2.append(p)\n",
        "#     score_arr.append(t1)\n",
        "#     temp_arr.append(t2)\n",
        "\n",
        "# len(temp_arr), len(arr2)"
      ],
      "metadata": {
        "id": "achtK6osIM7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N=0\n",
        "# N+=1\n",
        "\n",
        "# # print(N, arr3[N], np.max(arr2[N]), np.unique(arr2[N], return_counts=True))\n",
        "# mae_pathes = np.array(mae_arr[N])[np.argsort(arr2[N])]\n",
        "# mae_scores = np.array(arr2[N])[np.argsort(arr2[N])]\n",
        "# # print(N, list(np.round(mae_scores[:5], 2)))\n",
        "# fig = plt.figure(figsize=(5, 5))\n",
        "# for i in range(20):\n",
        "#   img = plt.imread(mae_pathes[i])\n",
        "#   img = cv2.resize(img, (100, 100))\n",
        "#   fig.add_subplot(4, 5, i+1)\n",
        "#   plt.gca().set_title(round(mae_scores[i], 2))\n",
        "#   plt.grid(False)\n",
        "\n",
        "#   plt.axis('off')\n",
        "#   plt.imshow(img)"
      ],
      "metadata": {
        "id": "DgpR9cNyH2c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for id, path_list in tqdm.tqdm(enumerate(mae_arr)):\n",
        "  best_img, best_size = path_list[0], 0\n",
        "  for n, p in enumerate(path_list):\n",
        "      if os.path.exists(p):\n",
        "          img = cv2.imread(p)\n",
        "          img_size = img.shape[0] * img.shape[1]\n",
        "          if img_size >= best_size:\n",
        "              best_img  = p\n",
        "              best_size = img_size\n",
        "          else:\n",
        "              os.replace(p, f'./Rubish/img_{id}.jpg')\n",
        "  if os.path.exists(p):\n",
        "      os.replace(best_img, f'{Path}/Images/img_{id}.jpg')"
      ],
      "metadata": {
        "id": "aR6FCf1bIyaH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c92388-e64a-43b2-d131-aa26286df452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1970it [01:50, 17.76it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for id, path_list in tqdm.tqdm(enumerate(mae_arr)):\n",
        "#   if len(path_list) > 1:\n",
        "#     best_img, best_size = path_list[0], 0\n",
        "#     for n, p in enumerate(path_list):\n",
        "#         if os.path.exists(p):\n",
        "#             img = cv2.imread(p)\n",
        "#             img_size = img.shape[0] * img.shape[1]\n",
        "#             if img_size > best_size:\n",
        "#                 best_img  = p\n",
        "#                 best_size = img_size\n",
        "#             else:\n",
        "#                 os.replace(p, f'./Rubish/img_{id}.jpg')\n",
        "#     if os.path.exists(p):\n",
        "#         os.replace(best_img, f'{Path}/Images/img_{id}.jpg')\n",
        "#   else:\n",
        "#      p = path_list[0]\n",
        "#      if os.path.exists(p):\n",
        "#         os.replace(p, f'{Path}/Images/img_{id}.jpg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ae6WIxpohoAU",
        "outputId": "5d284d5c-4e24-4a58-808f-cbdae540b8fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1970it [01:22, 23.78it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mae_arr = []\n",
        "# for i, A in tqdm.tqdm(enumerate(arr)):\n",
        "#   mean_mae_arr = np.array([(abs(A - b)).mean() for b in arr])\n",
        "#   mae_arr.append(mean_mae_arr)\n",
        "\n",
        "# mae_arr = np.array(mae_arr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2DnlM_dmPlz",
        "outputId": "eb5369a8-482f-4c67-99ca-6582653c57a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1970it [00:37, 53.15it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(os.listdir('/content/data/images/')))\n",
        "print(len(os.listdir('/content/Rubish')))\n",
        "print(len(os.listdir('/content/data/images/Images')))\n",
        "print(len(os.listdir('/content/data/images/Videos')))\n",
        "708+317+914+39"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JMYbRnG_S_F",
        "outputId": "dddd35c9-e3b4-4775-8948-eed7fcfe9b80"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "708\n",
            "317\n",
            "914\n",
            "39\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1978"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# os.listdir('/content/data/images/')"
      ],
      "metadata": {
        "id": "XnJsu8fBDUh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a7RznfYbhmRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def categorize(arr):\n",
        "  arr[arr <= 10] = 0\n",
        "  # arr[np.logical_and(arr > 1 , arr <= 10)] = 10\n",
        "  arr[np.logical_and(arr > 10, arr <= 20)] = 20\n",
        "  arr[np.logical_and(arr > 20, arr <= 30)] = 30\n",
        "  arr[np.logical_and(arr > 30, arr <= 40)] = 40\n",
        "  arr[arr > 40] = 100\n",
        "  return arr"
      ],
      "metadata": {
        "id": "voQP561IyzMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae_arr = []\n",
        "arr2 = []\n",
        "arr3 = []\n",
        "for i, A in tqdm.tqdm(enumerate(arr)):\n",
        "  temp = []\n",
        "  mean_mae_arr = np.array([(abs(A - b)).mean() for b in arr])\n",
        "  # mean_mae_arr = categorize(mean_mae_arr)\n",
        "  mean_mae = np.mean(mean_mae_arr)\n",
        "  arr2.append(mean_mae_arr)\n",
        "  arr3.append(mean_mae)\n",
        "  for j, mae in enumerate(arr):\n",
        "    temp.append(data[j])\n",
        "  mae_arr.append(temp)\n",
        "\n",
        "arr2 = np.array(arr2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6tkcS_cvs-v",
        "outputId": "4648b618-76ff-4437-e710-da356238ffa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1970it [00:38, 51.29it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr_cat = []\n",
        "for a in arr2:\n",
        "    arr_cat.append(categorize(a))\n",
        "\n",
        "arr_cat[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf09g6XMIOWA",
        "outputId": "51940b42-7820-4913-965f-67c37e22442c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([  0., 100., 100., ..., 100., 100., 100.]),\n",
              " array([100.,   0., 100., ..., 100., 100., 100.]),\n",
              " array([100., 100.,   0., ..., 100., 100., 100.]),\n",
              " array([100., 100., 100., ..., 100., 100., 100.]),\n",
              " array([100., 100., 100., ..., 100., 100., 100.]),\n",
              " array([100., 100., 100., ..., 100., 100., 100.]),\n",
              " array([100., 100., 100., ..., 100., 100., 100.]),\n",
              " array([100., 100., 100., ..., 100., 100., 100.]),\n",
              " array([100., 100., 100., ..., 100., 100., 100.]),\n",
              " array([100., 100., 100., ..., 100., 100., 100.])]"
            ]
          },
          "metadata": {},
          "execution_count": 448
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_arr = []\n",
        "score_arr= []\n",
        "for score, path in zip(arr2, mae_arr):\n",
        "  if len(np.unique(score)) > 2:\n",
        "    t1, t2 = [], []\n",
        "    for s, p in zip(score, path):\n",
        "      if s < 50:\n",
        "        t1.append(s)\n",
        "        t2.append(p)\n",
        "    score_arr.append(t1)\n",
        "    temp_arr.append(t2)\n",
        "\n",
        "len(temp_arr), len(arr2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6S1dcb2jtYs",
        "outputId": "c55346cc-d80a-4d96-a859-65c3aef6f97e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(759, 1970)"
            ]
          },
          "metadata": {},
          "execution_count": 449
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p=0\n",
        "p+=1\n",
        "N = ss[p]\n",
        "\n",
        "mae_pathes = np.array(temp_arr[N])[np.argsort(score_arr[N])]\n",
        "mae_scores = np.array(score_arr[N])[np.argsort(score_arr[N])]\n",
        "print(N, list(np.round(mae_scores[:5], 2)))\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "for i in range(min(len(mae_scores), 20)):\n",
        "  img = plt.imread(mae_pathes[i])\n",
        "  img = cv2.resize(img, (100, 100))\n",
        "  fig.add_subplot(4, 5, i+1)\n",
        "  plt.gca().set_title(round(mae_scores[i], 2))\n",
        "  plt.grid(False)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(img)"
      ],
      "metadata": {
        "id": "AfmiPbhqrmFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N=0\n",
        "N+=1\n",
        "# 10 [0.0, 40.0 | 100.0, 100.0, 100.0]\n",
        "# 18 [0.0, 0.0, 10.0, 10.0 | 100.0]\n",
        "# 27 [0.0, 27.0, 28.45, 34.09 | 41]\n",
        "# 28 [0.0, 3.69| 47.34]\n",
        "# 29 [0.0, 0.77, 9.37, 9.38, 13.45, 13.55 | 52]\n",
        "# 31 [0.0, 0.0, 0.13, 0.13| 33.55]\n",
        "# 33 [0.0, 0.9, 3.21, 3.3, 3.75, 3.9 | 49]\n",
        "# 38 [0.0, 0.93 | 28.42]\n",
        "# 53 [0.0, 8.37, 16.78, 17.0 | 53.69]\n",
        "# 58 [0.0, 0.0, 0.13, 0.13 | 33.54]\n",
        "# 59 [0.0, 21.51, 27.0, 31.85 | 46.31]\n",
        "# 61 [0.0, 1.35, 1.77, 1.87, 37.57, 37.87, 49.9]\n",
        "# 88 [0.0, 10.0, 30.0, 30.0 | 100.0]\n",
        "# 89 [0.0, 0.0 | 20.0, 30.0, 30.0]\n",
        "# 94 [0.0, 10.0 | 100.0, 100.0, 100.0]\n",
        "# 99 [0.0, 10.0, 40.0, 40.0 | 100.0]\n",
        "\n",
        "\n",
        "print(N, arr3[N], np.max(arr2[N]), np.unique(arr2[N], return_counts=True))\n",
        "mae_pathes = np.array(mae_arr[N])[np.argsort(arr2[N])]\n",
        "mae_scores = np.array(arr2[N])[np.argsort(arr2[N])]\n",
        "print(N, list(np.round(mae_scores[:5], 2)))\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "for i in range(20):\n",
        "  img = plt.imread(mae_pathes[i])\n",
        "  img = cv2.resize(img, (100, 100))\n",
        "  fig.add_subplot(4, 5, i+1)\n",
        "  plt.gca().set_title(round(mae_scores[i], 2))\n",
        "  plt.grid(False)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(img)"
      ],
      "metadata": {
        "id": "Nns05Fxou6fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(2^16, 2^16))\n",
        "plt.imshow(arr2)\n",
        "plt.savefig('heatmap.png')\n",
        "plt.savefig('heatmap.pdf')"
      ],
      "metadata": {
        "id": "9lyloqy_mPP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for id, path_list in tqdm.tqdm(enumerate(mae_arr)):\n",
        "  if len(path_list) > 1:\n",
        "    best_img, best_size= path_list[0], 0\n",
        "    for n, p in enumerate(path_list):\n",
        "        if os.path.exists(p):\n",
        "            img = cv2.imread(p)\n",
        "            img_size = img.shape[0] * img.shape[1]\n",
        "            if img_size > best_size:\n",
        "                best_img  = p\n",
        "                best_size = img_size\n",
        "    if os.path.exists(p):\n",
        "        os.replace(best_img, f'{Path}/Images/img_{id}.jpg')\n",
        "  else:\n",
        "     p = path_list[0]\n",
        "     if os.path.exists(p):\n",
        "        os.replace(p, f'{Path}/Images/img_{id}.jpg')"
      ],
      "metadata": {
        "id": "syFL3Gnjeoy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8qhB9yCbexcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BUnvD_2zexYk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}