{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Projects/blob/main/p123.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CvY1w4fDX3kL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision\n",
        "import torchsummary\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CXF4Mvu8YVIc"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle datasets download -d alenic/rsna-2023-atd-reduced-256-5mm\n",
        "!unzip \\*.zip && rm *.zip\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "enSdu7SbYVF_"
      },
      "outputs": [],
      "source": [
        "# class AbdominalData(torch.utils.data.Dataset):\n",
        "#     def __init__(self, df, current_fold, num_fold=5, train_folder=TRAIN_PATH, limit=-1):\n",
        "\n",
        "#         super().__init__()\n",
        "#         self.img_paths = []\n",
        "#         for patient in tqdm(os.listdir(train_folder)[:limit]):\n",
        "#             for scan in os.listdir(os.path.join(train_folder, patient)):\n",
        "#                 scans = []\n",
        "#                 for dicom_img in os.listdir(os.path.join(train_folder, patient, scan)):\n",
        "#                     scans.append(os.path.join(train_folder, patient, scan, dicom_img))\n",
        "#                 self.img_paths.append(scans)\n",
        "\n",
        "#         self.df = df\n",
        "#         self.current_fold = current_fold\n",
        "#         self.kf = model_selection.KFold(n_splits=num_fold)\n",
        "#         self.transform = torchvision.transforms.Compose([\n",
        "#                             torchvision.transforms.Resize((256, 256), antialias=True),\n",
        "#                             torchvision.transforms.RandomHorizontalFlip(),\n",
        "#                             torchvision.transforms.ColorJitter(brightness=0.2),\n",
        "#                             torchvision.transforms.ColorJitter(contrast=0.2),\n",
        "#                             torchvision.transforms.RandomAffine(degrees=0, shear=10),\n",
        "#                             torchvision.transforms.RandomAffine(degrees=0, scale=(0.8, 1.2)),\n",
        "#                             torchvision.transforms.RandomErasing(p=0.2, scale=(0.02, 0.2)),\n",
        "#         ])\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.img_paths)\n",
        "\n",
        "#     def __getitem__(self, idx, spacing = 2):\n",
        "\n",
        "#         elements = self.img_paths[idx]\n",
        "\n",
        "#         if len(elements) < spacing * 4:\n",
        "#             raise ValueError(\"Wrong number of elements.\")\n",
        "\n",
        "#         # We want to select elements in the middle part of the abdomen\n",
        "#         lower_bound = int(len(elements) * 0.4)\n",
        "#         upper_bound = int(len(elements) * 0.6)\n",
        "\n",
        "#         spacing = (upper_bound - lower_bound) // 3\n",
        "#         selected_indices = [lower_bound, lower_bound + spacing, lower_bound + (2*spacing), upper_bound]\n",
        "#         dicom_images = [elements[index] for index in selected_indices]\n",
        "\n",
        "#         patient_id = dicom_images[0].split('/')[-3]\n",
        "#         images = []\n",
        "#         for d in dicom_images:\n",
        "#             img = cv2.imread(d)\n",
        "#             image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)/255\n",
        "#             images.append(image)\n",
        "\n",
        "#         images = np.stack(images)\n",
        "#         images = torch.tensor(images, dtype = torch.float).unsqueeze(dim = 1)\n",
        "#         image = self.transform(images).squeeze(dim = 1)\n",
        "#         label = self.df[self.df.patient_id == int(patient_id)].values[0][1:]\n",
        "#         return image, label\n",
        "\n",
        "#     def get_splits(self):\n",
        "#         fold_data = list(self.kf.split(self.img_paths))\n",
        "#         train_indices, val_indices = fold_data[self.current_fold]\n",
        "#         train_data = self._get_subset(train_indices)\n",
        "#         val_data = self._get_subset(val_indices)\n",
        "#         return train_data, val_data\n",
        "\n",
        "#     def _get_subset(self, indices):\n",
        "#         return torch.utils.data.Subset(self, indices)\n",
        "\n",
        "\n",
        "# class AbdominalData(torch.utils.data.Dataset):\n",
        "#     def __init__(self, df=train_data, train_folder=TRAIN_PATH, limit=-1):\n",
        "\n",
        "#         super().__init__()\n",
        "#         self.img_paths = []\n",
        "#         for patient in tqdm(os.listdir(train_folder)):\n",
        "#             for scan in os.listdir(os.path.join(train_folder, patient)):\n",
        "#                 scans = []\n",
        "#                 for dicom_img in os.listdir(os.path.join(train_folder, patient, scan)):\n",
        "#                     scans.append(os.path.join(train_folder, patient, scan, dicom_img))\n",
        "#                 self.img_paths.append(scans)\n",
        "\n",
        "#         self.transform = torchvision.transforms.Compose([torchvision.transforms.Resize((256, 256), antialias=True)])\n",
        "\n",
        "#         print(self.img_paths)\n",
        "#         for folder in self.img_paths:\n",
        "#           images = []\n",
        "#           patient_id = folder[0].split('/')[-3]\n",
        "#           for elemnts in folder[:limit]:\n",
        "#             img = cv2.imread(elemnts)\n",
        "#             img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)/255\n",
        "#             images.append(img)\n",
        "\n",
        "#           images = np.stack(images)\n",
        "#           images = torch.tensor(images, dtype = torch.float).unsqueeze(dim = 1)\n",
        "#           image = self.transform(images).squeeze(dim = 1)\n",
        "#           label = df[df.patient_id == int(patient_id)].values[0][1:]\n",
        "#         return image, label"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_PATH = '/content/reduced_256_tickness_5'\n",
        "train_data = pd.read_csv('./train.csv')\n",
        "limit = 1\n",
        "img_paths = []\n",
        "img_list = []\n",
        "label_list = []\n",
        "for patient in tqdm(os.listdir(TRAIN_PATH)):\n",
        "    for scan in os.listdir(os.path.join(TRAIN_PATH, patient)):\n",
        "        scans = []\n",
        "        for dicom_img in os.listdir(os.path.join(TRAIN_PATH, patient, scan)):\n",
        "            scans.append(os.path.join(TRAIN_PATH, patient, scan, dicom_img))\n",
        "        img_paths.append(scans)\n",
        "\n",
        "        transform = torchvision.transforms.Compose([torchvision.transforms.Resize((256, 256), antialias=True)])\n",
        "\n",
        "        for folder in img_paths:\n",
        "          images = []\n",
        "          patient_id = folder[0].split('/')[-3]\n",
        "\n",
        "          for elemnts in folder[:limit]:\n",
        "            img = cv2.imread(elemnts)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)/255\n",
        "            images.append(img)\n",
        "\n",
        "          images = np.stack(images)\n",
        "          images = torch.tensor(images, dtype = torch.float).unsqueeze(dim = 1)\n",
        "          image = transform(images).squeeze(dim = 1)\n",
        "          label = train_data[train_data.patient_id == int(patient_id)].values[0][1:]\n",
        "          label_list.append(label)\n",
        "          img_list.append(image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "Sme0QUS1EpWw",
        "outputId": "021340ce-0646-4eae-b447-972888b84caa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b3f31c780be0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mTRAIN_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/reduced_256_tickness_5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimg_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEB1kK40YVD3",
        "outputId": "0f17cd84-a518-483a-9dc6-9954ee0c9be8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "class Config:\n",
        "    SEED = 42\n",
        "    IMAGE_SIZE = [256, 256]\n",
        "    BATCH_SIZE = 16\n",
        "    EPOCHS = 10\n",
        "    TARGET_COLS  = [\n",
        "       'bowel_healthy', 'bowel_injury', 'extravasation_healthy',\n",
        "       'extravasation_injury', 'kidney_healthy', 'kidney_low', 'kidney_high',\n",
        "       'liver_healthy', 'liver_low', 'liver_high', 'spleen_healthy',\n",
        "       'spleen_low', 'spleen_high', 'any_injury'\n",
        "    ]\n",
        "\n",
        "torch.manual_seed(Config.SEED)\n",
        "config = Config()\n",
        "len(Config.TARGET_COLS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpHnqB2JYdB5"
      },
      "outputs": [],
      "source": [
        "# Function to handle the split for each group\n",
        "def split_group(group, test_size=0.2):\n",
        "    if len(group) == 1:\n",
        "        return (group, pd.DataFrame()) if np.random.rand() < test_size else (pd.DataFrame(), group)\n",
        "    else:\n",
        "        return train_test_split(group, test_size=test_size, random_state=42)\n",
        "\n",
        "# Initialize the train and validation datasets\n",
        "train_data = pd.DataFrame()\n",
        "val_data = pd.DataFrame()\n",
        "\n",
        "# Iterate through the groups and split them, handling single-sample groups\n",
        "for _, group in dataframe.groupby(config.TARGET_COLS):\n",
        "    train_group, val_group = split_group(group)\n",
        "    train_data = pd.concat([train_data, train_group], ignore_index=True)\n",
        "    val_data = pd.concat([val_data, val_group], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q48TkpmMEth9"
      },
      "outputs": [],
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataframe, transform=True):\n",
        "        self.paths = dataframe.image_path.tolist()\n",
        "        self.labels = dataframe[config.TARGET_COLS].values\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = Image.open(self.paths[index]).convert('RGB')\n",
        "        label = torch.tensor(self.labels[index], dtype=torch.float32)\n",
        "\n",
        "        if self.transform:\n",
        "            image = torchvision.transforms.Compose([\n",
        "                    torchvision.transforms.Resize((256, 256)),\n",
        "                    torchvision.transforms.RandomResizedCrop(256),   # Random crop and resize\n",
        "                    torchvision.transforms.RandomHorizontalFlip(),    # Random horizontal flip\n",
        "                    torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color jitter\n",
        "                    torchvision.transforms.ToTensor(),\n",
        "                    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
        "                    )(image)\n",
        "        else:\n",
        "          image = torchvision.transforms.Compose([\n",
        "                    torchvision.transforms.Resize((256, 256)),\n",
        "                    torchvision.transforms.ToTensor(),\n",
        "                    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
        "                    )(image)\n",
        "\n",
        "\n",
        "\n",
        "      # def __getitem__(self, idx):\n",
        "\n",
        "      #   image_path = self.df['image_path'][idx]\n",
        "      #   dicom_file = pydicom.dcmread(image_path)\n",
        "      #   pixel_array = dicom_file.pixel_array.astype(np.int16)\n",
        "      #   image = Image.fromarray(pixel_array)\n",
        "      #   image = self.transform(image)\n",
        "      #   image = image.cuda()\n",
        "\n",
        "      #   target = self.df[config.TARGET_COLUMNS].iloc[idx]\n",
        "      #   target = torch.tensor(target.values, dtype=torch.float32)\n",
        "      #   target = target.cuda()\n",
        "\n",
        "      #   return image, target[:2], target[2:4], target[4:7], target[7:10], target[10:13]\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUhRbHfBYVBf"
      },
      "outputs": [],
      "source": [
        "# Create the datasets\n",
        "batch_size = 32\n",
        "\n",
        "dataset_train = MyDataset(train_data, transform=True)\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "dataset_val = MyDataset(val_data, transform=False)\n",
        "val_dataloader = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AbjAnt9YiQs",
        "outputId": "95a0db0f-7640-4a78-8b44-05588b271789"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 256, 256]) torch.Size([32, 14])\n"
          ]
        }
      ],
      "source": [
        "for img, label in train_dataloader:\n",
        "  print(img.shape, label.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMiR8CUjL7nQ"
      },
      "outputs": [],
      "source": [
        "#a function to display images\n",
        "def show_images(images):\n",
        "    fig, axes = plt.subplots(1, len(images), figsize=(15, 5))\n",
        "    for idx, image in enumerate(images):\n",
        "        image = torchvision.transforms.Compose([ torchvision.transforms.Normalize(mean = [ 0., 0., 0. ],std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
        "                                                 torchvision.transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ], std = [ 1., 1., 1. ])])(image)\n",
        "\n",
        "        image = image.permute(1, 2, 0)  # Convert from (C, H, W) to (H, W, C) for displaying\n",
        "        axes[idx].imshow(image)\n",
        "        axes[idx].axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUG8G9iIYiDP"
      },
      "outputs": [],
      "source": [
        "# Display the sample images\n",
        "indxs = torch.randint(len(dataset_train), size=(5,))\n",
        "show_images([dataset_train[i][0] for i in indxs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b_p_HP_YiAt",
        "outputId": "4abba4b8-5a58-4c1c-f617-292f46aff091"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# base = models.resnet152(weights=True)\n",
        "# conv1_weights = base.state_dict()['conv1.weight'].sum(dim=1, keepdim=True)\n",
        "# base.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "# base.state_dict()['conv1.weight'] = conv1_weights\n",
        "\n",
        "# # output for bowel and extravastaion\n",
        "# out_be = nn.Sequential(\n",
        "#     nn.Linear(1000, 512),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(0.5),\n",
        "#     nn.Linear(512, 2),\n",
        "#     nn.Softmax(dim=1)\n",
        "# )\n",
        "\n",
        "# # output for kidney, liver and spleen\n",
        "# out_kls = nn.Sequential(\n",
        "#     nn.Linear(1000, 512),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Dropout(0.5),\n",
        "#     nn.Linear(512, 3),\n",
        "#     nn.Softmax(dim=1)\n",
        "# )\n",
        "\n",
        "# class Model(nn.Module):\n",
        "#     def __init__(self, base, out_bowel, out_extravasation, out_kidney, out_liver, out_spleen):\n",
        "#         super(Model, self).__init__()\n",
        "#         self.base = base\n",
        "#         self.out_bowel = out_bowel\n",
        "#         self.out_extravasation = out_extravasation\n",
        "#         self.out_kidney = out_kidney\n",
        "#         self.out_liver = out_liver\n",
        "#         self.out_spleen = out_spleen\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.base(x)\n",
        "#         out_bowel = self.out_bowel(x)\n",
        "#         out_extravasation = self.out_extravasation(x)\n",
        "#         out_kidney = self.out_kidney(x)\n",
        "#         out_liver = self.out_liver(x)\n",
        "#         out_spleen = self.out_spleen(x)\n",
        "\n",
        "#         return out_bowel, out_extravasation, out_kidney, out_liver, out_spleen\n",
        "\n",
        "# model = Model(base, out_be, out_be, out_kls, out_kls, out_kls)"
      ],
      "metadata": {
        "id": "RnyZGj_aUL0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sq6VGo9dYpE2"
      },
      "outputs": [],
      "source": [
        "class MyModel(torch.nn.Module):\n",
        "    def __init__(self, input_shape=3):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.input = torch.nn.Conv2d(self.input_shape, 3, kernel_size=3)\n",
        "        self.model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "        # Freeze model parameters\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.model.fc = torch.nn.Sequential(\n",
        "            torch.nn.Linear(self.model.fc.in_features, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 32))\n",
        "\n",
        "        self.bowel = torch.nn.Linear(32, 2)\n",
        "        self.extra = torch.nn.Linear(32, 2)\n",
        "        self.liver = torch.nn.Linear(32, 3)\n",
        "        self.kidney = torch.nn.Linear(32, 3)\n",
        "        self.spleen = torch.nn.Linear(32, 3)\n",
        "        self.any_injury = torch.nn.Linear(32, 1)\n",
        "\n",
        "\n",
        "        for param in self.model.fc.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.input_shape != 3:\n",
        "          x = self.input(x)\n",
        "        model_out = self.model(x)\n",
        "        out1 = torch.nn.functional.softmax(self.bowel(model_out), dim=1)\n",
        "        out2 = torch.nn.functional.softmax(self.extra(model_out), dim=1)\n",
        "        out3 = torch.nn.functional.softmax(self.liver(model_out), dim=1)\n",
        "        out4 = torch.nn.functional.softmax(self.kidney(model_out), dim=1)\n",
        "        out5 = torch.nn.functional.softmax(self.spleen(model_out), dim=1)\n",
        "        out6 = torch.nn.functional.softmax(self.any_injury(model_out), dim=1)\n",
        "        out = torch.cat((out1, out2, out3, out4, out5, out6), dim=1)\n",
        "        return out\n",
        "\n",
        "# Instantiate the model\n",
        "model = MyModel().to(device)\n",
        "torchsummary.summary(model, (3, 256, 256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da_rXDNFYpB3"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model\n",
        "model = MyModel().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5, verbose=True)\n",
        "loss_one = torch.nn.BCEWithLogitsLoss(pos_weight = torch.tensor([6.0]).to(device))\n",
        "loss_two_bowl = torch.nn.CrossEntropyLoss(weight = torch.tensor([1.0, 2.0])).to(device)\n",
        "loss_two_extra = torch.nn.CrossEntropyLoss(weight = torch.tensor([1.0, 6.0])).to(device)\n",
        "loss_three  = torch.nn.CrossEntropyLoss(label_smoothing = 0.05, weight = torch.tensor([1.0, 2.0, 4.0])).to(device)\n",
        "\n",
        "\n",
        "\n",
        "total_epochs = 5\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "best_loss = 10000\n",
        "\n",
        "for epoch in range(total_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for train_ind, (images, labels) in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Move data to GPU\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        bowel_loss = loss_two_bowl(outputs[:, 0:2], labels[:, 0:2])\n",
        "        extravasation_loss = loss_two_extra(outputs[:, 2:4], labels[:, 2:4])\n",
        "        kidney_loss = loss_three(outputs[:, 4:7], labels[:, 4:7])\n",
        "        liver_loss = loss_three(outputs[:, 7:10], labels[:, 7:10])\n",
        "        spleen_loss = loss_three(outputs[:, 10:13], labels[:, 10:13])\n",
        "        any_injury_loss = loss_one(outputs[:, 13], labels[:, 13])\n",
        "\n",
        "\n",
        "        loss = bowel_loss + extravasation_loss + kidney_loss + liver_loss + spleen_loss + any_injury_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Update learning rate using the scheduler\n",
        "    # scheduler.step()\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for val_ind, (images, labels) in enumerate(val_dataloader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            bowel_loss = loss_two_bowl(outputs[:, 0:2], labels[:, 0:2]).item()\n",
        "            extravasation_loss = loss_two_extra(outputs[:, 2:4], labels[:, 2:4]).item()\n",
        "            kidney_loss = loss_three(outputs[:, 4:7], labels[:, 4:7]).item()\n",
        "            liver_loss = loss_three(outputs[:, 7:10], labels[:, 7:10]).item()\n",
        "            spleen_loss = loss_three(outputs[:, 10:13], labels[:, 10:13]).item()\n",
        "            any_injury_loss = loss_one(outputs[:, 13], labels[:, 13]).item()\n",
        "\n",
        "\n",
        "            val_loss += bowel_loss + extravasation_loss + kidney_loss + liver_loss + spleen_loss + any_injury_loss\n",
        "\n",
        "            predicted = torch.zeros(outputs.size()).to(device)\n",
        "            for ind, arg in enumerate(torch.argmax(outputs, dim=1)):\n",
        "              predicted[ind, arg] = 1\n",
        "\n",
        "            total += labels.size(0) * labels.size(1)  # Total number of predictions\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_dataloader)\n",
        "    val_accuracy = 100.0 * correct / total\n",
        "    if val_loss <= best_loss:\n",
        "      best_loss = val_loss\n",
        "      torch.save(model.state_dict(), './drive/MyDrive/model_weights.pth')\n",
        "     # Append loss and accuracy values to lists\n",
        "    train_losses.append(loss.item())\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    print(f\"Epoch [{epoch+1}/{total_epochs}] - Loss: {loss:.4f} - Val Loss: {val_loss:.4f} - Val Acc: {val_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Juuv2japviYk"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), './drive/MyDrive/model_Alexnet_weights.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3DdY61mYo9g"
      },
      "outputs": [],
      "source": [
        "# Plot training and validation progress\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train')\n",
        "plt.plot(val_losses, label='Validation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_accuracies, label='Validation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.title('Validation Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('./drive/MyDrive/DATA/model_weights.pth'))"
      ],
      "metadata": {
        "id": "sOQP_JxFUOXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1y5PBeEYU6n"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Select a random image from the validation dataset\n",
        "random_index = np.random.randint(len(dataset_val))\n",
        "image, label = dataset_val[random_index]\n",
        "\n",
        "# Move the image to the GPU if available\n",
        "image = image.to(device)\n",
        "\n",
        "# Pass the image through the model\n",
        "with torch.no_grad():\n",
        "    output = model(image.unsqueeze(0))  # Unsqueeze to add batch dimension\n",
        "\n",
        "# Convert the output logits to probabilities using sigmoid function\n",
        "predicted_probs = torch.sigmoid(output)[0]\n",
        "\n",
        "# Convert predicted probabilities to binary predictions\n",
        "predicted_labels = (predicted_probs > 0.5).int()\n",
        "\n",
        "# Display the image, actual labels, and predicted labels\n",
        "\n",
        "image = torchvision.transforms.Compose([ torchvision.transforms.Normalize(mean = [ 0., 0., 0. ],std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
        "                                torchvision.transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],std = [ 1., 1., 1. ])])(image.cpu())\n",
        "plt.imshow(image.permute(1, 2, 0))  # Move image to CPU and change channel order\n",
        "#plt.title(f\"Actual Labels: {label}\\nPredicted Labels: {predicted_labels}\")\n",
        "plt.title(f\"Actual Labels: {label}\\nPredicted Labels: {predicted_labels}\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cs3d-W2I2T43"
      },
      "outputs": [],
      "source": [
        "model = torch.load('model.pth')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "12SQbgBAE1bmRImRbs-8SQXdsSxHQiVOQ",
      "authorship_tag": "ABX9TyNx/8bjYZT6s2VT1ZK1Uu4R",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}